{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality in Houston - Step 1: Data Wrangling#\n",
    "\n",
    "## 1. Aim of the project and origin of the data ##\n",
    "\n",
    "### 1.1. The Problem: ###\n",
    "What is the future of indoor and outdoor air quality in Houston and its impact on Houstonians’ health?\n",
    "\n",
    "Air quality has been a concern for Houston’s officials and population for several years. Houston’ s legendary around-the-clock traffic jam, its growing population, its humid subtropical climate conditions,   and the sprawling potpourri of pollutants released by refineries and chemical plants have made Space City’s air hard to breathe for a lot of Houstonians. In 2018, The Mayor's Task Force on the Health Effects of Air Pollution has identified 12 pollutants as definite health risks for Houstonians, the main one being Ozone.\n",
    "\n",
    "In 2020, the plastic industry is growing rapidly, freeways are being widened to allow for more traffic to flow through, and more people are moving in. Houston  is currently the 5th largest metro population in the US with 6,997,384 inhabitants and is predicted to host 8.7 millions inhabitants by 2028 according to the Texas Demographic Center.  \n",
    "\n",
    "Where is the air quality headed?\n",
    "\n",
    "The aim of this capstone project is to predict the indoor and outdoor air quality in Houston for each upcoming decades up to 2050 using daily air data summaries, known potential drivers of air quality, and the city development forecast (i.e. population growth, change in land use...) from the Houston-Galveston Area Council. The impact of air quality on health will be presented by an overlay of AQI (Air Quality Index) calculated data and ELS (Effects Screening Levels). The analysis focuses on 6 pollutants of concern for Houston, namely Ozone (O3), Sulfur Dioxide (SO2), Carbon Monoxide (CO), Nitrogen Dioxide (NO2), and particulate matter (PM2.5 and PM10). In this project Air Quality refers to pollutant concentrations in the air.\n",
    "\n",
    "\n",
    "### 1.2. The Data: ###\n",
    "All the datasets used in this capstone were available online between 09/01/2020 and 09/15/2020 from the following websites:\n",
    "\n",
    "- Relationships of Indoor, Outdoor, and Personal Air (RIOPA) dataset:\n",
    "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/7UBE7P&version=1.0\n",
    "\n",
    "- Daily Pollutant concentration measurements for Houston and effect screening levels from the Texas Commision On Environmental Quality (TECQ): https://www.tceq.texas.gov/ \n",
    "\n",
    "- Daily Pollutant concentration measurements for Houston from the U.S. Environmental Protection Agency: https://www.epa.gov/\n",
    "\n",
    "- Weather daily summaries: https://www.noaa.gov/\n",
    "\n",
    "- Land use data, population data and forecast from the Houston-Galveston Area Council: http://www.h-gac.com/home/default.aspx\n",
    "\n",
    "- Road surface data: https://www.txdot.gov/inside-txdot/division/transportation-planning/roadway-inventory.html\n",
    "\n",
    "- Traffic count, road size data, census data: https://cohgis-mycity.opendata.arcgis.com\n",
    "\n",
    "\n",
    "### 1.3. Data and File Location On Github: ###\n",
    "This project is hosted on Github in \"Aurenkeelin18/TheFoxerine/CP2_AQ_HOUSTON\". The folder is organized using subfolders:\n",
    "- '00*_*OriginalData' contains all the original data files subdivided by role. 'AQ' contains air quality data and ancillary reports from TECQ and EPA. 'METEO' contains weather data from NOAA. 'RIOPA' contains the data from the indoor/outdoor stud from the RIOPA team.\n",
    "- '00*_*SavedDataframes' contains dataframes saved in excel files.\n",
    "- '00*_*StuffAndThings' contains images, maps or other miscellaneous items that were used in the project.\n",
    "- '00*_*ZeCollection' contains clean copies of piece of coding used in this project that felt worth setting aside for future usage (i.e. functions, code for mapping something, awesome looking plots...)\n",
    "- '01*_*DataWrangling' contains saved dataframes (pre-and post cleaning) and jupyter notebooks associated to the data wrangling.\n",
    "\n",
    "Additional subfolders will be added as the project progresses through the DSM following the same nomenclature - i.e. '02*_*EDA', '03*_*Modelling'...- as well as the final report and presentation.\n",
    "\n",
    "### 1.4. Getting Started ###\n",
    "Below are found the libraries used for data wrangling as well as the path to all subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict,OrderedDict, namedtuple\n",
    "import json\n",
    "\n",
    "path_header='C:\\\\Users\\\\Anne\\\\Documents\\\\GIT\\\\TheFoxerine\\\\'\n",
    "path_df='CP2_AQ_HOUSTON\\\\00_SavedDataframes\\\\'\n",
    "path_riopa='CP2_AQ_HOUSTON\\\\00_OriginalData\\\\RIOPA\\\\'\n",
    "path_meteo='CP2_AQ_HOUSTON\\\\00_OriginalData\\\\METEO\\\\'\n",
    "path_AQ='CP2_AQ_HOUSTON\\\\00_OriginalData\\\\AQ\\\\'\n",
    "path_demog='CP2_AQ_HOUSTON\\\\00_OriginalData\\\\DEMOG\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Data Collection##\n",
    "###2.1. Indoor/Outdoor Air Quality:###\n",
    "\n",
    "In this section the relevant csv files from the RIOPA dataset are loaded and merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1138 entries, 0 to 1137\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   LinkID         1138 non-null   object \n",
      " 1   SampleID       1138 non-null   int64  \n",
      " 2   HomeID         1138 non-null   object \n",
      " 3   Type           1138 non-null   object \n",
      " 4   PM25mass       1112 non-null   float64\n",
      " 5   Validity       171 non-null    float64\n",
      " 6   Mass_mg        1135 non-null   float64\n",
      " 7   Volume_m3      1117 non-null   float64\n",
      " 8   comments       1106 non-null   object \n",
      " 9   datestarted    1138 non-null   object \n",
      " 10  dateended      1138 non-null   object \n",
      " 11  PM25mass_flag  8 non-null      object \n",
      "dtypes: float64(4), int64(1), object(7)\n",
      "memory usage: 106.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1148 entries, 0 to 1147\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   LinkID      1148 non-null   object \n",
      " 1   ID          1148 non-null   int64  \n",
      " 2   HomeID      1148 non-null   object \n",
      " 3   Visit       1148 non-null   int64  \n",
      " 4   Start_Date  1148 non-null   object \n",
      " 5   End_Date    1148 non-null   object \n",
      " 6   Location    1148 non-null   object \n",
      " 7   temp_c      1148 non-null   float64\n",
      " 8   avg_rh      927 non-null    float64\n",
      " 9   source      408 non-null    float64\n",
      " 10  comments    719 non-null    object \n",
      "dtypes: float64(3), int64(2), object(6)\n",
      "memory usage: 98.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 414 entries, 0 to 413\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   LinkID     414 non-null    object\n",
      " 1   GRID_CODE  414 non-null    int64 \n",
      " 2   Class      414 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 9.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 533 entries, 0 to 532\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   LinkID      533 non-null    object \n",
      " 1   ID          533 non-null    int64  \n",
      " 2   HomeID      533 non-null    object \n",
      " 3   AER         524 non-null    float64\n",
      " 4   comment     14 non-null     object \n",
      " 5   Start_Date  533 non-null    object \n",
      " 6   End_Date    533 non-null    object \n",
      " 7   AER_flag    509 non-null    object \n",
      "dtypes: float64(1), int64(1), object(6)\n",
      "memory usage: 33.4+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9265 entries, 0 to 9264\n",
      "Data columns (total 18 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   LinkID                  9265 non-null   object \n",
      " 1   Wban_number             9265 non-null   int64  \n",
      " 2   datestarted             9255 non-null   object \n",
      " 3   dateended               9255 non-null   object \n",
      " 4   avg_Dry_bulb_temp       9255 non-null   float64\n",
      " 5   avg_Dew_point_temp      9255 non-null   float64\n",
      " 6   avg_Wet_bulb_temp       9255 non-null   float64\n",
      " 7   avg_RH                  9255 non-null   float64\n",
      " 8   avg_WS                  9255 non-null   float64\n",
      " 9   avg_Val_wind_char       9255 non-null   float64\n",
      " 10  avg_Pressure            9255 non-null   float64\n",
      " 11  avg_Sea_Level_Pressure  9255 non-null   float64\n",
      " 12  avg_Precipitation       9255 non-null   float64\n",
      " 13  avg_Sky_conditions      9255 non-null   object \n",
      " 14  avg_Visibility          9255 non-null   object \n",
      " 15  avg_Weather_type        9255 non-null   object \n",
      " 16  avg_Wind_Gust           9255 non-null   object \n",
      " 17  avg_WD                  9255 non-null   float64\n",
      "dtypes: float64(10), int64(1), object(7)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1461 entries, 0 to 1460\n",
      "Data columns (total 39 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   STATION    1461 non-null   object \n",
      " 1   NAME       1461 non-null   object \n",
      " 2   LATITUDE   1461 non-null   float64\n",
      " 3   LONGITUDE  1461 non-null   float64\n",
      " 4   ELEVATION  1461 non-null   float64\n",
      " 5   DATE       1461 non-null   object \n",
      " 6   AWND       1461 non-null   float64\n",
      " 7   FMTM       1460 non-null   float64\n",
      " 8   PGTM       1457 non-null   float64\n",
      " 9   PRCP       1461 non-null   float64\n",
      " 10  SNOW       1461 non-null   float64\n",
      " 11  SNWD       1461 non-null   float64\n",
      " 12  TAVG       1460 non-null   float64\n",
      " 13  TMAX       1461 non-null   int64  \n",
      " 14  TMIN       1461 non-null   int64  \n",
      " 15  TSUN       467 non-null    float64\n",
      " 16  WDF2       1461 non-null   int64  \n",
      " 17  WDF5       1461 non-null   int64  \n",
      " 18  WESD       1461 non-null   float64\n",
      " 19  WSF2       1461 non-null   float64\n",
      " 20  WSF5       1461 non-null   float64\n",
      " 21  WT01       738 non-null    float64\n",
      " 22  WT02       87 non-null     float64\n",
      " 23  WT03       272 non-null    float64\n",
      " 24  WT04       0 non-null      float64\n",
      " 25  WT05       4 non-null      float64\n",
      " 26  WT06       0 non-null      float64\n",
      " 27  WT07       0 non-null      float64\n",
      " 28  WT08       117 non-null    float64\n",
      " 29  WT09       0 non-null      float64\n",
      " 30  WT11       2 non-null      float64\n",
      " 31  WT13       696 non-null    float64\n",
      " 32  WT14       35 non-null     float64\n",
      " 33  WT15       0 non-null      float64\n",
      " 34  WT16       569 non-null    float64\n",
      " 35  WT17       0 non-null      float64\n",
      " 36  WT18       0 non-null      float64\n",
      " 37  WT21       76 non-null     float64\n",
      " 38  WV03       3 non-null      float64\n",
      "dtypes: float64(32), int64(4), object(3)\n",
      "memory usage: 445.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 414 entries, 0 to 413\n",
      "Data columns (total 2 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   LinkID                   414 non-null    object\n",
      " 1   Census_Block_Group_Code  414 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 6.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43439 entries, 0 to 43438\n",
      "Data columns (total 73 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   census_group_block_code  43439 non-null  int64  \n",
      " 1   home_lat                 43439 non-null  float64\n",
      " 2   home_long                43439 non-null  float64\n",
      " 3   geoid                    43439 non-null  float64\n",
      " 4   STATE                    43439 non-null  int64  \n",
      " 5   COUNTY                   43439 non-null  int64  \n",
      " 6   TRACT                    43439 non-null  int64  \n",
      " 7   BLKGRP                   43439 non-null  int64  \n",
      " 8   OBJECTID_1               43439 non-null  int64  \n",
      " 9   OBJECTID                 43439 non-null  int64  \n",
      " 10  MTFCC10                  43439 non-null  object \n",
      " 11  UR10                     43439 non-null  object \n",
      " 12  UACE10                   43439 non-null  object \n",
      " 13  FUNCSTAT10               43439 non-null  object \n",
      " 14  ALAND10                  43439 non-null  int64  \n",
      " 15  AWATER10                 43439 non-null  int64  \n",
      " 16  INTPTLAT10               43439 non-null  float64\n",
      " 17  INTPTLON10               43439 non-null  float64\n",
      " 18  SUMLEV                   43439 non-null  int64  \n",
      " 19  LOGRECNO                 43439 non-null  int64  \n",
      " 20  GEOID                    43439 non-null  float64\n",
      " 21  PLACE                    43439 non-null  int64  \n",
      " 22  BLOCK                    43439 non-null  int64  \n",
      " 23  CBSA                     43439 non-null  int64  \n",
      " 24  CD                       43439 non-null  int64  \n",
      " 25  SLDU                     43439 non-null  int64  \n",
      " 26  SLDL                     43439 non-null  int64  \n",
      " 27  VTD                      43439 non-null  int64  \n",
      " 28  VTDI                     43439 non-null  object \n",
      " 29  ZCTA5                    43439 non-null  object \n",
      " 30  SDSEC                    43439 non-null  int64  \n",
      " 31  SDUNI                    43439 non-null  int64  \n",
      " 32  NAME                     43439 non-null  object \n",
      " 33  POP100                   43439 non-null  int64  \n",
      " 34  HU100                    43439 non-null  int64  \n",
      " 35  TotPop                   43439 non-null  int64  \n",
      " 36  HispPop                  43439 non-null  int64  \n",
      " 37  NonHispPop               43439 non-null  int64  \n",
      " 38  NHOneRace                43439 non-null  int64  \n",
      " 39  NH_White                 43439 non-null  int64  \n",
      " 40  NH_Black                 43439 non-null  int64  \n",
      " 41  NH_AmInd                 43439 non-null  int64  \n",
      " 42  NH_Asian                 43439 non-null  int64  \n",
      " 43  NH_HawPacI               43439 non-null  int64  \n",
      " 44  NH_Other                 43439 non-null  int64  \n",
      " 45  NH_2orMore               43439 non-null  int64  \n",
      " 46  VAP_TotPop               43439 non-null  int64  \n",
      " 47  VAP_HispPo               43439 non-null  int64  \n",
      " 48  VAP_NonHis               43439 non-null  int64  \n",
      " 49  VAP_NHOneR               43439 non-null  int64  \n",
      " 50  VAP_NH_Whi               43439 non-null  int64  \n",
      " 51  VAP_NH_Bla               43439 non-null  int64  \n",
      " 52  VAP_NH_AmI               43439 non-null  int64  \n",
      " 53  VAP_NH_Asi               43439 non-null  int64  \n",
      " 54  VAP_HawPac               43439 non-null  int64  \n",
      " 55  VAP_NH_Oth               43439 non-null  int64  \n",
      " 56  VAP_NH_2or               43439 non-null  int64  \n",
      " 57  TotHousing               43439 non-null  int64  \n",
      " 58  OccHU                    43439 non-null  int64  \n",
      " 59  VacantHU                 43439 non-null  int64  \n",
      " 60  GEOID_1                  43439 non-null  float64\n",
      " 61  Owner_Occu               43439 non-null  int64  \n",
      " 62  Renter_occ               43439 non-null  int64  \n",
      " 63  Shape_Leng               43439 non-null  float64\n",
      " 64  Asian_Tota               43439 non-null  int64  \n",
      " 65  Other_Tota               43439 non-null  int64  \n",
      " 66  percent_white            43439 non-null  float64\n",
      " 67  percent_black            43439 non-null  float64\n",
      " 68  percent_hispanic         43439 non-null  float64\n",
      " 69  percent_asian            43439 non-null  float64\n",
      " 70  percent_other            43439 non-null  float64\n",
      " 71  Shapearea                43439 non-null  float64\n",
      " 72  Shapelen                 43439 non-null  float64\n",
      "dtypes: float64(15), int64(51), object(7)\n",
      "memory usage: 24.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Loading all files pertinent to the Indoor/Outdoor AQ analysis\n",
    "riopa_pm25=pd.read_csv(path_header+path_riopa+'PM_Mass.csv')\n",
    "riopa_temp=pd.read_csv(path_header+path_riopa+'TempRH.csv')\n",
    "riopa_landuse=pd.read_csv(path_header+path_riopa+'Land_Use.csv')\n",
    "riopa_aer=pd.read_csv(path_header+path_riopa+'AER.csv')\n",
    "riopa_meteo=pd.read_csv(path_header+path_riopa+'met_avg_linkid.csv')\n",
    "riopa_blockgroup=pd.read_csv(path_header+path_riopa+'CensusBlockGroup_Home.csv')\n",
    "coh_blockgroup=pd.read_csv(path_header+path_demog+'COH_DEMOGRAPHICS.csv')\n",
    "\n",
    "### let's have a look to the column names\n",
    "riopa_list=[riopa_pm25,riopa_temp,riopa_landuse,riopa_aer,riopa_meteo,riopa_meteo_noaa,riopa_blockgroup,coh_blockgroup]\n",
    "for name in riopa_list:\n",
    "    print(name.info())   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinkID is the primary key in the RIOPA relational database. It is a unique subject home identifier, first two digits state code (NJ,TX, CA), three digit numbers assigned to home in \"Home ID\"\n",
    "during first visit, one digit number specifying the visit number (1 =first, 2=second), one digit sample type (lndoor=1,Outdoor=2, Personal Adult=3, Personal Child 1-4 ~ 4-7, Blank\n",
    "=8, Control = 9, Vehicle =0), one digit for duplicate/QA code (Sample = 0, Duplicate =1, Repeat Analysis = 2, Backup of PUF sample for Breakthrough =3, Backup Duplicate=4). Only LinkID starting with TX will be kept.\n",
    "\n",
    "The RIOPA datasets will be joined using the link ID and startdate. The weather daily summaries from NOAA (riopa*_*meteo*_*noaa) wil be joined to startdate with DATE.\n",
    "As not all the data present in the datasets are relevant to the analysis, each dataset will be filtered prior to the merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter riopa_pm25\n",
    "f_riopa_pm25=riopa_pm25[['LinkID','SampleID','HomeID','Type','PM25mass','Validity','comments','datestarted','dateended']]\n",
    "\n",
    "### Filter riopa_temp\n",
    "f_riopa_temp=riopa_temp[['LinkID','ID','HomeID','Visit','Start_Date','Location','temp_c','avg_rh']]\n",
    "\n",
    "### Filter riopa_landuse\n",
    "f_riopa_landuse=riopa_landuse[['LinkID','Class']]\n",
    "\n",
    "### Filter riopa_aer\n",
    "f_riopa_aer=riopa_aer[['LinkID','HomeID','AER','comment','Start_Date','End_Date']]\n",
    "\n",
    "### Filter riopa_meteo\n",
    "f_riopa_meteo=riopa_meteo[['LinkID','datestarted','dateended','avg_Dry_bulb_temp','avg_Dew_point_temp','avg_Wet_bulb_temp','avg_RH']]\n",
    "\n",
    "### Filter riopa_blockgroup\n",
    "f_riopa_blockgroup=riopa_blockgroup[['LinkID', 'Census_Block_Group_Code']]\n",
    "\n",
    "### Filter coh_blockgroup\n",
    "f_coh_blockgroup=coh_blockgroup[['census_group_block_code', 'home_lat', 'home_long', 'geoid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am renaming the columns of the dataframes below to make the merge/join coding easier to read and to keep the metadata intact (i.e. rename 'comment' and dates columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clarify column names\n",
    "col_f_riopa_pm25={'LinkID':'linkid','SampleID':'sampleid','HomeID':'homeid','Type':'airtype','PM25mass':'pm25','Validity':'validity','comments':'comments_pm25','datestarted':'date_start_pm25','dateended':'date_end_pm25'}\n",
    "fr_riopa_pm25=f_riopa_pm25.rename(columns=col_f_riopa_pm25)\n",
    "\n",
    "col_f_riopa_temp={'LinkID':'linkid','ID':'tempid','HomeID':'homeid','Visit':'visitnumber','Start_Date':'date_temp','Location':'location','temp_c':'ambient_temp_c','avg_rh':'ambient_rh'}\n",
    "fr_riopa_temp=f_riopa_temp.rename(columns=col_f_riopa_temp)\n",
    "\n",
    "col_f_riopa_landuse={'LinkID':'linkid','Class':'landuse_class'}\n",
    "fr_riopa_landuse=f_riopa_landuse.rename(columns=col_f_riopa_landuse)\n",
    "\n",
    "col_f_riopa_aer={'LinkID':'linkid','HomeID':'homeid','AER':'airexrate','comment':'comment_aer','Start_Date':'date_start_aer','End_Date':'date_end_aer'}\n",
    "fr_riopa_aer=f_riopa_aer.rename(columns=col_f_riopa_aer)\n",
    "\n",
    "col_f_riopa_meteo={'LinkID':'linkid','datestarted':'date','dateended':'date_end_meteo','avg_Dry_bulb_temp':'temp_dry','avg_Dew_point_temp':'dew_point','avg_Wet_bulb_temp':'temp_wet','avg_RH':'rh'}\n",
    "fr_riopa_meteo=f_riopa_meteo.rename(columns=col_f_riopa_meteo)\n",
    "\n",
    "col_f_riopa_blockgroup={'LinkID':'linkid','Census_Block_Group_Code':'census_group_block_code'}\n",
    "fr_riopa_blockgroup=f_riopa_blockgroup.rename(columns=col_f_riopa_blockgroup)\n",
    "\n",
    "### coh_blockgroup is fine as is\n",
    "fr_coh_blockgroup=f_coh_blockgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RIOPA datasets contain data from Houston (TX), Elizabeth (NJ), and Los Angeles (CA). Only the texan data is of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing non texan data\n",
    "tx_riopa_pm25=fr_riopa_pm25[fr_riopa_pm25.linkid.str.contains('TX')==True]\n",
    "tx_riopa_temp=fr_riopa_temp[fr_riopa_temp.linkid.str.contains('TX')==True]\n",
    "tx_riopa_landuse=fr_riopa_landuse[fr_riopa_landuse.linkid.str.contains('TX')==True]\n",
    "tx_riopa_aer=fr_riopa_aer[fr_riopa_aer.linkid.str.contains('TX')==True]\n",
    "tx_riopa_meteo=fr_riopa_meteo[fr_riopa_meteo.linkid.str.contains('TX')==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before merging is to convert the dates from 'object' to datetime and to add a formal \"date\" column to dataframes where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "### Converting date columns to datetime objects and adding a date column.\n",
    "tx_riopa_pm25['date_start_pm25'] = pd.to_datetime(tx_riopa_pm25['date_start_pm25'])\n",
    "tx_riopa_pm25['date_end_pm25'] = pd.to_datetime(tx_riopa_pm25['date_end_pm25'])\n",
    "tx_riopa_pm25['date']=tx_riopa_pm25['date_start_pm25']\n",
    "\n",
    "tx_riopa_temp['date_temp'] = pd.to_datetime(tx_riopa_temp['date_temp'])\n",
    "tx_riopa_temp['date']=tx_riopa_temp['date_temp']\n",
    "\n",
    "tx_riopa_aer['date_start_aer'] = pd.to_datetime(tx_riopa_aer['date_start_aer'])\n",
    "tx_riopa_aer['date_end_aer'] = pd.to_datetime(tx_riopa_aer['date_end_aer'])\n",
    "tx_riopa_aer['date']=tx_riopa_aer['date_start_aer']\n",
    "\n",
    "tx_riopa_meteo['date'] = pd.to_datetime(tx_riopa_meteo['date'])\n",
    "tx_riopa_meteo['date_end_meteo'] = pd.to_datetime(tx_riopa_meteo['date_end_meteo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the tx_riopa_meteo dataframe contains three measurements per sampling period that will create a duplication issue during merging (i.e. there will be 3 identical PM 2.5 measurement per given date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linkid</th>\n",
       "      <th>date</th>\n",
       "      <th>date_end_meteo</th>\n",
       "      <th>temp_dry</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>temp_wet</th>\n",
       "      <th>rh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6817</th>\n",
       "      <td>TX001110</td>\n",
       "      <td>1999-06-23 22:53:00</td>\n",
       "      <td>1999-06-25 22:53:00</td>\n",
       "      <td>80.5143</td>\n",
       "      <td>76.0082</td>\n",
       "      <td>77.3286</td>\n",
       "      <td>86.7959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>TX001110</td>\n",
       "      <td>1999-06-23 22:53:00</td>\n",
       "      <td>1999-06-25 22:53:00</td>\n",
       "      <td>80.7653</td>\n",
       "      <td>76.2286</td>\n",
       "      <td>77.5306</td>\n",
       "      <td>86.9796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6819</th>\n",
       "      <td>TX001110</td>\n",
       "      <td>1999-06-23 22:53:00</td>\n",
       "      <td>1999-06-25 22:53:00</td>\n",
       "      <td>80.9184</td>\n",
       "      <td>76.1878</td>\n",
       "      <td>77.5592</td>\n",
       "      <td>86.3061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6820</th>\n",
       "      <td>TX001120</td>\n",
       "      <td>1999-06-23 22:53:00</td>\n",
       "      <td>1999-06-25 22:53:00</td>\n",
       "      <td>80.5143</td>\n",
       "      <td>76.0082</td>\n",
       "      <td>77.3286</td>\n",
       "      <td>86.7959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6821</th>\n",
       "      <td>TX001120</td>\n",
       "      <td>1999-06-23 22:53:00</td>\n",
       "      <td>1999-06-25 22:53:00</td>\n",
       "      <td>80.7653</td>\n",
       "      <td>76.2286</td>\n",
       "      <td>77.5306</td>\n",
       "      <td>86.9796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        linkid                date      date_end_meteo  temp_dry  dew_point  \\\n",
       "6817  TX001110 1999-06-23 22:53:00 1999-06-25 22:53:00   80.5143    76.0082   \n",
       "6818  TX001110 1999-06-23 22:53:00 1999-06-25 22:53:00   80.7653    76.2286   \n",
       "6819  TX001110 1999-06-23 22:53:00 1999-06-25 22:53:00   80.9184    76.1878   \n",
       "6820  TX001120 1999-06-23 22:53:00 1999-06-25 22:53:00   80.5143    76.0082   \n",
       "6821  TX001120 1999-06-23 22:53:00 1999-06-25 22:53:00   80.7653    76.2286   \n",
       "\n",
       "      temp_wet       rh  \n",
       "6817   77.3286  86.7959  \n",
       "6818   77.5306  86.9796  \n",
       "6819   77.5592  86.3061  \n",
       "6820   77.3286  86.7959  \n",
       "6821   77.5306  86.9796  "
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_riopa_meteo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A non-indexed, groupby on the dataframe with an aggregation ('mean') will get unique records per date and linkid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linkid</th>\n",
       "      <th>date</th>\n",
       "      <th>temp_dry</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>temp_wet</th>\n",
       "      <th>rh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TX001110</td>\n",
       "      <td>1999-06-23</td>\n",
       "      <td>80.732667</td>\n",
       "      <td>76.141533</td>\n",
       "      <td>77.472800</td>\n",
       "      <td>86.693867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TX001120</td>\n",
       "      <td>1999-06-23</td>\n",
       "      <td>80.732667</td>\n",
       "      <td>76.141533</td>\n",
       "      <td>77.472800</td>\n",
       "      <td>86.693867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TX001130</td>\n",
       "      <td>1999-06-23</td>\n",
       "      <td>80.752400</td>\n",
       "      <td>76.197267</td>\n",
       "      <td>77.514967</td>\n",
       "      <td>86.809533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TX001140</td>\n",
       "      <td>1999-06-23</td>\n",
       "      <td>80.752400</td>\n",
       "      <td>76.197267</td>\n",
       "      <td>77.514967</td>\n",
       "      <td>86.809533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TX001210</td>\n",
       "      <td>2001-02-08</td>\n",
       "      <td>63.957467</td>\n",
       "      <td>57.645400</td>\n",
       "      <td>60.595767</td>\n",
       "      <td>80.602833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     linkid       date   temp_dry  dew_point   temp_wet         rh\n",
       "0  TX001110 1999-06-23  80.732667  76.141533  77.472800  86.693867\n",
       "1  TX001120 1999-06-23  80.732667  76.141533  77.472800  86.693867\n",
       "2  TX001130 1999-06-23  80.752400  76.197267  77.514967  86.809533\n",
       "3  TX001140 1999-06-23  80.752400  76.197267  77.514967  86.809533\n",
       "4  TX001210 2001-02-08  63.957467  57.645400  60.595767  80.602833"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_riopa_meteo2=tx_riopa_meteo.groupby(['linkid','date'], as_index=False).agg({'temp_dry':'mean','dew_point':'mean','temp_wet': 'mean','rh':'mean'})\n",
    "tx_riopa_meteo2['date']=tx_riopa_meteo2['date'].dt.date\n",
    "tx_riopa_meteo2['date'] = pd.to_datetime(tx_riopa_meteo2['date'])\n",
    "tx_riopa_meteo2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 832 entries, 0 to 831\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   linkid     832 non-null    object        \n",
      " 1   date       832 non-null    datetime64[ns]\n",
      " 2   temp_dry   832 non-null    float64       \n",
      " 3   dew_point  832 non-null    float64       \n",
      " 4   temp_wet   832 non-null    float64       \n",
      " 5   rh         832 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(4), object(1)\n",
      "memory usage: 45.5+ KB\n"
     ]
    }
   ],
   "source": [
    "tx_riopa_meteo2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are ready to be merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge landuse to pm25 to create riopa_m1\n",
    "riopa_m1=pd.merge(tx_riopa_pm25,tx_riopa_landuse,how='left',on='linkid')\n",
    "\n",
    "### merge riopa_m1 with tx_riopa_temp on linkid,homeid and date\n",
    "riopa_m2=pd.merge(riopa_m1,tx_riopa_temp,how='left',on=['linkid','homeid','date'])\n",
    "\n",
    "### merge riopa_m2 with tx_riopa_aer on linkid,homeid and date to create riopa_m3\n",
    "riopa_m3=pd.merge(riopa_m2,tx_riopa_aer,how='left',on=['linkid','homeid','date'])\n",
    "\n",
    "# merge riopa_m3 with tx_riopa_meteo on linkid to create riopa_m4\n",
    "riopa_m4=pd.merge(riopa_m3,tx_riopa_meteo2,how='left',on=['linkid','date'])\n",
    "\n",
    "# merge riopa_m4 with fr_riopa_blockgroup on linkid to create riopa_m5\n",
    "riopa_m5=pd.merge(riopa_m4,fr_riopa_blockgroup,how='left',on='linkid')\n",
    "\n",
    "# merge riopa_m5 with fr_coh_blockgroup on census_group_block_code to create riopa\n",
    "riopa=pd.merge(riopa_m5,fr_coh_blockgroup,how='left',on='census_group_block_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linkid</th>\n",
       "      <th>sampleid</th>\n",
       "      <th>homeid</th>\n",
       "      <th>airtype</th>\n",
       "      <th>pm25</th>\n",
       "      <th>validity</th>\n",
       "      <th>comments_pm25</th>\n",
       "      <th>date_start_pm25</th>\n",
       "      <th>date_end_pm25</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>date_start_aer</th>\n",
       "      <th>date_end_aer</th>\n",
       "      <th>temp_dry</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>temp_wet</th>\n",
       "      <th>rh</th>\n",
       "      <th>census_group_block_code</th>\n",
       "      <th>home_lat</th>\n",
       "      <th>home_long</th>\n",
       "      <th>geoid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TX103110</td>\n",
       "      <td>10951</td>\n",
       "      <td>EX103</td>\n",
       "      <td>INDOOR</td>\n",
       "      <td>15.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>2000-10-17</td>\n",
       "      <td>2000-10-19</td>\n",
       "      <td>2000-10-17</td>\n",
       "      <td>...</td>\n",
       "      <td>2000-10-17</td>\n",
       "      <td>2000-10-19</td>\n",
       "      <td>69.283667</td>\n",
       "      <td>60.049633</td>\n",
       "      <td>63.702133</td>\n",
       "      <td>75.858133</td>\n",
       "      <td>4.820125e+11</td>\n",
       "      <td>29.792019</td>\n",
       "      <td>-95.081076</td>\n",
       "      <td>4.820130e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TX103110</td>\n",
       "      <td>10951</td>\n",
       "      <td>EX103</td>\n",
       "      <td>INDOOR</td>\n",
       "      <td>15.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>2000-10-17</td>\n",
       "      <td>2000-10-19</td>\n",
       "      <td>2000-10-17</td>\n",
       "      <td>...</td>\n",
       "      <td>2000-10-17</td>\n",
       "      <td>2000-10-19</td>\n",
       "      <td>69.283667</td>\n",
       "      <td>60.049633</td>\n",
       "      <td>63.702133</td>\n",
       "      <td>75.858133</td>\n",
       "      <td>4.820125e+11</td>\n",
       "      <td>29.778960</td>\n",
       "      <td>-95.122067</td>\n",
       "      <td>4.820130e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TX103110</td>\n",
       "      <td>10951</td>\n",
       "      <td>EX103</td>\n",
       "      <td>INDOOR</td>\n",
       "      <td>15.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>2000-10-17</td>\n",
       "      <td>2000-10-19</td>\n",
       "      <td>2000-10-17</td>\n",
       "      <td>...</td>\n",
       "      <td>2000-10-17</td>\n",
       "      <td>2000-10-19</td>\n",
       "      <td>69.283667</td>\n",
       "      <td>60.049633</td>\n",
       "      <td>63.702133</td>\n",
       "      <td>75.858133</td>\n",
       "      <td>4.820125e+11</td>\n",
       "      <td>29.789025</td>\n",
       "      <td>-95.094954</td>\n",
       "      <td>4.820130e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TX103110</td>\n",
       "      <td>10951</td>\n",
       "      <td>EX103</td>\n",
       "      <td>INDOOR</td>\n",
       "      <td>15.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>2000-10-17</td>\n",
       "      <td>2000-10-19</td>\n",
       "      <td>2000-10-17</td>\n",
       "      <td>...</td>\n",
       "      <td>2000-10-17</td>\n",
       "      <td>2000-10-19</td>\n",
       "      <td>69.283667</td>\n",
       "      <td>60.049633</td>\n",
       "      <td>63.702133</td>\n",
       "      <td>75.858133</td>\n",
       "      <td>4.820125e+11</td>\n",
       "      <td>29.795403</td>\n",
       "      <td>-95.071440</td>\n",
       "      <td>4.820130e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TX103110</td>\n",
       "      <td>10951</td>\n",
       "      <td>EX103</td>\n",
       "      <td>INDOOR</td>\n",
       "      <td>15.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>2000-10-17</td>\n",
       "      <td>2000-10-19</td>\n",
       "      <td>2000-10-17</td>\n",
       "      <td>...</td>\n",
       "      <td>2000-10-17</td>\n",
       "      <td>2000-10-19</td>\n",
       "      <td>69.283667</td>\n",
       "      <td>60.049633</td>\n",
       "      <td>63.702133</td>\n",
       "      <td>75.858133</td>\n",
       "      <td>4.820125e+11</td>\n",
       "      <td>29.782531</td>\n",
       "      <td>-95.113221</td>\n",
       "      <td>4.820130e+14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     linkid  sampleid homeid airtype  pm25  validity comments_pm25  \\\n",
       "0  TX103110     10951  EX103  INDOOR  15.2       NaN            \\r   \n",
       "1  TX103110     10951  EX103  INDOOR  15.2       NaN            \\r   \n",
       "2  TX103110     10951  EX103  INDOOR  15.2       NaN            \\r   \n",
       "3  TX103110     10951  EX103  INDOOR  15.2       NaN            \\r   \n",
       "4  TX103110     10951  EX103  INDOOR  15.2       NaN            \\r   \n",
       "\n",
       "  date_start_pm25 date_end_pm25       date  ... date_start_aer  date_end_aer  \\\n",
       "0      2000-10-17    2000-10-19 2000-10-17  ...     2000-10-17    2000-10-19   \n",
       "1      2000-10-17    2000-10-19 2000-10-17  ...     2000-10-17    2000-10-19   \n",
       "2      2000-10-17    2000-10-19 2000-10-17  ...     2000-10-17    2000-10-19   \n",
       "3      2000-10-17    2000-10-19 2000-10-17  ...     2000-10-17    2000-10-19   \n",
       "4      2000-10-17    2000-10-19 2000-10-17  ...     2000-10-17    2000-10-19   \n",
       "\n",
       "    temp_dry  dew_point   temp_wet         rh  census_group_block_code  \\\n",
       "0  69.283667  60.049633  63.702133  75.858133             4.820125e+11   \n",
       "1  69.283667  60.049633  63.702133  75.858133             4.820125e+11   \n",
       "2  69.283667  60.049633  63.702133  75.858133             4.820125e+11   \n",
       "3  69.283667  60.049633  63.702133  75.858133             4.820125e+11   \n",
       "4  69.283667  60.049633  63.702133  75.858133             4.820125e+11   \n",
       "\n",
       "    home_lat  home_long         geoid  \n",
       "0  29.792019 -95.081076  4.820130e+14  \n",
       "1  29.778960 -95.122067  4.820130e+14  \n",
       "2  29.789025 -95.094954  4.820130e+14  \n",
       "3  29.795403 -95.071440  4.820130e+14  \n",
       "4  29.782531 -95.113221  4.820130e+14  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "riopa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant data from RIOPA has been merged together in the dataframe 'riopa'. Further processing of the data will be done later in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.2. Weather Daily Summaries:###\n",
    "The weather summaries from NOAA cover the daily recordings collected from three stations in the Houston area from 01/01/1999 to 12/31/2001 and 01/01/2008 to 09/01/2020. The stations are \"Hobby Airport\", \"IAH\" and \"Galveston\". Columns headers are identical from file to file. Only the 'date ' columns need to be converted to a datetime object prior to concatenating the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17533 entries, 0 to 1095\n",
      "Data columns (total 74 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   STATION          17533 non-null  object        \n",
      " 1   NAME             17533 non-null  object        \n",
      " 2   LATITUDE         17533 non-null  float64       \n",
      " 3   LONGITUDE        17533 non-null  float64       \n",
      " 4   ELEVATION        17533 non-null  float64       \n",
      " 5   DATE             17533 non-null  datetime64[ns]\n",
      " 6   AWND             17264 non-null  float64       \n",
      " 7   AWND_ATTRIBUTES  15803 non-null  object        \n",
      " 8   PGTM             7455 non-null   float64       \n",
      " 9   PGTM_ATTRIBUTES  5998 non-null   object        \n",
      " 10  PRCP             17523 non-null  float64       \n",
      " 11  PRCP_ATTRIBUTES  16062 non-null  object        \n",
      " 12  SNOW             12806 non-null  float64       \n",
      " 13  SNOW_ATTRIBUTES  11345 non-null  object        \n",
      " 14  SNWD             12771 non-null  float64       \n",
      " 15  SNWD_ATTRIBUTES  11310 non-null  object        \n",
      " 16  TAVG             13558 non-null  float64       \n",
      " 17  TAVG_ATTRIBUTES  12098 non-null  object        \n",
      " 18  TMAX             17494 non-null  float64       \n",
      " 19  TMAX_ATTRIBUTES  16033 non-null  object        \n",
      " 20  TMIN             17493 non-null  float64       \n",
      " 21  TMIN_ATTRIBUTES  16032 non-null  object        \n",
      " 22  WDF2             17296 non-null  float64       \n",
      " 23  WDF2_ATTRIBUTES  15835 non-null  object        \n",
      " 24  WDF5             17197 non-null  float64       \n",
      " 25  WDF5_ATTRIBUTES  15736 non-null  object        \n",
      " 26  WSF2             17296 non-null  float64       \n",
      " 27  WSF2_ATTRIBUTES  15835 non-null  object        \n",
      " 28  WSF5             17198 non-null  float64       \n",
      " 29  WSF5_ATTRIBUTES  15737 non-null  object        \n",
      " 30  WT01             6536 non-null   float64       \n",
      " 31  WT01_ATTRIBUTES  5798 non-null   object        \n",
      " 32  WT02             908 non-null    float64       \n",
      " 33  WT02_ATTRIBUTES  821 non-null    object        \n",
      " 34  WT03             2710 non-null   float64       \n",
      " 35  WT03_ATTRIBUTES  2438 non-null   object        \n",
      " 36  WT04             16 non-null     float64       \n",
      " 37  WT04_ATTRIBUTES  16 non-null     object        \n",
      " 38  WT05             575 non-null    float64       \n",
      " 39  WT05_ATTRIBUTES  571 non-null    object        \n",
      " 40  WT06             4 non-null      float64       \n",
      " 41  WT06_ATTRIBUTES  4 non-null      object        \n",
      " 42  WT08             1914 non-null   float64       \n",
      " 43  WT08_ATTRIBUTES  1797 non-null   object        \n",
      " 44  WT10             10 non-null     float64       \n",
      " 45  WT10_ATTRIBUTES  10 non-null     object        \n",
      " 46  FMTM             6385 non-null   float64       \n",
      " 47  FMTM_ATTRIBUTES  4925 non-null   object        \n",
      " 48  TSUN             1024 non-null   float64       \n",
      " 49  TSUN_ATTRIBUTES  557 non-null    object        \n",
      " 50  WT07             139 non-null    float64       \n",
      " 51  WT07_ATTRIBUTES  139 non-null    object        \n",
      " 52  WT09             56 non-null     float64       \n",
      " 53  WT09_ATTRIBUTES  56 non-null     object        \n",
      " 54  WT11             25 non-null     float64       \n",
      " 55  WT11_ATTRIBUTES  23 non-null     object        \n",
      " 56  WT13             2359 non-null   float64       \n",
      " 57  WT13_ATTRIBUTES  1663 non-null   object        \n",
      " 58  WT14             123 non-null    float64       \n",
      " 59  WT14_ATTRIBUTES  88 non-null     object        \n",
      " 60  WT15             4 non-null      float64       \n",
      " 61  WT15_ATTRIBUTES  4 non-null      object        \n",
      " 62  WT16             2354 non-null   float64       \n",
      " 63  WT16_ATTRIBUTES  1785 non-null   object        \n",
      " 64  WT18             7 non-null      float64       \n",
      " 65  WT18_ATTRIBUTES  7 non-null      object        \n",
      " 66  WT21             185 non-null    float64       \n",
      " 67  WT21_ATTRIBUTES  109 non-null    object        \n",
      " 68  WESD             3653 non-null   float64       \n",
      " 69  WT17             4 non-null      float64       \n",
      " 70  WV03             6 non-null      float64       \n",
      " 71  WESD_ATTRIBUTES  2192 non-null   object        \n",
      " 72  WT17_ATTRIBUTES  4 non-null      object        \n",
      " 73  WV03_ATTRIBUTES  3 non-null      object        \n",
      "dtypes: datetime64[ns](1), float64(37), object(36)\n",
      "memory usage: 10.0+ MB\n"
     ]
    }
   ],
   "source": [
    "### Loading the files holding weather summaries\n",
    "meteo_gal_2008_2012=pd.read_csv(path_header+path_meteo+'METEO_Galveston_2008_2012.csv')\n",
    "meteo_gal_2013_2020=pd.read_csv(path_header+path_meteo+'METEO_Galveston_2013_2020.csv')\n",
    "meteo_hob_2008_2012=pd.read_csv(path_header+path_meteo+'METEO_Houston_Hobby_2008_2012.csv')\n",
    "meteo_hob_2013_2020=pd.read_csv(path_header+path_meteo+'METEO_Houston_Hobby_2013_2020.csv')\n",
    "meteo_iah_2008_2012=pd.read_csv(path_header+path_meteo+'METEO_Houston_IAH_2008_2012.csv')\n",
    "meteo_iah_2013_2020=pd.read_csv(path_header+path_meteo+'METEO_Houston_IAH_2013_2020.csv')\n",
    "meteo_hob_1999_2001=pd.read_csv(path_header+path_meteo+'METEO_Houston_Hobby_1999_2001.csv')\n",
    "meteo_iah_1999_2001=pd.read_csv(path_header+path_meteo+'METEO_Houston_IAH_1999_2001.csv')\n",
    "meteo_gal_1999_2001=pd.read_csv(path_header+path_meteo+'METEO_Galveston_1999_2001.csv')\n",
    "\n",
    "### Converting the \"DATE\" column from object to datetime\n",
    "meteo_gal_2008_2012['DATE'] = pd.to_datetime(meteo_gal_2008_2012['DATE'])\n",
    "meteo_gal_2013_2020['DATE'] = pd.to_datetime(meteo_gal_2013_2020['DATE'])\n",
    "meteo_hob_2008_2012['DATE'] = pd.to_datetime(meteo_hob_2008_2012['DATE'])\n",
    "meteo_hob_2013_2020['DATE'] = pd.to_datetime(meteo_hob_2013_2020['DATE'])\n",
    "meteo_iah_2008_2012['DATE'] = pd.to_datetime(meteo_iah_2008_2012['DATE'])\n",
    "meteo_iah_2013_2020['DATE'] = pd.to_datetime(meteo_iah_2013_2020['DATE'])\n",
    "meteo_hob_1999_2001['DATE'] = pd.to_datetime(meteo_hob_1999_2001['DATE'])\n",
    "meteo_iah_1999_2001['DATE'] = pd.to_datetime(meteo_iah_1999_2001['DATE'])\n",
    "meteo_gal_1999_2001['DATE'] = pd.to_datetime(meteo_gal_1999_2001['DATE'])\n",
    "\n",
    "# Concatenating all dataframes in one called meteo_all\n",
    "meteo_all=pd.concat([meteo_gal_2008_2012,meteo_gal_2013_2020,meteo_gal_1999_2001,\n",
    "                     meteo_hob_2008_2012,meteo_hob_2013_2020, meteo_hob_1999_2001,\n",
    "                     meteo_iah_2008_2012,meteo_iah_2013_2020, meteo_iah_1999_2001])\n",
    "meteo_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately NOAA provides a key to decode the cryptic column names. The relevant columns are renamed and the dataframe is subset to create the final weather daily data table 'meteo'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rename columns I want to keep\n",
    "col_meteo={'STATION':'station_code', 'NAME':'station_name', 'LONGITUDE':'station_lon', 'LATITUDE':'station_lat', 'DATE':'date',  \n",
    "           'TAVG':'temp_avg','TMIN':'temp_min','TMAX':'temp_max','WT08':'smoke_haze','PRCP':'rain_prcp','WT16':'rain','WT03':'thunder',\n",
    "           'WT21':'fog_ground','WT02':'fog_heavy','WT01':'fog','AWND':'wind_avgspeed','PGTM':'wind_peak_gust_time','FMTM':'wind_time_fastest_mile',\n",
    "           'WSF2':'wind_fastest_2min','WSF5':'wind_fastest_5min','WDF2':'wind_fastest_2min_direction','WDF5':'wind_fastest_5min_direction',\n",
    "           'WT11':'wind_high_dmg','WT07':'dust_sand','WT13':'rain_mist','WT14':'drizzle','WT15':'drizzle_freezing','WT17':'rain_freezing',\n",
    "           'WT18':'snow_grains','SNOW':'snow','SNWD':'snow_depth','WT09':'snow_drifting','WT04':'snow_sleet','WT05':'rain_hail',\n",
    "           'WT06':'snow_glaze_rime','WT10':'wind_tornado'}\n",
    "meteo_all=meteo_all.rename(columns=col_meteo)\n",
    "\n",
    "### subsetting \n",
    "subsetlist=list({v for k, v in col_meteo.items()})\n",
    "subsetlist.sort()\n",
    "meteo=meteo_all[subsetlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further processing of the dataframe meteo is done in Section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Outdoor Air Quality:###\n",
    "This is the biggest chunk of the data collection process where 91 files from EPA and 36 files from TECQ will be concatenated or merged. To speedup the process, loading functions are being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function glob_load which use glob to gather all the filenames\n",
    "### in a folder and return the data into one dataframe\n",
    "def glob_load(path='C:\\\\Users\\\\',file_header='EPA',file_pollutant='CO'):\n",
    "    ### list of filenames\n",
    "    allfiles = glob.glob(path + file_header+'_'+file_pollutant+'_*.csv')\n",
    "    ### mydata list collect the data read by pd.read_csv\n",
    "    mydata = []\n",
    "    for filename in allfiles:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        mydata.append(df)\n",
    "    ### convert mydata list into a dataframe\n",
    "    mydataframe = pd.concat(mydata, axis=0, ignore_index=True)\n",
    "    ### return the dataframe\n",
    "    return mydataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using glob load to get all the data from the EPA csv files\n",
    "epa_co=glob_load(path=path_header+path_AQ+'\\\\',file_header='EPA',file_pollutant='CO')\n",
    "epa_no2=glob_load(path=path_header+path_AQ+'\\\\',file_header='EPA',file_pollutant='NO2')\n",
    "epa_so2=glob_load(path=path_header+path_AQ+'\\\\',file_header='EPA',file_pollutant='SO2')\n",
    "epa_ozone=glob_load(path=path_header+path_AQ+'\\\\',file_header='EPA',file_pollutant='OZONE')\n",
    "epa_pb=glob_load(path=path_header+path_AQ+'\\\\',file_header='EPA',file_pollutant='Pb')\n",
    "epa_pm25=glob_load(path=path_header+path_AQ+'\\\\',file_header='EPA',file_pollutant='PM2_5')\n",
    "epa_pm10=glob_load(path=path_header+path_AQ+'\\\\',file_header='EPA',file_pollutant='PM10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20242 entries, 0 to 20241\n",
      "Data columns (total 20 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   Date                               20242 non-null  object \n",
      " 1   Source                             20242 non-null  object \n",
      " 2   Site ID                            20242 non-null  int64  \n",
      " 3   POC                                20242 non-null  int64  \n",
      " 4   Daily Max 8-hour CO Concentration  20242 non-null  float64\n",
      " 5   UNITS                              20242 non-null  object \n",
      " 6   DAILY_AQI_VALUE                    20242 non-null  int64  \n",
      " 7   Site Name                          20242 non-null  object \n",
      " 8   DAILY_OBS_COUNT                    20242 non-null  int64  \n",
      " 9   PERCENT_COMPLETE                   20242 non-null  float64\n",
      " 10  AQS_PARAMETER_CODE                 20242 non-null  int64  \n",
      " 11  AQS_PARAMETER_DESC                 20242 non-null  object \n",
      " 12  CBSA_CODE                          20242 non-null  int64  \n",
      " 13  CBSA_NAME                          20242 non-null  object \n",
      " 14  STATE_CODE                         20242 non-null  int64  \n",
      " 15  STATE                              20242 non-null  object \n",
      " 16  COUNTY_CODE                        20242 non-null  int64  \n",
      " 17  COUNTY                             20242 non-null  object \n",
      " 18  SITE_LATITUDE                      20242 non-null  float64\n",
      " 19  SITE_LONGITUDE                     20242 non-null  float64\n",
      "dtypes: float64(4), int64(8), object(8)\n",
      "memory usage: 3.1+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 71195 entries, 0 to 71194\n",
      "Data columns (total 20 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   Date                                71195 non-null  object \n",
      " 1   Source                              71195 non-null  object \n",
      " 2   Site ID                             71195 non-null  int64  \n",
      " 3   POC                                 71195 non-null  int64  \n",
      " 4   Daily Max 1-hour NO2 Concentration  71195 non-null  float64\n",
      " 5   UNITS                               71195 non-null  object \n",
      " 6   DAILY_AQI_VALUE                     71195 non-null  int64  \n",
      " 7   Site Name                           71195 non-null  object \n",
      " 8   DAILY_OBS_COUNT                     71195 non-null  int64  \n",
      " 9   PERCENT_COMPLETE                    71195 non-null  float64\n",
      " 10  AQS_PARAMETER_CODE                  71195 non-null  int64  \n",
      " 11  AQS_PARAMETER_DESC                  71195 non-null  object \n",
      " 12  CBSA_CODE                           71195 non-null  int64  \n",
      " 13  CBSA_NAME                           71195 non-null  object \n",
      " 14  STATE_CODE                          71195 non-null  int64  \n",
      " 15  STATE                               71195 non-null  object \n",
      " 16  COUNTY_CODE                         71195 non-null  int64  \n",
      " 17  COUNTY                              71195 non-null  object \n",
      " 18  SITE_LATITUDE                       71195 non-null  float64\n",
      " 19  SITE_LONGITUDE                      71195 non-null  float64\n",
      "dtypes: float64(4), int64(8), object(8)\n",
      "memory usage: 10.9+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35641 entries, 0 to 35640\n",
      "Data columns (total 20 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   Date                                35641 non-null  object \n",
      " 1   Source                              35641 non-null  object \n",
      " 2   Site ID                             35641 non-null  int64  \n",
      " 3   POC                                 35641 non-null  int64  \n",
      " 4   Daily Max 1-hour SO2 Concentration  35641 non-null  float64\n",
      " 5   UNITS                               35641 non-null  object \n",
      " 6   DAILY_AQI_VALUE                     35641 non-null  int64  \n",
      " 7   Site Name                           35641 non-null  object \n",
      " 8   DAILY_OBS_COUNT                     35641 non-null  int64  \n",
      " 9   PERCENT_COMPLETE                    35641 non-null  float64\n",
      " 10  AQS_PARAMETER_CODE                  35641 non-null  int64  \n",
      " 11  AQS_PARAMETER_DESC                  35641 non-null  object \n",
      " 12  CBSA_CODE                           35641 non-null  int64  \n",
      " 13  CBSA_NAME                           35641 non-null  object \n",
      " 14  STATE_CODE                          35641 non-null  int64  \n",
      " 15  STATE                               35641 non-null  object \n",
      " 16  COUNTY_CODE                         35641 non-null  int64  \n",
      " 17  COUNTY                              35641 non-null  object \n",
      " 18  SITE_LATITUDE                       35641 non-null  float64\n",
      " 19  SITE_LONGITUDE                      35641 non-null  float64\n",
      "dtypes: float64(4), int64(8), object(8)\n",
      "memory usage: 5.4+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 91476 entries, 0 to 91475\n",
      "Data columns (total 20 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   Date                                  91476 non-null  object \n",
      " 1   Source                                91476 non-null  object \n",
      " 2   Site ID                               91476 non-null  int64  \n",
      " 3   POC                                   91476 non-null  int64  \n",
      " 4   Daily Max 8-hour Ozone Concentration  91476 non-null  float64\n",
      " 5   UNITS                                 91476 non-null  object \n",
      " 6   DAILY_AQI_VALUE                       91476 non-null  int64  \n",
      " 7   Site Name                             91476 non-null  object \n",
      " 8   DAILY_OBS_COUNT                       91476 non-null  int64  \n",
      " 9   PERCENT_COMPLETE                      91476 non-null  float64\n",
      " 10  AQS_PARAMETER_CODE                    91476 non-null  int64  \n",
      " 11  AQS_PARAMETER_DESC                    91476 non-null  object \n",
      " 12  CBSA_CODE                             91476 non-null  int64  \n",
      " 13  CBSA_NAME                             91476 non-null  object \n",
      " 14  STATE_CODE                            91476 non-null  int64  \n",
      " 15  STATE                                 91476 non-null  object \n",
      " 16  COUNTY_CODE                           91476 non-null  int64  \n",
      " 17  COUNTY                                91476 non-null  object \n",
      " 18  SITE_LATITUDE                         91476 non-null  float64\n",
      " 19  SITE_LONGITUDE                        91476 non-null  float64\n",
      "dtypes: float64(4), int64(8), object(8)\n",
      "memory usage: 14.0+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 656 entries, 0 to 655\n",
      "Data columns (total 20 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Date                         656 non-null    object \n",
      " 1   Source                       656 non-null    object \n",
      " 2   Site ID                      656 non-null    int64  \n",
      " 3   POC                          656 non-null    int64  \n",
      " 4   Daily Mean Pb Concentration  656 non-null    float64\n",
      " 5   UNITS                        656 non-null    object \n",
      " 6   DAILY_AQI_VALUE              656 non-null    object \n",
      " 7   Site Name                    656 non-null    object \n",
      " 8   DAILY_OBS_COUNT              656 non-null    int64  \n",
      " 9   PERCENT_COMPLETE             656 non-null    float64\n",
      " 10  AQS_PARAMETER_CODE           656 non-null    int64  \n",
      " 11  AQS_PARAMETER_DESC           656 non-null    object \n",
      " 12  CBSA_CODE                    656 non-null    int64  \n",
      " 13  CBSA_NAME                    656 non-null    object \n",
      " 14  STATE_CODE                   656 non-null    int64  \n",
      " 15  STATE                        656 non-null    object \n",
      " 16  COUNTY_CODE                  656 non-null    int64  \n",
      " 17  COUNTY                       656 non-null    object \n",
      " 18  SITE_LATITUDE                656 non-null    float64\n",
      " 19  SITE_LONGITUDE               656 non-null    float64\n",
      "dtypes: float64(4), int64(7), object(9)\n",
      "memory usage: 102.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51311 entries, 0 to 51310\n",
      "Data columns (total 20 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   Date                            51311 non-null  object \n",
      " 1   Source                          51311 non-null  object \n",
      " 2   Site ID                         51311 non-null  int64  \n",
      " 3   POC                             51311 non-null  int64  \n",
      " 4   Daily Mean PM2.5 Concentration  51311 non-null  float64\n",
      " 5   UNITS                           51311 non-null  object \n",
      " 6   DAILY_AQI_VALUE                 51311 non-null  int64  \n",
      " 7   Site Name                       51311 non-null  object \n",
      " 8   DAILY_OBS_COUNT                 51311 non-null  int64  \n",
      " 9   PERCENT_COMPLETE                51311 non-null  float64\n",
      " 10  AQS_PARAMETER_CODE              51311 non-null  int64  \n",
      " 11  AQS_PARAMETER_DESC              51311 non-null  object \n",
      " 12  CBSA_CODE                       51311 non-null  int64  \n",
      " 13  CBSA_NAME                       51311 non-null  object \n",
      " 14  STATE_CODE                      51311 non-null  int64  \n",
      " 15  STATE                           51311 non-null  object \n",
      " 16  COUNTY_CODE                     51311 non-null  int64  \n",
      " 17  COUNTY                          51311 non-null  object \n",
      " 18  SITE_LATITUDE                   51311 non-null  float64\n",
      " 19  SITE_LONGITUDE                  51311 non-null  float64\n",
      "dtypes: float64(4), int64(8), object(8)\n",
      "memory usage: 7.8+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7344 entries, 0 to 7343\n",
      "Data columns (total 20 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   Date                           7344 non-null   object \n",
      " 1   Source                         7344 non-null   object \n",
      " 2   Site ID                        7344 non-null   int64  \n",
      " 3   POC                            7344 non-null   int64  \n",
      " 4   Daily Mean PM10 Concentration  7344 non-null   int64  \n",
      " 5   UNITS                          7344 non-null   object \n",
      " 6   DAILY_AQI_VALUE                7344 non-null   int64  \n",
      " 7   Site Name                      7344 non-null   object \n",
      " 8   DAILY_OBS_COUNT                7344 non-null   int64  \n",
      " 9   PERCENT_COMPLETE               7344 non-null   float64\n",
      " 10  AQS_PARAMETER_CODE             7344 non-null   int64  \n",
      " 11  AQS_PARAMETER_DESC             7344 non-null   object \n",
      " 12  CBSA_CODE                      7344 non-null   int64  \n",
      " 13  CBSA_NAME                      7344 non-null   object \n",
      " 14  STATE_CODE                     7344 non-null   int64  \n",
      " 15  STATE                          7344 non-null   object \n",
      " 16  COUNTY_CODE                    7344 non-null   int64  \n",
      " 17  COUNTY                         7344 non-null   object \n",
      " 18  SITE_LATITUDE                  7344 non-null   float64\n",
      " 19  SITE_LONGITUDE                 7344 non-null   float64\n",
      "dtypes: float64(3), int64(9), object(8)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "epa_co.info()\n",
    "epa_no2.info()\n",
    "epa_so2.info()\n",
    "epa_ozone.info()\n",
    "epa_pb.info()\n",
    "epa_pm25.info()\n",
    "epa_pm10.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EPA columns look good. The columns 'DATE' need to be converted to a datetime object. Some columns will have to be renamed to facilitate the merging of all these dataframes into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dictionaries to rename columns \n",
    "col_epa_ozone={'Date':'date', 'Source':'ozone_8hr_source','Site ID':'site_id','POC':'ozone_8hr_poc',\n",
    "               'Daily Max 8-hour Ozone Concentration':'ozone_8hr_max', 'UNITS':'ozone_8hr_units',\n",
    "               'Site Name':'site_name', 'DAILY_OBS_COUNT':'ozone_8hr_obs_count', \n",
    "               'SITE_LATITUDE':'site_lat','SITE_LONGITUDE':'site_lon'}\n",
    "\n",
    "col_epa_co={'Date':'date', 'Source':'co_8hr_source','POC':'co_8hr_poc','Site ID':'site_id',\n",
    "            'Daily Max 8-hour CO Concentration':'co_8hr_max', 'UNITS':'co_8hr_units',\n",
    "            'Site Name':'site_name', 'DAILY_OBS_COUNT':'co_8hr_obs_count',\n",
    "            'SITE_LATITUDE':'site_lat', 'SITE_LONGITUDE':'site_lon'}\n",
    "     \n",
    "    \n",
    "col_epa_no2={'Date':'date', 'Source':'no2_1hr_source','Site ID':'site_id','POC':'no2_1hr_poc',\n",
    "             'Daily Max 1-hour NO2 Concentration':'no2_1hr_max', 'UNITS':'no2_1hr_units',\n",
    "             'Site Name':'site_name', 'DAILY_OBS_COUNT':'no2_1hr_obs_count',\n",
    "             'SITE_LATITUDE':'site_lat','SITE_LONGITUDE':'site_lon'}\n",
    "\n",
    "col_epa_so2={'Date':'date', 'Source':'so2_1hr_source','Site ID':'site_id','POC':'so2_1hr_poc',\n",
    "             'Daily Max 1-hour SO2 Concentration':'so2_1hr_max', 'UNITS':'so2_1hr_units',  \n",
    "             'Site Name':'site_name', 'DAILY_OBS_COUNT':'so2_1hr_obs_count', \n",
    "             'SITE_LATITUDE':'site_lat','SITE_LONGITUDE':'site_lon'}\n",
    "    \n",
    "col_epa_pb={'Date':'date', 'Source':'pb_24hr_source','Site ID':'site_id','POC':'pb_24hr_poc',\n",
    "            'Daily Mean Pb Concentration':'pb_24hr_mean','UNITS':'pb_24hr_units',\n",
    "            'Site Name':'site_name', 'DAILY_OBS_COUNT':'pb_24hr_obs_count', \n",
    "            'SITE_LATITUDE':'site_lat','SITE_LONGITUDE':'site_lon'}\n",
    "    \n",
    "col_epa_pm25={'Date':'date', 'Source':'pm25_24hr_source','Site ID':'site_id','POC':'pm25_24hr_poc',\n",
    "              'Daily Mean PM2.5 Concentration':'pm25_24hr_mean','UNITS':'pm25_24hr_units',\n",
    "              'Site Name':'site_name', 'DAILY_OBS_COUNT':'pm25_24hr_obs_count',\n",
    "              'SITE_LATITUDE':'site_lat','SITE_LONGITUDE':'site_lon'}\n",
    "\n",
    "col_epa_pm10={'Date':'date','Source':'pm10_24hr_source','Site ID':'site_id','POC':'pm10_24hr_poc',\n",
    "              'Daily Mean PM10 Concentration':'pm10_24hr_mean', 'UNITS':'pm10_24hr_units', \n",
    "              'Site Name':'site_name', 'DAILY_OBS_COUNT':'pm10_24hr_obs_count',\n",
    "              'SITE_LATITUDE':'site_lat','SITE_LONGITUDE':'site_lon'}\n",
    "\n",
    "### rename columns\n",
    "repa_ozone=epa_ozone.rename(columns=col_epa_ozone)\n",
    "repa_co=epa_co.rename(columns=col_epa_co)\n",
    "repa_no2=epa_no2.rename(columns=col_epa_no2)\n",
    "repa_so2=epa_so2.rename(columns=col_epa_so2)\n",
    "repa_pb=epa_pb.rename(columns=col_epa_pb)\n",
    "repa_pm25=epa_pm25.rename(columns=col_epa_pm25)\n",
    "repa_pm10=epa_pm10.rename(columns=col_epa_pm10)\n",
    "\n",
    "# convert date to datetime\n",
    "repa_ozone['date'] = pd.to_datetime(repa_ozone['date'])\n",
    "repa_co['date'] = pd.to_datetime(repa_co['date'])\n",
    "repa_no2['date'] = pd.to_datetime(repa_no2['date'])\n",
    "repa_so2['date'] = pd.to_datetime(repa_so2['date'])\n",
    "repa_pb['date'] = pd.to_datetime(repa_pb['date'])\n",
    "repa_pm25['date'] = pd.to_datetime(repa_pm25['date'])\n",
    "repa_pm10['date'] = pd.to_datetime(repa_pm10['date'])\n",
    "\n",
    "### subsetting\n",
    "newcol_ozone=col_epa_ozone.values()\n",
    "subset_ozone=[name for name in newcol_ozone]\n",
    "frepa_ozone=repa_ozone[subset_ozone]\n",
    "\n",
    "newcol_co=col_epa_co.values()\n",
    "subset_co=[name for name in newcol_co]\n",
    "frepa_co=repa_co[subset_co]\n",
    "\n",
    "newcol_no2=col_epa_no2.values()\n",
    "subset_no2=[name for name in newcol_no2]\n",
    "frepa_no2=repa_no2[subset_no2]\n",
    "\n",
    "newcol_so2=col_epa_so2.values()\n",
    "subset_so2=[name for name in newcol_so2]\n",
    "frepa_so2=repa_so2[subset_so2]\n",
    "\n",
    "newcol_pb=col_epa_pb.values()\n",
    "subset_pb=[name for name in newcol_pb]\n",
    "frepa_pb=repa_pb[subset_pb]\n",
    "\n",
    "newcol_pm25=col_epa_pm25.values()\n",
    "subset_pm25=[name for name in newcol_pm25]\n",
    "frepa_pm25=repa_pm25[subset_pm25]\n",
    "\n",
    "newcol_pm10=col_epa_pm10.values()\n",
    "subset_pm10=[name for name in newcol_pm10]\n",
    "frepa_pm10=repa_pm10[subset_pm10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge the epa dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge in order ozone, co, no2, so2, pb, pm10, pm25\n",
    "epa1=pd.merge(frepa_ozone,frepa_co,how='outer',on=['date','site_id','site_name','site_lat','site_lon'])\n",
    "epa2=pd.merge(epa1,frepa_no2,how='outer',on=['date','site_id','site_name','site_lat','site_lon'])\n",
    "epa3=pd.merge(epa2,frepa_so2,how='outer',on=['date','site_id','site_name','site_lat','site_lon'])\n",
    "epa4=pd.merge(epa3,frepa_pb,how='outer',on=['date','site_id','site_name','site_lat','site_lon'])\n",
    "epa5=pd.merge(epa4,frepa_pm10,how='outer',on=['date','site_id','site_name','site_lat','site_lon'])\n",
    "epa=pd.merge(epa5,frepa_pm25,how='outer',on=['date','site_id','site_name','site_lat','site_lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second chunk of air quality data comes from TECQ and is composed of txt files which all have a header summary. To speed up loading of multiples files, another glob function is being used, called 'glob*_*load*_*txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3417: DtypeWarning: Columns (22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3337: DtypeWarning: Columns (107) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "### Function glob_load_txt which use glob to gather all the filenames\n",
    "### in a folder and return the data into one dataframe\n",
    "def glob_load_txt(path='C:\\\\Users\\\\',file_header='EPA',file_pollutant='CO'):\n",
    "    ### list of filenames\n",
    "    allfiles = glob.glob(path + file_header+'_'+file_pollutant+'_*.txt')\n",
    "    ### mydata list collect the data read by pd.read_table\n",
    "    mydata = []\n",
    "    for filename in allfiles:\n",
    "        ### read_table must skip the header summary, hence the skiprows=9\n",
    "        df = pd.read_table(filename,header=0,skiprows=9,sep='\\t')\n",
    "        mydata.append(df)\n",
    "    ### convert mydata list into a dataframe\n",
    "    mydataframe = pd.concat(mydata, axis=0, ignore_index=True)\n",
    "    ### return the dataframe\n",
    "    return mydataframe\n",
    "\n",
    "### Using glob_load_txt to load the data\n",
    "tamis_co=glob_load_txt(path=path_header+path_AQ+'\\\\',file_header='TAMIS',file_pollutant='CO')\n",
    "tamis_no2=glob_load_txt(path=path_header+path_AQ+'\\\\',file_header='TAMIS',file_pollutant='NO2')\n",
    "tamis_so2=glob_load_txt(path=path_header+path_AQ+'\\\\',file_header='TAMIS',file_pollutant='SO2')\n",
    "tamis_ozone=glob_load_txt(path=path_header+path_AQ+'\\\\',file_header='TAMIS',file_pollutant='Ozone')\n",
    "\n",
    "### Using just read_table for single files\n",
    "tamis_pm10_24hr=pd.read_table(path_header+path_AQ+'\\\\TAMIS_PM10_24HR.txt',header=0,skiprows=9,sep='\\t')\n",
    "tamis_pm10_detail=pd.read_table(path_header+path_AQ+'\\\\TAMIS_PM10_2008_2020.txt',header=0,skiprows=9,sep='\\t')\n",
    "tamis_pm25=pd.read_table(path_header+path_AQ+'\\\\TAMIS_PM2_5_24HR_2008_2020.txt',header=0,skiprows=9,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1302264 entries, 0 to 1302263\n",
      "Data columns (total 19 columns):\n",
      " #   Column                Non-Null Count    Dtype  \n",
      "---  ------                --------------    -----  \n",
      " 0   State                 1302264 non-null  object \n",
      " 1   Region                1302264 non-null  object \n",
      " 2   County                1302264 non-null  object \n",
      " 3   City                  1302264 non-null  object \n",
      " 4   AQS Code              1302264 non-null  int64  \n",
      " 5   Site Name             1302264 non-null  object \n",
      " 6   Latitude              1302264 non-null  float64\n",
      " 7   Longitude             1302264 non-null  float64\n",
      " 8   Year                  1302264 non-null  int64  \n",
      " 9   Month                 1302264 non-null  int64  \n",
      " 10  Day                   1302264 non-null  int64  \n",
      " 11  Date                  1302264 non-null  int64  \n",
      " 12  Start Hour            1302264 non-null  int64  \n",
      " 13  Start Minute          1302264 non-null  int64  \n",
      " 14  Start Time            1302264 non-null  object \n",
      " 15  Duration              1302264 non-null  object \n",
      " 16  Sampler Type          1302264 non-null  object \n",
      " 17  SOC                   1302264 non-null  int64  \n",
      " 18  Ozone (ppbv) <44201>  1302264 non-null  float64\n",
      "dtypes: float64(3), int64(8), object(8)\n",
      "memory usage: 188.8+ MB\n"
     ]
    }
   ],
   "source": [
    "### An example of output\n",
    "tamis_ozone.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TECQ*/*Tamis dataframes require some adjustement before merging. \n",
    "- The 'Date' column has to be converted into datetime. \n",
    "- Lead (Pb) concentrations are included in tamis_pm10_detail and therefore has to be retrieved.\n",
    "- TECQ provides hourly measurements of ozone and carbon monoxide concentrations. The maximum values for each 8 hour period has to be extracted. This task will be done later on because there some interest at keeping the hourly data for modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting date format from yyyymmdd to datetime yyyy-mm-dd\n",
    "\n",
    "tamis_ozone['date'] = pd.to_datetime(tamis_ozone['Date'], format='%Y%m%d').dt.strftime(\"%Y-%m-%d\")\n",
    "tamis_ozone['date']=pd.to_datetime(tamis_ozone['date'])\n",
    "tamis_co['date'] = pd.to_datetime(tamis_co['Date'], format='%Y%m%d').dt.strftime(\"%Y-%m-%d\")\n",
    "tamis_co['date']=pd.to_datetime(tamis_co['date'])\n",
    "tamis_no2['date'] = pd.to_datetime(tamis_no2['Date'], format='%Y%m%d').dt.strftime(\"%Y-%m-%d\")\n",
    "tamis_no2['date']=pd.to_datetime(tamis_no2['date'])\n",
    "tamis_so2['date'] = pd.to_datetime(tamis_so2['Date'], format='%Y%m%d').dt.strftime(\"%Y-%m-%d\")\n",
    "tamis_so2['date']=pd.to_datetime(tamis_so2['date'])\n",
    "tamis_pm10_24hr['date'] = pd.to_datetime(tamis_pm10_24hr['Date'], format='%Y%m%d').dt.strftime(\"%Y-%m-%d\")\n",
    "tamis_pm10_24hr['date']=pd.to_datetime(tamis_pm10_24hr['date'])\n",
    "tamis_pm25['date'] = pd.to_datetime(tamis_pm25['Date'], format='%Y%m%d').dt.strftime(\"%Y-%m-%d\")\n",
    "tamis_pm25['date']=pd.to_datetime(tamis_pm25['date'])\n",
    "tamis_pm10_detail['date'] = pd.to_datetime(tamis_pm10_detail['Date'], format='%Y%m%d').dt.strftime(\"%Y-%m-%d\")\n",
    "tamis_pm10_detail['date']=pd.to_datetime(tamis_pm10_detail['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  renaming columns\n",
    "\n",
    "col_tamis_ozone={'City':'city', 'AQS Code':'site_code', 'Site Name':'site_name',\n",
    "                 'Latitude':'site_lat', 'Longitude':'site_lon','Start Hour':'start_hour',\n",
    "                 'Start Time':'start_time','Duration':'duration','SOC':'ozone_soc',\n",
    "                 'Ozone (ppbv) <44201>':'ozone_1hr', 'date':'date'}\n",
    "\n",
    "col_tamis_co={'City':'city', 'AQS Code':'site_code', 'Site Name':'site_name',\n",
    "                 'Latitude':'site_lat', 'Longitude':'site_lon','Start Hour':'start_hour',\n",
    "                 'Start Time':'start_time','Duration':'duration','SOC':'co_soc',\n",
    "                 'Carbon Monoxide (ppmv) <42101>':'co_1hr', 'date':'date'}\n",
    "\n",
    "col_tamis_no2={'City':'city', 'AQS Code':'site_code', 'Site Name':'site_name',\n",
    "                 'Latitude':'site_lat', 'Longitude':'site_lon','Start Hour':'start_hour',\n",
    "                 'Start Time':'start_time','Duration':'duration','SOC':'no2_soc',\n",
    "                 'Nitrogen Dioxide (NO2) (ppbv) <42602>':'no2_1hr', 'date':'date'}\n",
    "\n",
    "col_tamis_so2={'City':'city', 'AQS Code':'site_code', 'Site Name':'site_name',\n",
    "                 'Latitude':'site_lat', 'Longitude':'site_lon','Start Hour':'start_hour',\n",
    "                 'Start Time':'start_time','Duration':'duration','SOC':'so2_soc',\n",
    "                 'Sulfur Dioxide (ppbv) <42401>':'so2_1hr', 'date':'date'}\n",
    "\n",
    "col_tamis_pm10_24hr={'City':'city', 'AQS Code':'site_code', 'Site Name':'site_name',\n",
    "                 'Latitude':'site_lat', 'Longitude':'site_lon','Start Hour':'start_hour',\n",
    "                 'Start Time':'start_time','Duration':'duration','SOC':'pm10_24hr_soc',\n",
    "                     'Pm10 - Lc (ug/m3 (LC)) <85101>':'pm10_24hr','Pm10 Total 0-10um Stp (ug/m3 (25 C)) <81102>':'pm10_total_24hr',\n",
    "                     'Pm10-2.5 - Local Conditions (ug/m3 (LC)) <86101>':'pm10_minus_pm2_5_24hr','date':'date'}\n",
    "\n",
    "col_tamis_pm25={'City':'city', 'AQS Code':'site_code', 'Site Name':'site_name',\n",
    "                 'Latitude':'site_lat', 'Longitude':'site_lon','Start Hour':'start_hour',\n",
    "                 'Start Time':'start_time','Duration':'duration','SOC':'pm25_soc',\n",
    "                 'Pm2.5 - Local Conditions (ug/m3 (LC)) <88101>':'pm25_24hr', 'date':'date'}\n",
    "     \n",
    "col_tamis_pm10_detail={'City':'city', 'AQS Code':'site_code', 'Site Name':'site_name',\n",
    "                 'Latitude':'site_lat', 'Longitude':'site_lon','Start Hour':'start_hour',\n",
    "                 'Start Time':'start_time','Duration':'duration',\n",
    "                 'Lead Pm10 Stp (ug/m3 (25 C)) <82128>':'pb_24hr', 'date':'date'}\n",
    "\n",
    "rtamis_ozone=tamis_ozone.rename(columns=col_tamis_ozone)\n",
    "rtamis_co=tamis_co.rename(columns=col_tamis_co)\n",
    "rtamis_no2=tamis_no2.rename(columns=col_tamis_no2)\n",
    "rtamis_so2=tamis_so2.rename(columns=col_tamis_so2)\n",
    "rtamis_pm10_24hr=tamis_pm10_24hr.rename(columns=col_tamis_pm10_24hr)\n",
    "rtamis_pm25=tamis_pm25.rename(columns=col_tamis_pm25)\n",
    "rtamis_pm10_detail=tamis_pm10_detail.rename(columns=col_tamis_pm10_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1302264 entries, 0 to 1302263\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count    Dtype         \n",
      "---  ------      --------------    -----         \n",
      " 0   city        1302264 non-null  object        \n",
      " 1   site_code   1302264 non-null  int64         \n",
      " 2   site_name   1302264 non-null  object        \n",
      " 3   site_lat    1302264 non-null  float64       \n",
      " 4   site_lon    1302264 non-null  float64       \n",
      " 5   start_hour  1302264 non-null  int64         \n",
      " 6   start_time  1302264 non-null  object        \n",
      " 7   duration    1302264 non-null  object        \n",
      " 8   ozone_soc   1302264 non-null  int64         \n",
      " 9   ozone_1hr   1302264 non-null  float64       \n",
      " 10  date        1302264 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(3), int64(3), object(4)\n",
      "memory usage: 109.3+ MB\n"
     ]
    }
   ],
   "source": [
    "### subsetting\n",
    "newcol_tamis_ozone=col_tamis_ozone.values()\n",
    "sub_ozone=[name for name in newcol_tamis_ozone]\n",
    "frtamis_ozone=rtamis_ozone[sub_ozone]\n",
    "\n",
    "newcol_tamis_co=col_tamis_co.values()\n",
    "sub_co=[name for name in newcol_tamis_co]\n",
    "frtamis_co=rtamis_co[sub_co]\n",
    "\n",
    "newcol_tamis_no2=col_tamis_no2.values()\n",
    "sub_no2=[name for name in newcol_tamis_no2]\n",
    "frtamis_no2=rtamis_no2[sub_no2]\n",
    "\n",
    "newcol_tamis_so2=col_tamis_so2.values()\n",
    "sub_so2=[name for name in newcol_tamis_so2]\n",
    "frtamis_so2=rtamis_so2[sub_so2]\n",
    "\n",
    "newcol_tamis_pm10_24hr=col_tamis_pm10_24hr.values()\n",
    "sub_pm10_24hr=[name for name in newcol_tamis_pm10_24hr]\n",
    "frtamis_pm10_24hr=rtamis_pm10_24hr[sub_pm10_24hr]\n",
    "\n",
    "newcol_tamis_pm25=col_tamis_pm25.values()\n",
    "sub_pm25=[name for name in newcol_tamis_pm25]\n",
    "frtamis_pm25=rtamis_pm25[sub_pm25]\n",
    "\n",
    "### subsetting Lead data\n",
    "newcol_tamis_pm10_detail=col_tamis_pm10_detail.values()\n",
    "sub_pm10_detail=[name for name in newcol_tamis_pm10_detail]\n",
    "frtamis_pb=rtamis_pm10_detail[sub_pm10_detail]\n",
    "frtamis_ozone.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation of the Air Quality Index requires, among other parameters, a 8-hour measurement of the concentration of ozone. EPA ozone data contains the maximum 8 hour ozone measurement for each day. Let's take the maximum measurement of ozone from tamis for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 55526 entries, 0 to 1302131\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   city           55526 non-null  object        \n",
      " 1   site_code      55526 non-null  int64         \n",
      " 2   site_name      55526 non-null  object        \n",
      " 3   site_lat       55526 non-null  float64       \n",
      " 4   site_lon       55526 non-null  float64       \n",
      " 5   date           55526 non-null  datetime64[ns]\n",
      " 6   ozone_8hr_max  55526 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(3), int64(1), object(2)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "### Pick the max ozone for each day\n",
    "ozone_max=frtamis_ozone.groupby(['date','site_code'], as_index=False).agg({'ozone_1hr':'max'})\n",
    "ozone_max['date'] = pd.to_datetime(ozone_max['date'])\n",
    "ozone_max=ozone_max.rename(columns={'ozone_1hr':'ozone_8hr_max'})\n",
    "\n",
    "### Merge the ozone_8hr_max column to the initial dataframe\n",
    "mfrtamis_ozone=pd.merge(frtamis_ozone,ozone_max,how='left',on=['date','site_code'])\n",
    "mfrtamis_ozone.columns\n",
    "\n",
    "### Drop useless columns and duplicates\n",
    "mfrtamis_ozone.drop(columns=['start_hour', 'start_time', 'duration', 'ozone_soc', 'ozone_1hr'], inplace=True)\n",
    "mfrtamis_ozone.drop_duplicates(inplace=True)\n",
    "mfrtamis_ozone.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation trimmed down the ozone dataset from tamis from 1,302,131 entries to 55,526. Further processing of the tamis dataframes will occur in section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Monitoring Stations Information:###\n",
    "Information about each weather station are extracted to match their location to interesting attributes such as landuse, source emission...etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "### Monitoring stations from EPA\n",
    "epa_stations_ozone=frepa_ozone[['site_id','site_name','site_lat','site_lon']]\n",
    "epa_stations_co=frepa_co[['site_id','site_name','site_lat','site_lon']]\n",
    "epa_stations_no2=frepa_no2[['site_id','site_name','site_lat','site_lon']]\n",
    "epa_stations_so2=frepa_so2[['site_id','site_name','site_lat','site_lon']]\n",
    "epa_stations_pm10=frepa_pm10[['site_id','site_name','site_lat','site_lon']]\n",
    "epa_stations_pm25=frepa_pm25[['site_id','site_name','site_lat','site_lon']]\n",
    "epa_stations_pb=frepa_pb[['site_id','site_name','site_lat','site_lon']]\n",
    "\n",
    "epa_stations_ozone['poll']='ozone'\n",
    "epa_stations_co['poll']='co'\n",
    "epa_stations_no2['poll']='no2'\n",
    "epa_stations_so2['poll']='so2'\n",
    "epa_stations_pm10['poll']='pm10'\n",
    "epa_stations_pm25['poll']='pm25'\n",
    "epa_stations_pb['poll']='pb'\n",
    "\n",
    "epa_stations_ozone.drop_duplicates(inplace=True)\n",
    "epa_stations_co.drop_duplicates(inplace=True)\n",
    "epa_stations_no2.drop_duplicates(inplace=True)\n",
    "epa_stations_so2.drop_duplicates(inplace=True)\n",
    "epa_stations_pm10.drop_duplicates(inplace=True)\n",
    "epa_stations_pm25.drop_duplicates(inplace=True)\n",
    "epa_stations_pb.drop_duplicates(inplace=True)\n",
    "\n",
    "epa_stations=pd.concat([epa_stations_ozone,epa_stations_co,epa_stations_no2,epa_stations_so2,epa_stations_pm10,epa_stations_pm25,epa_stations_pb], axis=0)\n",
    "epa_stations.drop_duplicates(inplace=True)\n",
    "\n",
    "### Monitoring Stations from TECQ/Tamis\n",
    "tamis_station_ozone=mfrtamis_ozone[['city','site_code','site_lat','site_lon']]\n",
    "tamis_station_co=frtamis_co[['city','site_code','site_lat','site_lon']]\n",
    "tamis_station_no2=frtamis_no2[['city','site_code','site_lat','site_lon']]\n",
    "tamis_station_so2=frtamis_so2[['city','site_code','site_lat','site_lon']]\n",
    "tamis_station_pm10_24hr=frtamis_pm10_24hr[['city','site_code','site_lat','site_lon']]\n",
    "tamis_station_pm25=frtamis_pm25[['city','site_code','site_lat','site_lon']]\n",
    "tamis_station_pb=frtamis_pb[['city','site_code','site_lat','site_lon']]\n",
    "\n",
    "tamis_station_ozone.drop_duplicates(inplace=True)\n",
    "tamis_station_co.drop_duplicates(inplace=True)\n",
    "tamis_station_no2.drop_duplicates(inplace=True)\n",
    "tamis_station_so2.drop_duplicates(inplace=True)\n",
    "tamis_station_pm10_24hr.drop_duplicates(inplace=True)\n",
    "tamis_station_pm25.drop_duplicates(inplace=True)\n",
    "tamis_station_pb.drop_duplicates(inplace=True)\n",
    "\n",
    "tamis_station_ozone['poll']='ozone'\n",
    "tamis_station_co['poll']='co'\n",
    "tamis_station_no2['poll']='no2'\n",
    "tamis_station_so2['poll']='so2'\n",
    "tamis_station_pm10_24hr['poll']='pm10'\n",
    "tamis_station_pm25['poll']='pm25'\n",
    "tamis_station_pb['poll']='pb'\n",
    "\n",
    "tamis_stations=pd.concat([tamis_station_ozone,tamis_station_co,tamis_station_no2,tamis_station_so2,tamis_station_pm10_24hr,tamis_station_pm25,tamis_station_pb],axis=0)\n",
    "tamis_stations.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exported xlsx files containing stations information that is plotted with all the shape files provided by the HGA tool to extract population data, landuse, roads...etc... The completed tables will be reimported in the EDA jupyter notebook. The mapping work will be*/*is stored in the folder '00*_*StuffAndThings' when polished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3. Data Definition and Cleaning:## \n",
    "###3.1. Location of Air Quality Stations:\n",
    "Logically the cleaning has to start by looking at duplicate stations between EPA and TECQ before looking at duplicate within the air quality data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 4) (58, 4)\n"
     ]
    }
   ],
   "source": [
    "epa_solo=epa_stations.drop('poll',axis=1)\n",
    "tamis_solo=tamis_stations.drop('poll',axis=1)\n",
    "print(epa_solo.shape,tamis_solo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 79 EPA stations and 58 TECQ stations. As TECQ reports data annually to EPA, there might be an overlap. Let's use the option 'indicator' of pd.merge to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### comparing epa_stations to tamis_stations\n",
    "allstations= epa_stations.merge(tamis_stations, indicator=True,how='outer')\n",
    "allstations._merge[allstations._merge=='both'].count()                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated by the presence of 'both', there are 22 redundent stations. Let's find out which ones and store this information in the dataframe 'redund_stations'. This dataframe will be used later during the cleaning of air quality data. Here is a preview of redund_stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>site_lat</th>\n",
       "      <th>site_lon</th>\n",
       "      <th>poll</th>\n",
       "      <th>city</th>\n",
       "      <th>site_code</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>482010047.0</td>\n",
       "      <td>Lang</td>\n",
       "      <td>29.834167</td>\n",
       "      <td>-95.489167</td>\n",
       "      <td>ozone</td>\n",
       "      <td>Houston</td>\n",
       "      <td>482010047.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>482010051.0</td>\n",
       "      <td>Houston Croquet</td>\n",
       "      <td>29.623889</td>\n",
       "      <td>-95.474167</td>\n",
       "      <td>ozone</td>\n",
       "      <td>Houston</td>\n",
       "      <td>482010051.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>482010062.0</td>\n",
       "      <td>Houston Monroe</td>\n",
       "      <td>29.625556</td>\n",
       "      <td>-95.267222</td>\n",
       "      <td>ozone</td>\n",
       "      <td>Houston</td>\n",
       "      <td>482010062.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>482010066.0</td>\n",
       "      <td>Houston Westhollow</td>\n",
       "      <td>29.723333</td>\n",
       "      <td>-95.635833</td>\n",
       "      <td>ozone</td>\n",
       "      <td>Houston</td>\n",
       "      <td>482010066.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>482010075.0</td>\n",
       "      <td>Houston Texas Avenue</td>\n",
       "      <td>29.752778</td>\n",
       "      <td>-95.350278</td>\n",
       "      <td>ozone</td>\n",
       "      <td>Houston</td>\n",
       "      <td>482010075.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        site_id             site_name   site_lat   site_lon   poll     city  \\\n",
       "7   482010047.0                  Lang  29.834167 -95.489167  ozone  Houston   \n",
       "8   482010051.0       Houston Croquet  29.623889 -95.474167  ozone  Houston   \n",
       "10  482010062.0        Houston Monroe  29.625556 -95.267222  ozone  Houston   \n",
       "11  482010066.0    Houston Westhollow  29.723333 -95.635833  ozone  Houston   \n",
       "13  482010075.0  Houston Texas Avenue  29.752778 -95.350278  ozone  Houston   \n",
       "\n",
       "      site_code _merge  \n",
       "7   482010047.0   both  \n",
       "8   482010051.0   both  \n",
       "10  482010062.0   both  \n",
       "11  482010066.0   both  \n",
       "13  482010075.0   both  "
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redund_stations=allstations.loc[allstations['_merge']=='both']\n",
    "redund_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another fact to keep in mind, as shown below, is that some stations measure multiple types of pollutant, and some measure only one pollutant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>poll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site_lat</th>\n",
       "      <th>site_lon</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29.043759</th>\n",
       "      <th>-95.472946</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.254474</th>\n",
       "      <th>-94.861289</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.384444</th>\n",
       "      <th>-94.930833</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.385234</th>\n",
       "      <th>-94.931520</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.520443</th>\n",
       "      <th>-95.392509</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.583047</th>\n",
       "      <th>-95.015544</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.583333</th>\n",
       "      <th>-95.105000</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.589444</th>\n",
       "      <th>-95.353611</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.623889</th>\n",
       "      <th>-95.474167</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.625556</th>\n",
       "      <th>-95.267222</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.670025</th>\n",
       "      <th>-95.128508</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.686389</th>\n",
       "      <th>-95.294722</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">29.695729</th>\n",
       "      <th>-95.499219</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-95.499219</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.716483</th>\n",
       "      <th>-95.201330</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.717600</th>\n",
       "      <th>-95.341400</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.721600</th>\n",
       "      <th>-95.492650</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.723333</th>\n",
       "      <th>-95.635833</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">29.733726</th>\n",
       "      <th>-95.257593</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-95.257593</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.735120</th>\n",
       "      <th>-95.315600</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.735129</th>\n",
       "      <th>-95.315583</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.752778</th>\n",
       "      <th>-95.350278</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.758889</th>\n",
       "      <th>-95.079444</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.764788</th>\n",
       "      <th>-95.178538</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">29.767997</th>\n",
       "      <th>-95.220582</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-95.220582</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.770698</th>\n",
       "      <th>-95.031232</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.778333</th>\n",
       "      <th>-95.538056</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.802707</th>\n",
       "      <th>-95.125495</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.810556</th>\n",
       "      <th>-95.806111</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.814530</th>\n",
       "      <th>-95.387690</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.823319</th>\n",
       "      <th>-94.983786</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">29.828086</th>\n",
       "      <th>-95.284096</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-95.284096</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.833056</th>\n",
       "      <th>-95.656944</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.834167</th>\n",
       "      <th>-95.489167</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.858611</th>\n",
       "      <th>-95.160278</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">29.901036</th>\n",
       "      <th>-95.326137</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-95.326137</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.961944</th>\n",
       "      <th>-95.235000</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30.011667</th>\n",
       "      <th>-95.522500</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30.038056</th>\n",
       "      <th>-95.381111</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30.039524</th>\n",
       "      <th>-95.673951</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30.058460</th>\n",
       "      <th>-95.189751</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30.350302</th>\n",
       "      <th>-95.425128</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      poll\n",
       "site_lat  site_lon        \n",
       "29.043759 -95.472946     2\n",
       "29.254474 -94.861289     3\n",
       "29.384444 -94.930833     1\n",
       "29.385234 -94.931520     1\n",
       "29.520443 -95.392509     2\n",
       "29.583047 -95.015544     4\n",
       "29.583333 -95.105000     1\n",
       "29.589444 -95.353611     1\n",
       "29.623889 -95.474167     2\n",
       "29.625556 -95.267222     4\n",
       "29.670025 -95.128508     7\n",
       "29.686389 -95.294722     5\n",
       "29.695729 -95.499219     2\n",
       "          -95.499219     2\n",
       "29.716483 -95.201330     1\n",
       "29.717600 -95.341400     1\n",
       "29.721600 -95.492650     1\n",
       "29.723333 -95.635833     3\n",
       "29.733726 -95.257593     6\n",
       "          -95.257593     7\n",
       "29.735120 -95.315600     2\n",
       "29.735129 -95.315583     2\n",
       "29.752778 -95.350278     3\n",
       "29.758889 -95.079444     2\n",
       "29.764788 -95.178538     2\n",
       "29.767997 -95.220582     2\n",
       "          -95.220582     4\n",
       "29.770698 -95.031232     1\n",
       "29.778333 -95.538056     1\n",
       "29.802707 -95.125495     3\n",
       "29.810556 -95.806111     1\n",
       "29.814530 -95.387690     3\n",
       "29.823319 -94.983786     2\n",
       "29.828086 -95.284096     2\n",
       "          -95.284096     2\n",
       "29.833056 -95.656944     1\n",
       "29.834167 -95.489167     5\n",
       "29.858611 -95.160278     1\n",
       "29.901036 -95.326137     5\n",
       "          -95.326137     6\n",
       "29.961944 -95.235000     1\n",
       "30.011667 -95.522500     1\n",
       "30.038056 -95.381111     1\n",
       "30.039524 -95.673951     2\n",
       "30.058460 -95.189751     1\n",
       "30.350302 -95.425128     3"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#byloc=pd.melt(allstations,id_vars=['poll'],value_vars=['site_lat','site_lon'])\n",
    "#byloc.head()\n",
    "byloc=allstations.groupby(['site_lat','site_lon']).agg({'poll':'count'})\n",
    "byloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize the available information regarding air quality stations:\n",
    "- the dataframes epa_stations and tamis_stations contain the information about the stations.\n",
    "- the dataframe 'allstations' contain both datasets and a column '_merge' which indicates duplicate stations between both sets.\n",
    "- 22 stations are common to both EPA and TECQ datasets. There are saved under 'redund_stations'\n",
    "- the column 'poll' provides the type of measurement done at each station.\n",
    "- some stations measure multiple pollutants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.2. Meteorology Data:\n",
    "Houston is a very large city. The weather tends to vary from one side to the other. With this in mind, the weather data was collected from three far apart stations: Hobby airport to the North, IAH to the South, and the Galveston station to the East.This hypothesis will have to be chekced after cleaning the data.\n",
    "The method 'describe' shows that some data is numerical (i.e. temp, wind speed), some data is boolean (format 1 or Nan) and some data is informational (i.e. direction of wind). The dataset needs to be simplified into fewer columns showing the occurrence or not of an average meteorological event (i.e. rain, snow, wind, smoke, fog), some important numerical values (i.e. temperature, wind speed, rain amount) and of course date and the location of the weather stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drizzle</th>\n",
       "      <th>drizzle_freezing</th>\n",
       "      <th>dust_sand</th>\n",
       "      <th>fog</th>\n",
       "      <th>fog_ground</th>\n",
       "      <th>fog_heavy</th>\n",
       "      <th>rain</th>\n",
       "      <th>rain_freezing</th>\n",
       "      <th>rain_hail</th>\n",
       "      <th>rain_mist</th>\n",
       "      <th>...</th>\n",
       "      <th>thunder</th>\n",
       "      <th>wind_avgspeed</th>\n",
       "      <th>wind_fastest_2min</th>\n",
       "      <th>wind_fastest_2min_direction</th>\n",
       "      <th>wind_fastest_5min</th>\n",
       "      <th>wind_fastest_5min_direction</th>\n",
       "      <th>wind_high_dmg</th>\n",
       "      <th>wind_peak_gust_time</th>\n",
       "      <th>wind_time_fastest_mile</th>\n",
       "      <th>wind_tornado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>123.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>6536.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>2354.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>2359.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2710.0</td>\n",
       "      <td>17264.000000</td>\n",
       "      <td>17296.000000</td>\n",
       "      <td>17296.000000</td>\n",
       "      <td>17198.000000</td>\n",
       "      <td>17197.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7455.000000</td>\n",
       "      <td>6385.000000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.646697</td>\n",
       "      <td>18.961153</td>\n",
       "      <td>166.340772</td>\n",
       "      <td>24.307856</td>\n",
       "      <td>165.760307</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1371.543662</td>\n",
       "      <td>1575.029287</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.802111</td>\n",
       "      <td>5.556988</td>\n",
       "      <td>95.502780</td>\n",
       "      <td>7.224481</td>\n",
       "      <td>95.631063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>538.049793</td>\n",
       "      <td>1245.755801</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.820000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>19.900000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1123.000000</td>\n",
       "      <td>1214.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1434.000000</td>\n",
       "      <td>1513.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.960000</td>\n",
       "      <td>21.900000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1705.000000</td>\n",
       "      <td>1730.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.910000</td>\n",
       "      <td>91.900000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>221.900000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       drizzle  drizzle_freezing  dust_sand     fog  fog_ground  fog_heavy  \\\n",
       "count    123.0               4.0      139.0  6536.0       185.0      908.0   \n",
       "mean       1.0               1.0        1.0     1.0         1.0        1.0   \n",
       "std        0.0               0.0        0.0     0.0         0.0        0.0   \n",
       "min        1.0               1.0        1.0     1.0         1.0        1.0   \n",
       "25%        1.0               1.0        1.0     1.0         1.0        1.0   \n",
       "50%        1.0               1.0        1.0     1.0         1.0        1.0   \n",
       "75%        1.0               1.0        1.0     1.0         1.0        1.0   \n",
       "max        1.0               1.0        1.0     1.0         1.0        1.0   \n",
       "\n",
       "         rain  rain_freezing  rain_hail  rain_mist  ...  thunder  \\\n",
       "count  2354.0            4.0      575.0     2359.0  ...   2710.0   \n",
       "mean      1.0            1.0        1.0        1.0  ...      1.0   \n",
       "std       0.0            0.0        0.0        0.0  ...      0.0   \n",
       "min       1.0            1.0        1.0        1.0  ...      1.0   \n",
       "25%       1.0            1.0        1.0        1.0  ...      1.0   \n",
       "50%       1.0            1.0        1.0        1.0  ...      1.0   \n",
       "75%       1.0            1.0        1.0        1.0  ...      1.0   \n",
       "max       1.0            1.0        1.0        1.0  ...      1.0   \n",
       "\n",
       "       wind_avgspeed  wind_fastest_2min  wind_fastest_2min_direction  \\\n",
       "count   17264.000000       17296.000000                 17296.000000   \n",
       "mean        8.646697          18.961153                   166.340772   \n",
       "std         3.802111           5.556988                    95.502780   \n",
       "min         0.000000           2.900000                    10.000000   \n",
       "25%         5.820000          15.000000                   110.000000   \n",
       "50%         8.050000          17.900000                   150.000000   \n",
       "75%        10.960000          21.900000                   200.000000   \n",
       "max        36.910000          91.900000                   360.000000   \n",
       "\n",
       "       wind_fastest_5min  wind_fastest_5min_direction  wind_high_dmg  \\\n",
       "count       17198.000000                 17197.000000           25.0   \n",
       "mean           24.307856                   165.760307            1.0   \n",
       "std             7.224481                    95.631063            0.0   \n",
       "min             0.000000                     0.000000            1.0   \n",
       "25%            19.900000                   110.000000            1.0   \n",
       "50%            23.000000                   150.000000            1.0   \n",
       "75%            28.000000                   200.000000            1.0   \n",
       "max           221.900000                   360.000000            1.0   \n",
       "\n",
       "       wind_peak_gust_time  wind_time_fastest_mile  wind_tornado  \n",
       "count          7455.000000             6385.000000          10.0  \n",
       "mean           1371.543662             1575.029287           1.0  \n",
       "std             538.049793             1245.755801           0.0  \n",
       "min               0.000000                0.000000           1.0  \n",
       "25%            1123.000000             1214.000000           1.0  \n",
       "50%            1434.000000             1513.000000           1.0  \n",
       "75%            1705.000000             1730.000000           1.0  \n",
       "max            2359.000000             9999.000000           1.0  \n",
       "\n",
       "[8 rows x 33 columns]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4569: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  method=method,\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "### Cleaning meteo\n",
    "dicofog={'fog':'new','fog_ground':'new','fog_heavy':'new'}\n",
    "meteo['fog_all']=meteo.groupby(dicofog,axis=1).sum()\n",
    "meteo['fog_all'].replace([1,2,3],1,inplace=True)\n",
    "\n",
    "dicosnow={'snow_sleet':'new','snow_grains':'new','drizzle_freezing':'new','snow':'new',\n",
    "        'snow_drifting':'new','snow_glaze_rime':'new','rain_freezing':'new'}\n",
    "meteo['snow_all']=meteo.groupby(dicofog,axis=1).sum()\n",
    "meteo['snow_all'].replace([1,2,3,4,5,6,7,8],1,inplace=True)\n",
    "\n",
    "dicorain={'rain':'new','drizzle':'new','rain_mist':'new','drizzle':'new',\n",
    "        'rain_hail':'new'}\n",
    "meteo['rain_all']=meteo.groupby(dicofog,axis=1).sum()\n",
    "meteo['rain_all'].replace([1,2,3,4,5,6,7,8],1,inplace=True)\n",
    "\n",
    "dicowind_dmg={'wind_tornado':'new','wind_high_dmg':'new'}\n",
    "meteo['wind_dmg']=meteo.groupby(dicofog,axis=1).sum()\n",
    "meteo['wind_dmg'].replace([1,2,3,4,5,6,7,8],1,inplace=True)\n",
    "\n",
    "meteo_simple=meteo[['date', 'dust_sand','rain_prcp', 'smoke_haze', 'station_code',\n",
    "       'station_lat', 'station_lon', 'station_name', 'temp_avg', 'temp_max',\n",
    "       'temp_min', 'thunder', 'wind_avgspeed', 'wind_fastest_2min',\n",
    "       'fog_all', 'snow_all', 'rain_all', 'wind_dmg']]\n",
    "\n",
    "### Replace Nan by 0 in no numerical columns\n",
    "meteo_simple['dust_sand'] = meteo_simple['dust_sand'].fillna(0)\n",
    "meteo_simple['smoke_haze'] = meteo_simple['smoke_haze'].fillna(0)\n",
    "meteo_simple['thunder'] = meteo_simple['thunder'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temperature columns have some Nan values. It looks like it would be better to drop the rows without temp_min and temp_max, and to replace the NaN values in the temp*_*avg column by calculating an average from temp*_*min and temp*_*max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteo_simple['temp_min'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteo_simple['temp_max'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3975"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteo_simple['temp_avg'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                 0\n",
       "dust_sand            0\n",
       "rain_prcp            0\n",
       "smoke_haze           0\n",
       "station_code         0\n",
       "station_lat          0\n",
       "station_lon          0\n",
       "station_name         0\n",
       "temp_max             0\n",
       "temp_min             0\n",
       "thunder              0\n",
       "wind_avgspeed        0\n",
       "wind_fastest_2min    0\n",
       "fog_all              0\n",
       "snow_all             0\n",
       "rain_all             0\n",
       "wind_dmg             0\n",
       "temp_avg             0\n",
       "wind_all             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Drop NaN in temperature columns\n",
    "meteo_simple = meteo_simple.dropna(axis=0, subset=['temp_max'])\n",
    "meteo_simple = meteo_simple.dropna(axis=0, subset=['temp_min'])\n",
    "\n",
    "### Fill up\n",
    "meteo_simple['temp_avg_where'] = np.where(meteo_simple.temp_avg.isnull(),(meteo_simple.temp_max+meteo_simple.temp_min)/2,meteo_simple.temp_avg)\n",
    "\n",
    "### Create a boolean wind column\n",
    "meteo_simple['wind_all'] = np.where(meteo_simple.wind_avgspeed.isnull(), '0', '1')\n",
    "\n",
    "### Remove remaining NaN\n",
    "meteo_simple['wind_avgspeed'] = meteo_simple['wind_avgspeed'].fillna(0)\n",
    "meteo_simple['wind_fastest_2min'] = meteo_simple['wind_fastest_2min'].fillna(0)\n",
    "meteo_simple['rain_prcp'] = meteo_simple['rain_prcp'].fillna(0)\n",
    "\n",
    "### Drop 'temp_avg' and replace it by 'temp_avg_where'\n",
    "\n",
    "meteo_simple.drop(['temp_avg'], axis=1,inplace=True)\n",
    "meteo_simple.rename(columns={'temp_avg_where': 'temp_avg'},inplace=True)\n",
    "meteo_simple.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.3. Riopa Data Cleaning: ###\n",
    "There are some NaN and NaT values in the riopa dataset. NaT are simply due to the fact that no measurement of the particular pollutant was made in the respective row. That is fine because the main 'date' column has no missing values. The 9 missing temp*_*mean can be recalculated using temp*_*min and temp*_*max. Rows with missing pm25 will have to be dropped.\n",
    "As part of the sampling is done on people, which is of no interest in this project, the riopa dataset has to be subset on airtype (= indoor and outdoor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linkid                       0\n",
       "sampleid                     0\n",
       "homeid                       0\n",
       "airtype                      0\n",
       "pm25                        34\n",
       "validity                   724\n",
       "comments_pm25               16\n",
       "date_start_pm25              0\n",
       "date_end_pm25                0\n",
       "date                         0\n",
       "landuse_class              265\n",
       "tempid                     205\n",
       "visitnumber                205\n",
       "date_temp                  205\n",
       "location                   205\n",
       "ambient_temp_c             205\n",
       "ambient_rh                 205\n",
       "airexrate                  359\n",
       "comment_aer                830\n",
       "date_start_aer             359\n",
       "date_end_aer               359\n",
       "temp_dry                    47\n",
       "dew_point                   47\n",
       "temp_wet                    47\n",
       "rh                          47\n",
       "census_group_block_code    265\n",
       "home_lat                   350\n",
       "home_long                  350\n",
       "geoid                      350\n",
       "dtype: int64"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "riopa.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linkid                       0\n",
       "sampleid                     0\n",
       "homeid                       0\n",
       "airtype                      0\n",
       "pm25                         0\n",
       "date_start_pm25              0\n",
       "date_end_pm25                0\n",
       "date                         0\n",
       "landuse_class                0\n",
       "ambient_temp_c               0\n",
       "ambient_rh                   0\n",
       "airexrate                    0\n",
       "temp_dry                    46\n",
       "dew_point                   46\n",
       "temp_wet                    46\n",
       "rh                          46\n",
       "census_group_block_code    254\n",
       "home_lat                   332\n",
       "home_long                  332\n",
       "geoid                      332\n",
       "dtype: int64"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Cleaning riopa\n",
    "\n",
    "### Drop rows with missing pm25\n",
    "riopa.dropna(subset=['pm25'],inplace=True)\n",
    "\n",
    "### Replacing nan by 0\n",
    "riopa['comment_aer'] = riopa['comment_aer'].fillna(0)\n",
    "riopa['airexrate'] = riopa['airexrate'].fillna(0)\n",
    "riopa['ambient_rh'] = riopa['ambient_rh'].fillna(0)\n",
    "riopa['ambient_temp_c'] = riopa['ambient_temp_c'].fillna(0)\n",
    "riopa['location'] = riopa['location'].fillna(0)\n",
    "riopa['visitnumber'] = riopa['visitnumber'].fillna(0)\n",
    "riopa['tempid'] = riopa['tempid'].fillna(0)\n",
    "riopa['validity'] = riopa['validity'].fillna(0)\n",
    "riopa['comments_pm25'] = riopa['comments_pm25'].fillna(0)\n",
    "riopa['landuse_class'] = riopa['landuse_class'].fillna('tbd')\n",
    "\n",
    "### subset to keep only interesting columns (i.e. removing multiple dates)\n",
    "riopa_slim=riopa.drop(['validity','comments_pm25','tempid','visitnumber','date_temp','location','comment_aer','date_start_aer','date_end_aer'], axis=1,inplace=False)\n",
    "\n",
    "### It looks good\n",
    "riopa_slim.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 126 entries, 8 to 833\n",
      "Data columns (total 20 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   linkid                   126 non-null    object        \n",
      " 1   sampleid                 126 non-null    int64         \n",
      " 2   homeid                   126 non-null    object        \n",
      " 3   airtype                  126 non-null    object        \n",
      " 4   pm25                     126 non-null    float64       \n",
      " 5   date_start_pm25          126 non-null    datetime64[ns]\n",
      " 6   date_end_pm25            126 non-null    datetime64[ns]\n",
      " 7   date                     126 non-null    datetime64[ns]\n",
      " 8   landuse_class            126 non-null    object        \n",
      " 9   ambient_temp_c           126 non-null    float64       \n",
      " 10  ambient_rh               126 non-null    float64       \n",
      " 11  airexrate                126 non-null    float64       \n",
      " 12  temp_dry                 125 non-null    float64       \n",
      " 13  dew_point                125 non-null    float64       \n",
      " 14  temp_wet                 125 non-null    float64       \n",
      " 15  rh                       125 non-null    float64       \n",
      " 16  census_group_block_code  0 non-null      float64       \n",
      " 17  home_lat                 0 non-null      float64       \n",
      " 18  home_long                0 non-null      float64       \n",
      " 19  geoid                    0 non-null      float64       \n",
      "dtypes: datetime64[ns](3), float64(12), int64(1), object(4)\n",
      "memory usage: 20.7+ KB\n"
     ]
    }
   ],
   "source": [
    "### subset outdoor, indoor and leaving 'person' behind\n",
    "riopa_outdoor=riopa_slim.loc[riopa.airtype=='OUTDOOR']\n",
    "riopa_indoor=riopa_slim.loc[riopa.airtype=='INDOOR']\n",
    "riopa_outdoor.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.4. Air Quality Data Cleaning: ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values in the 'frepa*_*pollutant' individual dataframes. There are null values in the merged epa dataframe because each air quality station may not record all the pollutants. That is expected and fine.\n",
    "It looks like there are no null values in the 'frtamis*_*pollutant' individual frames but this is because all the null values are question marks. The function 'theanswer' takes a dataframe and a column name to drop the rows for which the column contains a question mark and convert the column into a numerical column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city          0\n",
       "site_code     0\n",
       "site_name     0\n",
       "site_lat      0\n",
       "site_lon      0\n",
       "start_hour    0\n",
       "start_time    0\n",
       "duration      0\n",
       "pb_24hr       0\n",
       "date          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### frtamis_pb displays no null values\n",
    "frtamis_pb.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>site_code</th>\n",
       "      <th>site_name</th>\n",
       "      <th>site_lat</th>\n",
       "      <th>site_lon</th>\n",
       "      <th>start_hour</th>\n",
       "      <th>start_time</th>\n",
       "      <th>duration</th>\n",
       "      <th>pb_24hr</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Houston</td>\n",
       "      <td>482011035</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>29.733726</td>\n",
       "      <td>-95.257593</td>\n",
       "      <td>0</td>\n",
       "      <td>00:00</td>\n",
       "      <td>24 HOURS</td>\n",
       "      <td>?</td>\n",
       "      <td>2008-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Houston</td>\n",
       "      <td>482011035</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>29.733726</td>\n",
       "      <td>-95.257593</td>\n",
       "      <td>0</td>\n",
       "      <td>00:00</td>\n",
       "      <td>24 HOURS</td>\n",
       "      <td>.003</td>\n",
       "      <td>2008-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Houston</td>\n",
       "      <td>482010066</td>\n",
       "      <td>Houston Westhollow</td>\n",
       "      <td>29.723333</td>\n",
       "      <td>-95.635833</td>\n",
       "      <td>0</td>\n",
       "      <td>00:00</td>\n",
       "      <td>24 HOURS</td>\n",
       "      <td>?</td>\n",
       "      <td>2008-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Houston</td>\n",
       "      <td>482010047</td>\n",
       "      <td>Lang</td>\n",
       "      <td>29.834167</td>\n",
       "      <td>-95.489167</td>\n",
       "      <td>0</td>\n",
       "      <td>00:00</td>\n",
       "      <td>24 HOURS</td>\n",
       "      <td>?</td>\n",
       "      <td>2008-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Houston</td>\n",
       "      <td>482010024</td>\n",
       "      <td>Houston Aldine</td>\n",
       "      <td>29.901036</td>\n",
       "      <td>-95.326137</td>\n",
       "      <td>0</td>\n",
       "      <td>00:00</td>\n",
       "      <td>24 HOURS</td>\n",
       "      <td>?</td>\n",
       "      <td>2008-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      city  site_code           site_name   site_lat   site_lon  start_hour  \\\n",
       "0  Houston  482011035             Clinton  29.733726 -95.257593           0   \n",
       "1  Houston  482011035             Clinton  29.733726 -95.257593           0   \n",
       "2  Houston  482010066  Houston Westhollow  29.723333 -95.635833           0   \n",
       "3  Houston  482010047                Lang  29.834167 -95.489167           0   \n",
       "4  Houston  482010024      Houston Aldine  29.901036 -95.326137           0   \n",
       "\n",
       "  start_time  duration pb_24hr       date  \n",
       "0      00:00  24 HOURS       ? 2008-01-01  \n",
       "1      00:00  24 HOURS    .003 2008-01-01  \n",
       "2      00:00  24 HOURS       ? 2008-01-01  \n",
       "3      00:00  24 HOURS       ? 2008-01-01  \n",
       "4      00:00  24 HOURS       ? 2008-01-01  "
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### although frtamis_pb contains ? in the pb_24hr column\n",
    "frtamis_pb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4167: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:68: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return op(a, b)\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4167: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:68: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return op(a, b)\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4167: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "### remove all question marks with the function 'theanswer'\n",
    "\n",
    "def theanswer(df,col):\n",
    "    ''' function drops rows where col has a ?'''\n",
    "    dfcond=df[df[col]=='?'].index\n",
    "    df.drop(dfcond,inplace=True)\n",
    "    ''' function converts column to numeric'''    \n",
    "    df[col]=pd.to_numeric(df[col])\n",
    "    return df\n",
    "\n",
    "ctamis_pm10=theanswer(frtamis_pm10_24hr,'pm10_24hr')\n",
    "ctamis_pm10=theanswer(frtamis_pm10_24hr,'pm10_total_24hr')\n",
    "ctamis_pm10=theanswer(frtamis_pm10_24hr,'pm10_minus_pm2_5_24hr')\n",
    "ctamis_pm25=theanswer(frtamis_pm25,'pm25_24hr')\n",
    "ctamis_pb=theanswer(frtamis_pb,'pb_24hr')\n",
    "\n",
    "### naming dataframes without ? in the data as \"clean\" like the dataframes above\n",
    "ctamis_ozone=mfrtamis_ozone\n",
    "ctamis_no2=frtamis_no2\n",
    "ctamis_so2=frtamis_so2\n",
    "ctamis_co=frtamis_co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4. Wrapping up:##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are ready for EDA.The method .describe() provides basics statistics for each dataset or individual columns. Additional work is required to get an understanding of the spatial distribution of the data. The geospatial work is done in another notebook and will be introduced in the EDA notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to save all the dataframes under '00*_*SavedDataFrames'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print out all DSs for EDA\n",
    "\n",
    "epa_stations.to_excel(path_header+path_df+'stations_epa.xlsx',sheet_name='epa_stations')\n",
    "tamis_stations.to_excel(path_header+path_df+'stations_tamis.xlsx',sheet_name='tamis_stations')\n",
    "allstations.to_excel(path_header+path_df+'stations_epatamis.xlsx',sheet_name='epa_tamis')\n",
    "\n",
    "meteo_stations=meteo_simple[['station_name','station_lat','station_lon']].drop_duplicates()\n",
    "meteo_stations.to_excel(path_header+path_df+'stations_all.xlsx',sheet_name='meteo')\n",
    "meteo_stations.to_excel(path_header+path_df+'stations_meteo.xlsx',sheet_name='meteo')\n",
    "\n",
    "riopa_slim.to_excel(path_header+path_df+'riopa_slim.xlsx',sheet_name='riopa_slim')\n",
    "riopa_outdoor.to_excel(path_header+path_df+'riopa_outdoor.xlsx',sheet_name='riopa_outdoor')\n",
    "riopa_indoor.to_excel(path_header+path_df+'riopa_indoor.xlsx',sheet_name='riopa_indoor')\n",
    "\n",
    "meteo_simple.to_excel(path_header+path_df+'meteo_simple.xlsx',sheet_name='meteo_simple')\n",
    "\n",
    "\n",
    "frepa_ozone.to_excel(path_header+path_df+'epa_ozone.xlsx',sheet_name='epa_ozone')\n",
    "frepa_co.to_excel(path_header+path_df+'epa_co.xlsx',sheet_name='epa_co')\n",
    "frepa_no2.to_excel(path_header+path_df+'epa_no2.xlsx',sheet_name='epa_no2')\n",
    "frepa_so2.to_excel(path_header+path_df+'epa_so2.xlsx',sheet_name='epa_so2')\n",
    "frepa_pm10.to_excel(path_header+path_df+'epa_pm10.xlsx',sheet_name='epa_pm10')\n",
    "frepa_pm25.to_excel(path_header+path_df+'epa_pm25.xlsx',sheet_name='epa_pm25')\n",
    "frepa_pb.to_excel(path_header+path_df+'epa_pb.xlsx',sheet_name='epa_pb')\n",
    "epa.to_excel(path_header+path_df+'epa.xlsx',sheet_name='d_wrangling')\n",
    "\n",
    "ctamis_ozone.to_excel(path_header+path_df+'tamis_ozone.xlsx',sheet_name='tamis_ozone')\n",
    "ctamis_co.to_excel(path_header+path_df+'tamis_co.xlsx',sheet_name='tamis_co')\n",
    "ctamis_no2.to_excel(path_header+path_df+'tamis_no2.xlsx',sheet_name='tamis_no2')\n",
    "ctamis_so2.to_excel(path_header+path_df+'tamis_so2.xlsx',sheet_name='tamis_so2')\n",
    "ctamis_pm10.to_excel(path_header+path_df+'tamis_pm10.xlsx',sheet_name='tamis_pm10')\n",
    "ctamis_pm25.to_excel(path_header+path_df+'tamis_pm25.xlsx',sheet_name='tamis_pm25')\n",
    "ctamis_pb.to_excel(path_header+path_df+'tamis_pb.xlsx',sheet_name='tamis_pb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
