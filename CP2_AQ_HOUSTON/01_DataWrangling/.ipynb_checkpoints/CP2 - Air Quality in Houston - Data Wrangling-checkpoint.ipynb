{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality in Houston - Step 1: Data Wrangling#\n",
    "\n",
    "## 1. Aim of the project and origin of the data##\n",
    "\n",
    "### 1.1. The Problem:###\n",
    "What is the future of indoor and outdoor air quality in Houston and its impact on Houstonians’ health?\n",
    "\n",
    "Air quality has been a concern for Houston’s officials and population for several years. Houston’ s legendary around-the-clock traffic jam, its growing population, its humid subtropical climate conditions,   and the sprawling potpourri of pollutants released by refineries and chemical plants have made Space City’s air hard to breathe for a lot of Houstonians. In 2018, The Mayor's Task Force on the Health Effects of Air Pollution has identified 12 pollutants as definite health risks for Houstonians, the main one being Ozone.\n",
    "\n",
    "In 2020, the plastic industry is growing rapidly, freeways are being widened to allow for more traffic to flow through, and more people are moving in. Houston  is currently the 5th largest metro population in the US with 6,997,384 inhabitants and is predicted to host 8.7 millions inhabitants by 2028 according to the Texas Demographic Center.  \n",
    "\n",
    "Where is the air quality headed?\n",
    "\n",
    "The aim of this capstone project is to predict the indoor and outdoor air quality in Houston for each upcoming decades up to 2050 using daily air data summaries, known potential drivers of air quality, and the city development forecast (i.e. population growth, change in land use...) from the Houston-Galveston Area Council. The impact of air quality on health will be presented by an overlay of AQI (Air Quality Index) calculated data and ELS (Effects Screening Levels). The analysis focuses on 6 pollutants of concern for Houston, namely Ozone (O3), Sulfur Dioxide (SO2), Carbon Monoxide (CO), Nitrogen Dioxide (NO2), and particulate matter (PM2.5 and PM10). In this project Air Quality refers to pollutant concentrations in the air.\n",
    "\n",
    "\n",
    "### 1.2. The Data:###\n",
    "All the datasets used in this capstone were available online between 09/01/2020 and 09/15/2020 from the following websites:\n",
    "\n",
    "- Relationships of Indoor, Outdoor, and Personal Air (RIOPA) dataset:\n",
    "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/7UBE7P&version=1.0\n",
    "\n",
    "- Daily Pollutant concentration measurements for Houston and effect screening levels from the Texas Commision On Environmental Quality (TECQ): https://www.tceq.texas.gov/ \n",
    "\n",
    "- Daily Pollutant concentration measurements for Houston from the U.S. Environmental Protection Agency: https://www.epa.gov/\n",
    "\n",
    "- Weather daily summaries: https://www.noaa.gov/\n",
    "\n",
    "- Land use data, population data and forecast from the Houston-Galveston Area Council: http://www.h-gac.com/home/default.aspx\n",
    "\n",
    "- Road surface data: https://www.txdot.gov/inside-txdot/division/transportation-planning/roadway-inventory.html\n",
    "\n",
    "- Traffic count, road size data: https://cohgis-mycity.opendata.arcgis.com/datasets/trafficcounts-opendata-wm/data\n",
    "\n",
    "\n",
    "### 1.3. Data and File Location On Github:###\n",
    "This project is hosted on Github in \"Aurenkeelin18/TheFoxerine/CP2_AQ_HOUSTON\". The folder is organized using subfolders:\n",
    "- '00*_*OriginalData' contains all the original data files subdivided by role. 'AQ' contains air quality data and ancillary reports from TECQ and EPA. 'METEO' contains weather data from NOAA. 'RIOPA' contains the data from the indoor/outdoor stud from the RIOPA team.\n",
    "- '00*_*SavedDataframes' contains dataframes saved in excel files.\n",
    "- '00*_*StuffAndThings' contains images, maps or other miscellaneous items that were used in the project.\n",
    "- '00*_*ZeCollection' contains clean copies of piece of coding used in this project that felt worth setting aside for future usage (i.e. functions, code for mapping something, awesome looking plots...)\n",
    "- '01*_*DataWrangling' contains saved dataframes (pre-and post cleaning) and jupyter notebooks associated to the data wrangling.\n",
    "\n",
    "Additional subfolders will be added as the project progresses through the DSM following the same nomenclature - i.e. '02*_*EDA', '03*_*Modelling'...- as well as the final report and presentation.\n",
    "\n",
    "### 1.4. Getting Started###\n",
    "Below are found the libraries used for data wrangling as well as the path to all subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict,OrderedDict, namedtuple\n",
    "import json\n",
    "\n",
    "path_header='C:\\\\Users\\\\Anne\\\\Documents\\\\GIT\\\\TheFoxerine\\\\'\n",
    "path_df='CP2_AQ_HOUSTON\\\\00_SavedDataframes\\\\'\n",
    "path_riopa='CP2_AQ_HOUSTON\\\\00_OriginalData\\\\RIOPA\\\\'\n",
    "path_meteo='CP2_AQ_HOUSTON\\\\00_OriginalData\\\\METEO\\\\'\n",
    "path_AQ='CP2_AQ_HOUSTON\\\\00_OriginalData\\\\AQ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Data Collection##\n",
    "###2.1. Indoor/Outdoor Air Quality:###\n",
    "\n",
    "In this section the relevant csv files from the RIOPA dataset are loaded and merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1138 entries, 0 to 1137\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   LinkID         1138 non-null   object \n",
      " 1   SampleID       1138 non-null   int64  \n",
      " 2   HomeID         1138 non-null   object \n",
      " 3   Type           1138 non-null   object \n",
      " 4   PM25mass       1112 non-null   float64\n",
      " 5   Validity       171 non-null    float64\n",
      " 6   Mass_mg        1135 non-null   float64\n",
      " 7   Volume_m3      1117 non-null   float64\n",
      " 8   comments       1106 non-null   object \n",
      " 9   datestarted    1138 non-null   object \n",
      " 10  dateended      1138 non-null   object \n",
      " 11  PM25mass_flag  8 non-null      object \n",
      "dtypes: float64(4), int64(1), object(7)\n",
      "memory usage: 106.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1148 entries, 0 to 1147\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   LinkID      1148 non-null   object \n",
      " 1   ID          1148 non-null   int64  \n",
      " 2   HomeID      1148 non-null   object \n",
      " 3   Visit       1148 non-null   int64  \n",
      " 4   Start_Date  1148 non-null   object \n",
      " 5   End_Date    1148 non-null   object \n",
      " 6   Location    1148 non-null   object \n",
      " 7   temp_c      1148 non-null   float64\n",
      " 8   avg_rh      927 non-null    float64\n",
      " 9   source      408 non-null    float64\n",
      " 10  comments    719 non-null    object \n",
      "dtypes: float64(3), int64(2), object(6)\n",
      "memory usage: 98.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 414 entries, 0 to 413\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   LinkID     414 non-null    object\n",
      " 1   GRID_CODE  414 non-null    int64 \n",
      " 2   Class      414 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 9.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 533 entries, 0 to 532\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   LinkID      533 non-null    object \n",
      " 1   ID          533 non-null    int64  \n",
      " 2   HomeID      533 non-null    object \n",
      " 3   AER         524 non-null    float64\n",
      " 4   comment     14 non-null     object \n",
      " 5   Start_Date  533 non-null    object \n",
      " 6   End_Date    533 non-null    object \n",
      " 7   AER_flag    509 non-null    object \n",
      "dtypes: float64(1), int64(1), object(6)\n",
      "memory usage: 33.4+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9265 entries, 0 to 9264\n",
      "Data columns (total 18 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   LinkID                  9265 non-null   object \n",
      " 1   Wban_number             9265 non-null   int64  \n",
      " 2   datestarted             9255 non-null   object \n",
      " 3   dateended               9255 non-null   object \n",
      " 4   avg_Dry_bulb_temp       9255 non-null   float64\n",
      " 5   avg_Dew_point_temp      9255 non-null   float64\n",
      " 6   avg_Wet_bulb_temp       9255 non-null   float64\n",
      " 7   avg_RH                  9255 non-null   float64\n",
      " 8   avg_WS                  9255 non-null   float64\n",
      " 9   avg_Val_wind_char       9255 non-null   float64\n",
      " 10  avg_Pressure            9255 non-null   float64\n",
      " 11  avg_Sea_Level_Pressure  9255 non-null   float64\n",
      " 12  avg_Precipitation       9255 non-null   float64\n",
      " 13  avg_Sky_conditions      9255 non-null   object \n",
      " 14  avg_Visibility          9255 non-null   object \n",
      " 15  avg_Weather_type        9255 non-null   object \n",
      " 16  avg_Wind_Gust           9255 non-null   object \n",
      " 17  avg_WD                  9255 non-null   float64\n",
      "dtypes: float64(10), int64(1), object(7)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1461 entries, 0 to 1460\n",
      "Data columns (total 39 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   STATION    1461 non-null   object \n",
      " 1   NAME       1461 non-null   object \n",
      " 2   LATITUDE   1461 non-null   float64\n",
      " 3   LONGITUDE  1461 non-null   float64\n",
      " 4   ELEVATION  1461 non-null   float64\n",
      " 5   DATE       1461 non-null   object \n",
      " 6   AWND       1461 non-null   float64\n",
      " 7   FMTM       1460 non-null   float64\n",
      " 8   PGTM       1457 non-null   float64\n",
      " 9   PRCP       1461 non-null   float64\n",
      " 10  SNOW       1461 non-null   float64\n",
      " 11  SNWD       1461 non-null   float64\n",
      " 12  TAVG       1460 non-null   float64\n",
      " 13  TMAX       1461 non-null   int64  \n",
      " 14  TMIN       1461 non-null   int64  \n",
      " 15  TSUN       467 non-null    float64\n",
      " 16  WDF2       1461 non-null   int64  \n",
      " 17  WDF5       1461 non-null   int64  \n",
      " 18  WESD       1461 non-null   float64\n",
      " 19  WSF2       1461 non-null   float64\n",
      " 20  WSF5       1461 non-null   float64\n",
      " 21  WT01       738 non-null    float64\n",
      " 22  WT02       87 non-null     float64\n",
      " 23  WT03       272 non-null    float64\n",
      " 24  WT04       0 non-null      float64\n",
      " 25  WT05       4 non-null      float64\n",
      " 26  WT06       0 non-null      float64\n",
      " 27  WT07       0 non-null      float64\n",
      " 28  WT08       117 non-null    float64\n",
      " 29  WT09       0 non-null      float64\n",
      " 30  WT11       2 non-null      float64\n",
      " 31  WT13       696 non-null    float64\n",
      " 32  WT14       35 non-null     float64\n",
      " 33  WT15       0 non-null      float64\n",
      " 34  WT16       569 non-null    float64\n",
      " 35  WT17       0 non-null      float64\n",
      " 36  WT18       0 non-null      float64\n",
      " 37  WT21       76 non-null     float64\n",
      " 38  WV03       3 non-null      float64\n",
      "dtypes: float64(32), int64(4), object(3)\n",
      "memory usage: 445.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Loading all files pertinent to the Indoor/Outdoor AQ analysis\n",
    "riopa_pm25=pd.read_csv(path_header+path_riopa+'PM_Mass.csv')\n",
    "riopa_temp=pd.read_csv(path_header+path_riopa+'TempRH.csv')\n",
    "riopa_landuse=pd.read_csv(path_header+path_riopa+'Land_Use.csv')\n",
    "riopa_aer=pd.read_csv(path_header+path_riopa+'AER.csv')\n",
    "riopa_meteo=pd.read_csv(path_header+path_riopa+'met_avg_linkid.csv')\n",
    "riopa_meteo_noaa=pd.read_csv(path_header+path_meteo+'METEO_Houston_Hobby_1999_2001.csv')\n",
    "\n",
    "### let's have a look to the column names\n",
    "riopa_list=[riopa_pm25,riopa_temp,riopa_landuse,riopa_aer,riopa_meteo,riopa_meteo_noaa]\n",
    "for name in riopa_list:\n",
    "    print(name.info())   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinkID is the primary key in the RIOPA relational database. It is a unique subject home identifier, first two digits state code (NJ,TX, CA), three digit numbers assigned to home in \"Home ID\"\n",
    "during first visit, one digit number specifying the visit number (1 =first, 2=second), one digit sample type (lndoor=1,Outdoor=2, Personal Adult=3, Personal Child 1-4 ~ 4-7, Blank\n",
    "=8, Control = 9, Vehicle =0), one digit for duplicate/QA code (Sample = 0, Duplicate =1, Repeat Analysis = 2, Backup of PUF sample for Breakthrough =3, Backup Duplicate=4). Only LinkID starting with TX will be kept.\n",
    "\n",
    "The RIOPA datasets will be joined using the link ID and startdate. The weather daily summaries from NOAA (riopa*_*meteo*_*noaa) wil be joined to startdate with DATE.\n",
    "As not all the data present in the datasets are relevant to the analysis, each dataset will be filtered prior to the merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter riopa_pm25\n",
    "f_riopa_pm25=riopa_pm25[['LinkID','SampleID','HomeID','Type','PM25mass','Validity','comments','datestarted','dateended']]\n",
    "\n",
    "### Filter riopa_temp\n",
    "f_riopa_temp=riopa_temp[['LinkID','ID','HomeID','Visit','Start_Date','Location','temp_c','avg_rh']]\n",
    "\n",
    "### Filter riopa_landuse\n",
    "f_riopa_landuse=riopa_landuse[['LinkID','Class']]\n",
    "\n",
    "### Filter riopa_aer\n",
    "f_riopa_aer=riopa_aer[['LinkID','HomeID','AER','comment','Start_Date','End_Date']]\n",
    "\n",
    "### Filter riopa_meteo_\n",
    "f_riopa_meteo=riopa_meteo[['LinkID','datestarted','dateended','avg_Dry_bulb_temp','avg_Dew_point_temp','avg_Wet_bulb_temp','avg_RH']]\n",
    "\n",
    "### Filter riopa_meteo_noaa\n",
    "f_riopa_meteo_noaa=riopa_meteo_noaa[['STATION','NAME','LATITUDE','LONGITUDE','DATE','TMIN','TMAX','TAVG','WT16','WT21','WT01','WT08']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am renaming the columns of the dataframes below to make the merge/join coding easier to read and to keep the metadata intact (i.e. rename 'comment' and dates columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1138 entries, 0 to 1137\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   linkid           1138 non-null   object \n",
      " 1   sampleid         1138 non-null   int64  \n",
      " 2   homeid           1138 non-null   object \n",
      " 3   airtype          1138 non-null   object \n",
      " 4   pm25             1112 non-null   float64\n",
      " 5   validity         171 non-null    float64\n",
      " 6   comments_pm25    1106 non-null   object \n",
      " 7   date_start_pm25  1138 non-null   object \n",
      " 8   date_end_pm25    1138 non-null   object \n",
      "dtypes: float64(2), int64(1), object(6)\n",
      "memory usage: 80.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1148 entries, 0 to 1147\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   linkid          1148 non-null   object \n",
      " 1   tempid          1148 non-null   int64  \n",
      " 2   homeid          1148 non-null   object \n",
      " 3   visitnumber     1148 non-null   int64  \n",
      " 4   date_temp       1148 non-null   object \n",
      " 5   location        1148 non-null   object \n",
      " 6   ambient_temp_c  1148 non-null   float64\n",
      " 7   ambient_rh      927 non-null    float64\n",
      "dtypes: float64(2), int64(2), object(4)\n",
      "memory usage: 71.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 414 entries, 0 to 413\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   linkid         414 non-null    object\n",
      " 1   landuse_class  414 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 6.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 533 entries, 0 to 532\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   linkid          533 non-null    object \n",
      " 1   homeid          533 non-null    object \n",
      " 2   airexrate       524 non-null    float64\n",
      " 3   comment_aer     14 non-null     object \n",
      " 4   date_start_aer  533 non-null    object \n",
      " 5   date_end_aer    533 non-null    object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 25.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9265 entries, 0 to 9264\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   linkid            9265 non-null   object \n",
      " 1   date_start_meteo  9255 non-null   object \n",
      " 2   date_end_meteo    9255 non-null   object \n",
      " 3   temp_dry          9255 non-null   float64\n",
      " 4   dew_point         9255 non-null   float64\n",
      " 5   temp_wet          9255 non-null   float64\n",
      " 6   rh                9255 non-null   float64\n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 506.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1461 entries, 0 to 1460\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   station     1461 non-null   object \n",
      " 1   name        1461 non-null   object \n",
      " 2   latitude    1461 non-null   float64\n",
      " 3   longitude   1461 non-null   float64\n",
      " 4   date_noaa   1461 non-null   object \n",
      " 5   temp_min    1461 non-null   int64  \n",
      " 6   temp_max    1461 non-null   int64  \n",
      " 7   temp_mean   1460 non-null   float64\n",
      " 8   rain        569 non-null    float64\n",
      " 9   groundfog   76 non-null     float64\n",
      " 10  fog         738 non-null    float64\n",
      " 11  smoke_haze  117 non-null    float64\n",
      "dtypes: float64(7), int64(2), object(3)\n",
      "memory usage: 137.1+ KB\n"
     ]
    }
   ],
   "source": [
    "### Clarify column names\n",
    "col_f_riopa_pm25={'LinkID':'linkid','SampleID':'sampleid','HomeID':'homeid','Type':'airtype','PM25mass':'pm25','Validity':'validity','comments':'comments_pm25','datestarted':'date_start_pm25','dateended':'date_end_pm25'}\n",
    "fr_riopa_pm25=f_riopa_pm25.rename(columns=col_f_riopa_pm25)\n",
    "fr_riopa_pm25.info()\n",
    "\n",
    "col_f_riopa_temp={'LinkID':'linkid','ID':'tempid','HomeID':'homeid','Visit':'visitnumber','Start_Date':'date_temp','Location':'location','temp_c':'ambient_temp_c','avg_rh':'ambient_rh'}\n",
    "fr_riopa_temp=f_riopa_temp.rename(columns=col_f_riopa_temp)\n",
    "fr_riopa_temp.info()\n",
    "\n",
    "col_f_riopa_landuse={'LinkID':'linkid','Class':'landuse_class'}\n",
    "fr_riopa_landuse=f_riopa_landuse.rename(columns=col_f_riopa_landuse)\n",
    "fr_riopa_landuse.info()\n",
    "\n",
    "col_f_riopa_aer={'LinkID':'linkid','HomeID':'homeid','AER':'airexrate','comment':'comment_aer','Start_Date':'date_start_aer','End_Date':'date_end_aer'}\n",
    "fr_riopa_aer=f_riopa_aer.rename(columns=col_f_riopa_aer)\n",
    "fr_riopa_aer.info()\n",
    "\n",
    "col_f_riopa_meteo={'LinkID':'linkid','datestarted':'date_start_meteo','dateended':'date_end_meteo','avg_Dry_bulb_temp':'temp_dry','avg_Dew_point_temp':'dew_point','avg_Wet_bulb_temp':'temp_wet','avg_RH':'rh'}\n",
    "fr_riopa_meteo=f_riopa_meteo.rename(columns=col_f_riopa_meteo)\n",
    "fr_riopa_meteo.info()\n",
    "\n",
    "col_f_riopa_meteo_noaa={'STATION':'station','NAME':'name','LATITUDE':'latitude','LONGITUDE':'longitude','DATE':'date_noaa','TMIN':'temp_min','TMAX':'temp_max','TAVG':'temp_mean','WT16':'rain','WT21':'groundfog','WT01':'fog','WT08':'smoke_haze'}\n",
    "fr_riopa_meteo_noaa=f_riopa_meteo_noaa.rename(columns=col_f_riopa_meteo_noaa)\n",
    "fr_riopa_meteo_noaa.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RIOPA datasets contain data from Houston (TX), Elizabeth (NJ), and Los Angeles (CA). Only the texan data is of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing non texan data\n",
    "tx_riopa_pm25=fr_riopa_pm25[fr_riopa_pm25.linkid.str.contains('TX')==True]\n",
    "tx_riopa_temp=fr_riopa_temp[fr_riopa_temp.linkid.str.contains('TX')==True]\n",
    "tx_riopa_landuse=fr_riopa_landuse[fr_riopa_landuse.linkid.str.contains('TX')==True]\n",
    "tx_riopa_aer=fr_riopa_aer[fr_riopa_aer.linkid.str.contains('TX')==True]\n",
    "tx_riopa_meteo=fr_riopa_meteo[fr_riopa_meteo.linkid.str.contains('TX')==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before merging is to convert the dates from 'object' to datetime and to add a formal \"date\" column to dataframes where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "### Converting date columns to datetime objects and adding a date column.\n",
    "tx_riopa_pm25['date_start_pm25'] = pd.to_datetime(tx_riopa_pm25['date_start_pm25'])\n",
    "tx_riopa_pm25['date_end_pm25'] = pd.to_datetime(tx_riopa_pm25['date_end_pm25'])\n",
    "tx_riopa_pm25['date']=tx_riopa_pm25['date_start_pm25']\n",
    "\n",
    "tx_riopa_temp['date_temp'] = pd.to_datetime(tx_riopa_temp['date_temp'])\n",
    "tx_riopa_temp['date']=tx_riopa_temp['date_temp']\n",
    "\n",
    "tx_riopa_aer['date_start_aer'] = pd.to_datetime(tx_riopa_aer['date_start_aer'])\n",
    "tx_riopa_aer['date_end_aer'] = pd.to_datetime(tx_riopa_aer['date_end_aer'])\n",
    "tx_riopa_aer['date']=tx_riopa_aer['date_start_aer']\n",
    "\n",
    "tx_riopa_meteo['date_start_meteo'] = pd.to_datetime(tx_riopa_meteo['date_start_meteo'])\n",
    "tx_riopa_meteo['date_end_meteo'] = pd.to_datetime(tx_riopa_meteo['date_end_meteo'])\n",
    "\n",
    "fr_riopa_meteo_noaa['date_noaa'] = pd.to_datetime(fr_riopa_meteo_noaa['date_noaa'])\n",
    "fr_riopa_meteo_noaa['date']=fr_riopa_meteo_noaa['date_noaa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are ready to be merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge landuse to pm25 to create riopa_m1\n",
    "riopa_m1=pd.merge(tx_riopa_pm25,tx_riopa_landuse,how='left',on='linkid')\n",
    "\n",
    "### merge riopa_m1 with tx_riopa_temp on linkid,homeid and date\n",
    "riopa_m2=pd.merge(riopa_m1,tx_riopa_temp,how='left',on=['linkid','homeid','date'])\n",
    "\n",
    "### merge riopa_m2 with tx_riopa_aer on linkid,homeid and date to create riopa_m3\n",
    "riopa_m3=pd.merge(riopa_m2,tx_riopa_aer,how='left',on=['linkid','homeid','date'])\n",
    "\n",
    "# merge riopa_m3 with tx_riopa_meteo on linkid to create riopa_m4\n",
    "riopa_m4=pd.merge(riopa_m3,tx_riopa_meteo,how='left',on='linkid')\n",
    "\n",
    "# merge riopa_m4 with fr_riopa_meteo_noaa on date to create riopa\n",
    "riopa=pd.merge(riopa_m4,fr_riopa_meteo_noaa,how='left',on='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final riopa dataset is ready. It is saved as 'dw*_*riopa'. I chose to keep the details of sampling dates (date*_*start and date*_*end) for analytical reasons and join all datasets with \"date\" which is their respective \"date*_*start\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1146 entries, 0 to 1145\n",
      "Data columns (total 39 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   linkid            1146 non-null   object        \n",
      " 1   sampleid          1146 non-null   int64         \n",
      " 2   homeid            1146 non-null   object        \n",
      " 3   airtype           1146 non-null   object        \n",
      " 4   pm25              1086 non-null   float64       \n",
      " 5   validity          195 non-null    float64       \n",
      " 6   comments_pm25     1098 non-null   object        \n",
      " 7   date_start_pm25   1146 non-null   datetime64[ns]\n",
      " 8   date_end_pm25     1146 non-null   datetime64[ns]\n",
      " 9   date              1146 non-null   datetime64[ns]\n",
      " 10  landuse_class     375 non-null    object        \n",
      " 11  tempid            705 non-null    float64       \n",
      " 12  visitnumber       705 non-null    float64       \n",
      " 13  date_temp         705 non-null    datetime64[ns]\n",
      " 14  location          705 non-null    object        \n",
      " 15  ambient_temp_c    705 non-null    float64       \n",
      " 16  ambient_rh        705 non-null    float64       \n",
      " 17  airexrate         336 non-null    float64       \n",
      " 18  comment_aer       3 non-null      object        \n",
      " 19  date_start_aer    336 non-null    datetime64[ns]\n",
      " 20  date_end_aer      336 non-null    datetime64[ns]\n",
      " 21  date_start_meteo  1146 non-null   datetime64[ns]\n",
      " 22  date_end_meteo    1146 non-null   datetime64[ns]\n",
      " 23  temp_dry          1146 non-null   float64       \n",
      " 24  dew_point         1146 non-null   float64       \n",
      " 25  temp_wet          1146 non-null   float64       \n",
      " 26  rh                1146 non-null   float64       \n",
      " 27  station           1146 non-null   object        \n",
      " 28  name              1146 non-null   object        \n",
      " 29  latitude          1146 non-null   float64       \n",
      " 30  longitude         1146 non-null   float64       \n",
      " 31  date_noaa         1146 non-null   datetime64[ns]\n",
      " 32  temp_min          1146 non-null   int64         \n",
      " 33  temp_max          1146 non-null   int64         \n",
      " 34  temp_mean         1137 non-null   float64       \n",
      " 35  rain              339 non-null    float64       \n",
      " 36  groundfog         54 non-null     float64       \n",
      " 37  fog               711 non-null    float64       \n",
      " 38  smoke_haze        126 non-null    float64       \n",
      "dtypes: datetime64[ns](9), float64(18), int64(3), object(9)\n",
      "memory usage: 358.1+ KB\n"
     ]
    }
   ],
   "source": [
    "riopa.to_excel(path_header+path_df+'riopa.xlsx',sheet_name='d_wrangling')\n",
    "riopa.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.2. Weather Daily Summaries:###\n",
    "The weather summaries from NOAA cover the daily recordings collected from three stations in the Houston area from 01/01/2008 to 09/01/2020. The stations are \"Hobby Airport\", \"IAH\" and \"Galveston\". Columns headers are idnetical from file to file. Only the 'date ' columns need to be converted to a datetime object prior to concatenating the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13884 entries, 0 to 2800\n",
      "Data columns (total 70 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   STATION          13884 non-null  object        \n",
      " 1   NAME             13884 non-null  object        \n",
      " 2   LATITUDE         13884 non-null  float64       \n",
      " 3   LONGITUDE        13884 non-null  float64       \n",
      " 4   ELEVATION        13884 non-null  float64       \n",
      " 5   DATE             13884 non-null  datetime64[ns]\n",
      " 6   AWND             13737 non-null  float64       \n",
      " 7   AWND_ATTRIBUTES  13737 non-null  object        \n",
      " 8   PGTM             3942 non-null   float64       \n",
      " 9   PGTM_ATTRIBUTES  3942 non-null   object        \n",
      " 10  PRCP             13884 non-null  float64       \n",
      " 11  PRCP_ATTRIBUTES  13884 non-null  object        \n",
      " 12  SNOW             10249 non-null  float64       \n",
      " 13  SNOW_ATTRIBUTES  10249 non-null  object        \n",
      " 14  SNWD             10212 non-null  float64       \n",
      " 15  SNWD_ATTRIBUTES  10212 non-null  object        \n",
      " 16  TAVG             9921 non-null   float64       \n",
      " 17  TAVG_ATTRIBUTES  9921 non-null   object        \n",
      " 18  TMAX             13858 non-null  float64       \n",
      " 19  TMAX_ATTRIBUTES  13858 non-null  object        \n",
      " 20  TMIN             13858 non-null  float64       \n",
      " 21  TMIN_ATTRIBUTES  13858 non-null  object        \n",
      " 22  WDF2             13769 non-null  float64       \n",
      " 23  WDF2_ATTRIBUTES  13769 non-null  object        \n",
      " 24  WDF5             13670 non-null  float64       \n",
      " 25  WDF5_ATTRIBUTES  13670 non-null  object        \n",
      " 26  WSF2             13769 non-null  float64       \n",
      " 27  WSF2_ATTRIBUTES  13769 non-null  object        \n",
      " 28  WSF5             13670 non-null  float64       \n",
      " 29  WSF5_ATTRIBUTES  13670 non-null  object        \n",
      " 30  WT01             5215 non-null   float64       \n",
      " 31  WT01_ATTRIBUTES  5215 non-null   object        \n",
      " 32  WT02             746 non-null    float64       \n",
      " 33  WT02_ATTRIBUTES  746 non-null    object        \n",
      " 34  WT03             2237 non-null   float64       \n",
      " 35  WT03_ATTRIBUTES  2237 non-null   object        \n",
      " 36  WT04             16 non-null     float64       \n",
      " 37  WT04_ATTRIBUTES  16 non-null     object        \n",
      " 38  WT05             567 non-null    float64       \n",
      " 39  WT05_ATTRIBUTES  567 non-null    object        \n",
      " 40  WT06             4 non-null      float64       \n",
      " 41  WT06_ATTRIBUTES  4 non-null      object        \n",
      " 42  WT08             1710 non-null   float64       \n",
      " 43  WT08_ATTRIBUTES  1710 non-null   object        \n",
      " 44  WT10             10 non-null     float64       \n",
      " 45  WT10_ATTRIBUTES  10 non-null     object        \n",
      " 46  FMTM             2860 non-null   float64       \n",
      " 47  FMTM_ATTRIBUTES  2860 non-null   object        \n",
      " 48  WT07             139 non-null    float64       \n",
      " 49  WT07_ATTRIBUTES  139 non-null    object        \n",
      " 50  WT09             56 non-null     float64       \n",
      " 51  WT09_ATTRIBUTES  56 non-null     object        \n",
      " 52  WT11             21 non-null     float64       \n",
      " 53  WT11_ATTRIBUTES  21 non-null     object        \n",
      " 54  WT13             1113 non-null   float64       \n",
      " 55  WT13_ATTRIBUTES  1113 non-null   object        \n",
      " 56  WT14             65 non-null     float64       \n",
      " 57  WT14_ATTRIBUTES  65 non-null     object        \n",
      " 58  WT15             4 non-null      float64       \n",
      " 59  WT15_ATTRIBUTES  4 non-null      object        \n",
      " 60  WT16             1364 non-null   float64       \n",
      " 61  WT16_ATTRIBUTES  1364 non-null   object        \n",
      " 62  WT18             7 non-null      float64       \n",
      " 63  WT18_ATTRIBUTES  7 non-null      object        \n",
      " 64  WT21             48 non-null     float64       \n",
      " 65  WT21_ATTRIBUTES  48 non-null     object        \n",
      " 66  WESD             1096 non-null   float64       \n",
      " 67  WESD_ATTRIBUTES  1096 non-null   object        \n",
      " 68  WT17             4 non-null      float64       \n",
      " 69  WT17_ATTRIBUTES  4 non-null      object        \n",
      "dtypes: datetime64[ns](1), float64(35), object(34)\n",
      "memory usage: 7.5+ MB\n"
     ]
    }
   ],
   "source": [
    "### Loading the files holding weather summaries\n",
    "meteo_gal_2008_2012=pd.read_csv(path_header+path_meteo+'METEO_Galveston_2008_2012.csv')\n",
    "meteo_gal_2013_2020=pd.read_csv(path_header+path_meteo+'METEO_Galveston_2013_2020.csv')\n",
    "meteo_hob_2008_2012=pd.read_csv(path_header+path_meteo+'METEO_Houston_Hobby_2008_2012.csv')\n",
    "meteo_hob_2013_2020=pd.read_csv(path_header+path_meteo+'METEO_Houston_Hobby_2013_2020.csv')\n",
    "meteo_iah_2008_2012=pd.read_csv(path_header+path_meteo+'METEO_Houston_IAH_2008_2012.csv')\n",
    "meteo_iah_2013_2020=pd.read_csv(path_header+path_meteo+'METEO_Houston_IAH_2013_2020.csv')\n",
    "\n",
    "### Converting the \"DATE\" column from object to datetime\n",
    "meteo_gal_2008_2012['DATE'] = pd.to_datetime(meteo_gal_2008_2012['DATE'])\n",
    "meteo_gal_2013_2020['DATE'] = pd.to_datetime(meteo_gal_2013_2020['DATE'])\n",
    "meteo_hob_2008_2012['DATE'] = pd.to_datetime(meteo_hob_2008_2012['DATE'])\n",
    "meteo_hob_2013_2020['DATE'] = pd.to_datetime(meteo_hob_2013_2020['DATE'])\n",
    "meteo_iah_2008_2012['DATE'] = pd.to_datetime(meteo_iah_2008_2012['DATE'])\n",
    "meteo_iah_2013_2020['DATE'] = pd.to_datetime(meteo_iah_2013_2020['DATE'])\n",
    "\n",
    "# Concatenating all dataframes in one called meteo_all\n",
    "meteo_all=pd.concat([meteo_gal_2008_2012,meteo_gal_2013_2020,meteo_hob_2008_2012,meteo_hob_2013_2020,meteo_iah_2008_2012,meteo_iah_2013_2020])\n",
    "meteo_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately NOAA provides a key to decode the cryptic column names. The relevant columns are renamed and the dataframe is subset to create the final weather daily data table 'meteo'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13884 entries, 0 to 2800\n",
      "Data columns (total 36 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   date                         13884 non-null  datetime64[ns]\n",
      " 1   drizzle                      65 non-null     float64       \n",
      " 2   drizzle_freezing             4 non-null      float64       \n",
      " 3   dust_sand                    139 non-null    float64       \n",
      " 4   fog                          5215 non-null   float64       \n",
      " 5   fog_ground                   48 non-null     float64       \n",
      " 6   fog_heavy                    746 non-null    float64       \n",
      " 7   rain                         1364 non-null   float64       \n",
      " 8   rain_freezing                4 non-null      float64       \n",
      " 9   rain_hail                    567 non-null    float64       \n",
      " 10  rain_mist                    1113 non-null   float64       \n",
      " 11  rain_prcp                    13884 non-null  float64       \n",
      " 12  smoke_haze                   1710 non-null   float64       \n",
      " 13  snow                         10249 non-null  float64       \n",
      " 14  snow_depth                   10212 non-null  float64       \n",
      " 15  snow_drifting                56 non-null     float64       \n",
      " 16  snow_glaze_rime              4 non-null      float64       \n",
      " 17  snow_grains                  7 non-null      float64       \n",
      " 18  snow_sleet                   16 non-null     float64       \n",
      " 19  station_code                 13884 non-null  object        \n",
      " 20  station_lat                  13884 non-null  float64       \n",
      " 21  station_lon                  13884 non-null  float64       \n",
      " 22  station_name                 13884 non-null  object        \n",
      " 23  temp_avg                     9921 non-null   float64       \n",
      " 24  temp_max                     13858 non-null  float64       \n",
      " 25  temp_min                     13858 non-null  float64       \n",
      " 26  thunder                      2237 non-null   float64       \n",
      " 27  wind_avgspeed                13737 non-null  float64       \n",
      " 28  wind_fastest_2min            13769 non-null  float64       \n",
      " 29  wind_fastest_2min_direction  13769 non-null  float64       \n",
      " 30  wind_fastest_5min            13670 non-null  float64       \n",
      " 31  wind_fastest_5min_direction  13670 non-null  float64       \n",
      " 32  wind_high_dmg                21 non-null     float64       \n",
      " 33  wind_peak_gust_time          3942 non-null   float64       \n",
      " 34  wind_time_fastest_mile       2860 non-null   float64       \n",
      " 35  wind_tornado                 10 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(33), object(2)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "### Rename columns I want to keep\n",
    "col_meteo={'STATION':'station_code', 'NAME':'station_name', 'LONGITUDE':'station_lon', 'LATITUDE':'station_lat', 'DATE':'date',  \n",
    "           'TAVG':'temp_avg','TMIN':'temp_min','TMAX':'temp_max','WT08':'smoke_haze','PRCP':'rain_prcp','WT16':'rain','WT03':'thunder',\n",
    "           'WT21':'fog_ground','WT02':'fog_heavy','WT01':'fog','AWND':'wind_avgspeed','PGTM':'wind_peak_gust_time','FMTM':'wind_time_fastest_mile',\n",
    "           'WSF2':'wind_fastest_2min','WSF5':'wind_fastest_5min','WDF2':'wind_fastest_2min_direction','WDF5':'wind_fastest_5min_direction',\n",
    "           'WT11':'wind_high_dmg','WT07':'dust_sand','WT13':'rain_mist','WT14':'drizzle','WT15':'drizzle_freezing','WT17':'rain_freezing',\n",
    "           'WT18':'snow_grains','SNOW':'snow','SNWD':'snow_depth','WT09':'snow_drifting','WT04':'snow_sleet','WT05':'rain_hail',\n",
    "           'WT06':'snow_glaze_rime','WT10':'wind_tornado'}\n",
    "meteo_all=meteo_all.rename(columns=col_meteo)\n",
    "\n",
    "### subsetting \n",
    "subsetlist=list({v for k, v in col_meteo.items()})\n",
    "subsetlist.sort()\n",
    "meteo=meteo_all[subsetlist]\n",
    "\n",
    "### Saving meteo in '00_SavedDataframes'\n",
    "meteo.to_excel(path_header+path_df+'meteo.xlsx',sheet_name='d_wrangling')\n",
    "meteo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Outdoor Air Quality:###\n",
    "This is the biggest chunk of the data collection process where 91 files from EPA and 36 files from TECQ will be concatenated or merged. To speedup the process, loading functions are being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function glob_load which use glob to gather all the filenames\n",
    "### in a folder and return the data into one dataframe\n",
    "def glob_load(path='C:\\\\Users\\\\',file_header='EPA',file_pollutant='CO'):\n",
    "    ### list of filenames\n",
    "    allfiles = glob.glob(path + file_header+'_'+file_pollutant+'_*.csv')\n",
    "    ### mydata list collect the data read by pd.read_csv\n",
    "    mydata = []\n",
    "    for filename in allfiles:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        mydata.append(df)\n",
    "    ### convert mydata list into a dataframe\n",
    "    mydataframe = pd.concat(mydata, axis=0, ignore_index=True)\n",
    "    ### return the dataframe\n",
    "    return mydataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using glob load to get all the data from the EPA csv files\n",
    "epa_co=glob_load(path=path_header+path_AQ+'\\\\',file_header='EPA',file_pollutant='CO')\n",
    "epa_no2=glob_load(path=path_header+path_AQ+'\\\\',file_header='EPA',file_pollutant='NO2')\n",
    "epa_so2=glob_load(path=path_header+path_AQ+'\\\\',file_header='EPA',file_pollutant='SO2')\n",
    "epa_ozone=glob_load(path=path_header+path_AQ+'\\\\',file_header='EPA',file_pollutant='OZONE')\n",
    "epa_pb=glob_load(path=path_header+path_AQ+'\\\\',file_header='EPA',file_pollutant='Pb')\n",
    "epa_pm25=glob_load(path=path_header+path_AQ+'\\\\',file_header='EPA',file_pollutant='PM2_5')\n",
    "epa_pm10=glob_load(path=path_header+path_AQ+'\\\\',file_header='EPA',file_pollutant='PM10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20242 entries, 0 to 20241\n",
      "Data columns (total 20 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   Date                               20242 non-null  object \n",
      " 1   Source                             20242 non-null  object \n",
      " 2   Site ID                            20242 non-null  int64  \n",
      " 3   POC                                20242 non-null  int64  \n",
      " 4   Daily Max 8-hour CO Concentration  20242 non-null  float64\n",
      " 5   UNITS                              20242 non-null  object \n",
      " 6   DAILY_AQI_VALUE                    20242 non-null  int64  \n",
      " 7   Site Name                          20242 non-null  object \n",
      " 8   DAILY_OBS_COUNT                    20242 non-null  int64  \n",
      " 9   PERCENT_COMPLETE                   20242 non-null  float64\n",
      " 10  AQS_PARAMETER_CODE                 20242 non-null  int64  \n",
      " 11  AQS_PARAMETER_DESC                 20242 non-null  object \n",
      " 12  CBSA_CODE                          20242 non-null  int64  \n",
      " 13  CBSA_NAME                          20242 non-null  object \n",
      " 14  STATE_CODE                         20242 non-null  int64  \n",
      " 15  STATE                              20242 non-null  object \n",
      " 16  COUNTY_CODE                        20242 non-null  int64  \n",
      " 17  COUNTY                             20242 non-null  object \n",
      " 18  SITE_LATITUDE                      20242 non-null  float64\n",
      " 19  SITE_LONGITUDE                     20242 non-null  float64\n",
      "dtypes: float64(4), int64(8), object(8)\n",
      "memory usage: 3.1+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 71195 entries, 0 to 71194\n",
      "Data columns (total 20 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   Date                                71195 non-null  object \n",
      " 1   Source                              71195 non-null  object \n",
      " 2   Site ID                             71195 non-null  int64  \n",
      " 3   POC                                 71195 non-null  int64  \n",
      " 4   Daily Max 1-hour NO2 Concentration  71195 non-null  float64\n",
      " 5   UNITS                               71195 non-null  object \n",
      " 6   DAILY_AQI_VALUE                     71195 non-null  int64  \n",
      " 7   Site Name                           71195 non-null  object \n",
      " 8   DAILY_OBS_COUNT                     71195 non-null  int64  \n",
      " 9   PERCENT_COMPLETE                    71195 non-null  float64\n",
      " 10  AQS_PARAMETER_CODE                  71195 non-null  int64  \n",
      " 11  AQS_PARAMETER_DESC                  71195 non-null  object \n",
      " 12  CBSA_CODE                           71195 non-null  int64  \n",
      " 13  CBSA_NAME                           71195 non-null  object \n",
      " 14  STATE_CODE                          71195 non-null  int64  \n",
      " 15  STATE                               71195 non-null  object \n",
      " 16  COUNTY_CODE                         71195 non-null  int64  \n",
      " 17  COUNTY                              71195 non-null  object \n",
      " 18  SITE_LATITUDE                       71195 non-null  float64\n",
      " 19  SITE_LONGITUDE                      71195 non-null  float64\n",
      "dtypes: float64(4), int64(8), object(8)\n",
      "memory usage: 10.9+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35641 entries, 0 to 35640\n",
      "Data columns (total 20 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   Date                                35641 non-null  object \n",
      " 1   Source                              35641 non-null  object \n",
      " 2   Site ID                             35641 non-null  int64  \n",
      " 3   POC                                 35641 non-null  int64  \n",
      " 4   Daily Max 1-hour SO2 Concentration  35641 non-null  float64\n",
      " 5   UNITS                               35641 non-null  object \n",
      " 6   DAILY_AQI_VALUE                     35641 non-null  int64  \n",
      " 7   Site Name                           35641 non-null  object \n",
      " 8   DAILY_OBS_COUNT                     35641 non-null  int64  \n",
      " 9   PERCENT_COMPLETE                    35641 non-null  float64\n",
      " 10  AQS_PARAMETER_CODE                  35641 non-null  int64  \n",
      " 11  AQS_PARAMETER_DESC                  35641 non-null  object \n",
      " 12  CBSA_CODE                           35641 non-null  int64  \n",
      " 13  CBSA_NAME                           35641 non-null  object \n",
      " 14  STATE_CODE                          35641 non-null  int64  \n",
      " 15  STATE                               35641 non-null  object \n",
      " 16  COUNTY_CODE                         35641 non-null  int64  \n",
      " 17  COUNTY                              35641 non-null  object \n",
      " 18  SITE_LATITUDE                       35641 non-null  float64\n",
      " 19  SITE_LONGITUDE                      35641 non-null  float64\n",
      "dtypes: float64(4), int64(8), object(8)\n",
      "memory usage: 5.4+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 91476 entries, 0 to 91475\n",
      "Data columns (total 20 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   Date                                  91476 non-null  object \n",
      " 1   Source                                91476 non-null  object \n",
      " 2   Site ID                               91476 non-null  int64  \n",
      " 3   POC                                   91476 non-null  int64  \n",
      " 4   Daily Max 8-hour Ozone Concentration  91476 non-null  float64\n",
      " 5   UNITS                                 91476 non-null  object \n",
      " 6   DAILY_AQI_VALUE                       91476 non-null  int64  \n",
      " 7   Site Name                             91476 non-null  object \n",
      " 8   DAILY_OBS_COUNT                       91476 non-null  int64  \n",
      " 9   PERCENT_COMPLETE                      91476 non-null  float64\n",
      " 10  AQS_PARAMETER_CODE                    91476 non-null  int64  \n",
      " 11  AQS_PARAMETER_DESC                    91476 non-null  object \n",
      " 12  CBSA_CODE                             91476 non-null  int64  \n",
      " 13  CBSA_NAME                             91476 non-null  object \n",
      " 14  STATE_CODE                            91476 non-null  int64  \n",
      " 15  STATE                                 91476 non-null  object \n",
      " 16  COUNTY_CODE                           91476 non-null  int64  \n",
      " 17  COUNTY                                91476 non-null  object \n",
      " 18  SITE_LATITUDE                         91476 non-null  float64\n",
      " 19  SITE_LONGITUDE                        91476 non-null  float64\n",
      "dtypes: float64(4), int64(8), object(8)\n",
      "memory usage: 14.0+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 656 entries, 0 to 655\n",
      "Data columns (total 20 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Date                         656 non-null    object \n",
      " 1   Source                       656 non-null    object \n",
      " 2   Site ID                      656 non-null    int64  \n",
      " 3   POC                          656 non-null    int64  \n",
      " 4   Daily Mean Pb Concentration  656 non-null    float64\n",
      " 5   UNITS                        656 non-null    object \n",
      " 6   DAILY_AQI_VALUE              656 non-null    object \n",
      " 7   Site Name                    656 non-null    object \n",
      " 8   DAILY_OBS_COUNT              656 non-null    int64  \n",
      " 9   PERCENT_COMPLETE             656 non-null    float64\n",
      " 10  AQS_PARAMETER_CODE           656 non-null    int64  \n",
      " 11  AQS_PARAMETER_DESC           656 non-null    object \n",
      " 12  CBSA_CODE                    656 non-null    int64  \n",
      " 13  CBSA_NAME                    656 non-null    object \n",
      " 14  STATE_CODE                   656 non-null    int64  \n",
      " 15  STATE                        656 non-null    object \n",
      " 16  COUNTY_CODE                  656 non-null    int64  \n",
      " 17  COUNTY                       656 non-null    object \n",
      " 18  SITE_LATITUDE                656 non-null    float64\n",
      " 19  SITE_LONGITUDE               656 non-null    float64\n",
      "dtypes: float64(4), int64(7), object(9)\n",
      "memory usage: 102.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51311 entries, 0 to 51310\n",
      "Data columns (total 20 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   Date                            51311 non-null  object \n",
      " 1   Source                          51311 non-null  object \n",
      " 2   Site ID                         51311 non-null  int64  \n",
      " 3   POC                             51311 non-null  int64  \n",
      " 4   Daily Mean PM2.5 Concentration  51311 non-null  float64\n",
      " 5   UNITS                           51311 non-null  object \n",
      " 6   DAILY_AQI_VALUE                 51311 non-null  int64  \n",
      " 7   Site Name                       51311 non-null  object \n",
      " 8   DAILY_OBS_COUNT                 51311 non-null  int64  \n",
      " 9   PERCENT_COMPLETE                51311 non-null  float64\n",
      " 10  AQS_PARAMETER_CODE              51311 non-null  int64  \n",
      " 11  AQS_PARAMETER_DESC              51311 non-null  object \n",
      " 12  CBSA_CODE                       51311 non-null  int64  \n",
      " 13  CBSA_NAME                       51311 non-null  object \n",
      " 14  STATE_CODE                      51311 non-null  int64  \n",
      " 15  STATE                           51311 non-null  object \n",
      " 16  COUNTY_CODE                     51311 non-null  int64  \n",
      " 17  COUNTY                          51311 non-null  object \n",
      " 18  SITE_LATITUDE                   51311 non-null  float64\n",
      " 19  SITE_LONGITUDE                  51311 non-null  float64\n",
      "dtypes: float64(4), int64(8), object(8)\n",
      "memory usage: 7.8+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7344 entries, 0 to 7343\n",
      "Data columns (total 20 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   Date                           7344 non-null   object \n",
      " 1   Source                         7344 non-null   object \n",
      " 2   Site ID                        7344 non-null   int64  \n",
      " 3   POC                            7344 non-null   int64  \n",
      " 4   Daily Mean PM10 Concentration  7344 non-null   int64  \n",
      " 5   UNITS                          7344 non-null   object \n",
      " 6   DAILY_AQI_VALUE                7344 non-null   int64  \n",
      " 7   Site Name                      7344 non-null   object \n",
      " 8   DAILY_OBS_COUNT                7344 non-null   int64  \n",
      " 9   PERCENT_COMPLETE               7344 non-null   float64\n",
      " 10  AQS_PARAMETER_CODE             7344 non-null   int64  \n",
      " 11  AQS_PARAMETER_DESC             7344 non-null   object \n",
      " 12  CBSA_CODE                      7344 non-null   int64  \n",
      " 13  CBSA_NAME                      7344 non-null   object \n",
      " 14  STATE_CODE                     7344 non-null   int64  \n",
      " 15  STATE                          7344 non-null   object \n",
      " 16  COUNTY_CODE                    7344 non-null   int64  \n",
      " 17  COUNTY                         7344 non-null   object \n",
      " 18  SITE_LATITUDE                  7344 non-null   float64\n",
      " 19  SITE_LONGITUDE                 7344 non-null   float64\n",
      "dtypes: float64(3), int64(9), object(8)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "epa_co.info()\n",
    "epa_no2.info()\n",
    "epa_so2.info()\n",
    "epa_ozone.info()\n",
    "epa_pb.info()\n",
    "epa_pm25.info()\n",
    "epa_pm10.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EPA columns look good. The columns 'DATE' need to be converted to a datetime object. Some columns will have to be renamed to facilitate the merging of all these dataframes into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dictionaries to rename columns \n",
    "col_epa_ozone={'Date':'date', 'Source':'ozone_8hr_source','Site ID':'site_id','POC':'ozone_8hr_poc',\n",
    "               'Daily Max 8-hour Ozone Concentration':'ozone_8hr_max', 'UNITS':'ozone_8hr_units',\n",
    "               'Site Name':'site_name', 'DAILY_OBS_COUNT':'ozone_8hr_obs_count', \n",
    "               'SITE_LATITUDE':'site_lat','SITE_LONGITUDE':'site_lon'}\n",
    "\n",
    "col_epa_co={'Date':'date', 'Source':'co_8hr_source','POC':'co_8hr_poc','Site ID':'site_id',\n",
    "            'Daily Max 8-hour CO Concentration':'co_8hr_max', 'UNITS':'co_8hr_units',\n",
    "            'Site Name':'site_name', 'DAILY_OBS_COUNT':'co_8hr_obs_count',\n",
    "            'SITE_LATITUDE':'site_lat', 'SITE_LONGITUDE':'site_lon'}\n",
    "     \n",
    "    \n",
    "col_epa_no2={'Date':'date', 'Source':'no2_1hr_source','Site ID':'site_id','POC':'no2_1hr_poc',\n",
    "             'Daily Max 1-hour NO2 Concentration':'no2_1hr_max', 'UNITS':'no2_1hr_units',\n",
    "             'Site Name':'site_name', 'DAILY_OBS_COUNT':'no2_1hr_obs_count',\n",
    "             'SITE_LATITUDE':'site_lat','SITE_LONGITUDE':'site_lon'}\n",
    "\n",
    "col_epa_so2={'Date':'date', 'Source':'so2_1hr_source','Site ID':'site_id','POC':'so2_1hr_poc',\n",
    "             'Daily Max 1-hour SO2 Concentration':'so2_1hr_max', 'UNITS':'so2_1hr_units',  \n",
    "             'Site Name':'site_name', 'DAILY_OBS_COUNT':'so2_1hr_obs_count', \n",
    "             'SITE_LATITUDE':'site_lat','SITE_LONGITUDE':'site_lon'}\n",
    "    \n",
    "col_epa_pb={'Date':'date', 'Source':'pb_24hr_source','Site ID':'site_id','POC':'pb_24hr_poc',\n",
    "            'Daily Mean Pb Concentration':'pb_24hr_mean','UNITS':'pb_24hr_units',\n",
    "            'Site Name':'site_name', 'DAILY_OBS_COUNT':'pb_24hr_obs_count', \n",
    "            'SITE_LATITUDE':'site_lat','SITE_LONGITUDE':'site_lon'}\n",
    "    \n",
    "col_epa_pm25={'Date':'date', 'Source':'pm25_24hr_source','Site ID':'site_id','POC':'pm25_24hr_poc',\n",
    "              'Daily Mean PM2.5 Concentration':'pm25_24hr_mean','UNITS':'pm25_24hr_units',\n",
    "              'Site Name':'site_name', 'DAILY_OBS_COUNT':'pm25_24hr_obs_count',\n",
    "              'SITE_LATITUDE':'site_lat','SITE_LONGITUDE':'site_lon'}\n",
    "\n",
    "col_epa_pm10={'Date':'date','Source':'pm10_24hr_source','Site ID':'site_id','POC':'pm10_24hr_poc',\n",
    "              'Daily Mean PM10 Concentration':'pm10_24hr_mean', 'UNITS':'pm10_24hr_units', \n",
    "              'Site Name':'site_name', 'DAILY_OBS_COUNT':'pm10_24hr_obs_count',\n",
    "              'SITE_LATITUDE':'site_lat','SITE_LONGITUDE':'site_lon'}\n",
    "\n",
    "### rename columns\n",
    "repa_ozone=epa_ozone.rename(columns=col_epa_ozone)\n",
    "repa_co=epa_co.rename(columns=col_epa_co)\n",
    "repa_no2=epa_no2.rename(columns=col_epa_no2)\n",
    "repa_so2=epa_so2.rename(columns=col_epa_so2)\n",
    "repa_pb=epa_pb.rename(columns=col_epa_pb)\n",
    "repa_pm25=epa_pm25.rename(columns=col_epa_pm25)\n",
    "repa_pm10=epa_pm10.rename(columns=col_epa_pm10)\n",
    "\n",
    "# convert date to datetime\n",
    "repa_ozone['date'] = pd.to_datetime(repa_ozone['date'])\n",
    "repa_co['date'] = pd.to_datetime(repa_co['date'])\n",
    "repa_no2['date'] = pd.to_datetime(repa_no2['date'])\n",
    "repa_so2['date'] = pd.to_datetime(repa_so2['date'])\n",
    "repa_pb['date'] = pd.to_datetime(repa_pb['date'])\n",
    "repa_pm25['date'] = pd.to_datetime(repa_pm25['date'])\n",
    "repa_pm10['date'] = pd.to_datetime(repa_pm10['date'])\n",
    "\n",
    "### subsetting\n",
    "newcol_ozone=col_epa_ozone.values()\n",
    "subset_ozone=[name for name in newcol_ozone]\n",
    "frepa_ozone=repa_ozone[subset_ozone]\n",
    "\n",
    "newcol_co=col_epa_co.values()\n",
    "subset_co=[name for name in newcol_co]\n",
    "frepa_co=repa_co[subset_co]\n",
    "\n",
    "newcol_no2=col_epa_no2.values()\n",
    "subset_no2=[name for name in newcol_no2]\n",
    "frepa_no2=repa_no2[subset_no2]\n",
    "\n",
    "newcol_so2=col_epa_so2.values()\n",
    "subset_so2=[name for name in newcol_so2]\n",
    "frepa_so2=repa_so2[subset_so2]\n",
    "\n",
    "newcol_pb=col_epa_pb.values()\n",
    "subset_pb=[name for name in newcol_pb]\n",
    "frepa_pb=repa_pb[subset_pb]\n",
    "\n",
    "newcol_pm25=col_epa_pm25.values()\n",
    "subset_pm25=[name for name in newcol_pm25]\n",
    "frepa_pm25=repa_pm25[subset_pm25]\n",
    "\n",
    "newcol_pm10=col_epa_pm10.values()\n",
    "subset_pm10=[name for name in newcol_pm10]\n",
    "frepa_pm10=repa_pm10[subset_pm10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframes for each pollutant are saved in '00*_*SavedDataframes' as well as the resulting merged dataframe \"epa\". Note that at this stage the dataframe 'epa' does not contain unique record for each row because dates for each station and each pollutants are merged together. 'Groupby', 'melt' and subsetting between other methods can be used later to work with this dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the reshaped dataframes prior to merging\n",
    "frepa_ozone.to_excel(path_header+path_df+'epa_ozone.xlsx',sheet_name='d_wrangling')\n",
    "frepa_co.to_excel(path_header+path_df+'epa_co.xlsx',sheet_name='d_wrangling')\n",
    "frepa_no2.to_excel(path_header+path_df+'epa_no2.xlsx',sheet_name='d_wrangling')\n",
    "frepa_so2.to_excel(path_header+path_df+'epa_so2.xlsx',sheet_name='d_wrangling')\n",
    "frepa_pb.to_excel(path_header+path_df+'epa_pb.xlsx',sheet_name='d_wrangling')\n",
    "frepa_pm10.to_excel(path_header+path_df+'epa_no2.xlsx',sheet_name='d_wrangling')\n",
    "frepa_pm25.to_excel(path_header+path_df+'epa_so2.xlsx',sheet_name='d_wrangling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 123257 entries, 0 to 123256\n",
      "Data columns (total 40 columns):\n",
      " #   Column               Non-Null Count   Dtype         \n",
      "---  ------               --------------   -----         \n",
      " 0   date                 123257 non-null  datetime64[ns]\n",
      " 1   ozone_8hr_source     104808 non-null  object        \n",
      " 2   site_id              123257 non-null  int64         \n",
      " 3   ozone_8hr_poc        104808 non-null  float64       \n",
      " 4   ozone_8hr_max        104808 non-null  float64       \n",
      " 5   ozone_8hr_units      104808 non-null  object        \n",
      " 6   site_name            123257 non-null  object        \n",
      " 7   ozone_8hr_obs_count  104808 non-null  float64       \n",
      " 8   site_lat             123257 non-null  float64       \n",
      " 9   site_lon             123257 non-null  float64       \n",
      " 10  co_8hr_source        32334 non-null   object        \n",
      " 11  co_8hr_poc           32334 non-null   float64       \n",
      " 12  co_8hr_max           32334 non-null   float64       \n",
      " 13  co_8hr_units         32334 non-null   object        \n",
      " 14  co_8hr_obs_count     32334 non-null   float64       \n",
      " 15  no2_1hr_source       84341 non-null   object        \n",
      " 16  no2_1hr_poc          84341 non-null   float64       \n",
      " 17  no2_1hr_max          84341 non-null   float64       \n",
      " 18  no2_1hr_units        84341 non-null   object        \n",
      " 19  no2_1hr_obs_count    84341 non-null   float64       \n",
      " 20  so2_1hr_source       46841 non-null   object        \n",
      " 21  so2_1hr_poc          46841 non-null   float64       \n",
      " 22  so2_1hr_max          46841 non-null   float64       \n",
      " 23  so2_1hr_units        46841 non-null   object        \n",
      " 24  so2_1hr_obs_count    46841 non-null   float64       \n",
      " 25  pb_24hr_source       2376 non-null    object        \n",
      " 26  pb_24hr_poc          2376 non-null    float64       \n",
      " 27  pb_24hr_mean         2376 non-null    float64       \n",
      " 28  pb_24hr_units        2376 non-null    object        \n",
      " 29  pb_24hr_obs_count    2376 non-null    float64       \n",
      " 30  pm10_24hr_source     13047 non-null   object        \n",
      " 31  pm10_24hr_poc        13047 non-null   float64       \n",
      " 32  pm10_24hr_mean       13047 non-null   float64       \n",
      " 33  pm10_24hr_units      13047 non-null   object        \n",
      " 34  pm10_24hr_obs_count  13047 non-null   float64       \n",
      " 35  pm25_24hr_source     54274 non-null   object        \n",
      " 36  pm25_24hr_poc        54274 non-null   float64       \n",
      " 37  pm25_24hr_mean       54274 non-null   float64       \n",
      " 38  pm25_24hr_units      54274 non-null   object        \n",
      " 39  pm25_24hr_obs_count  54274 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(23), int64(1), object(15)\n",
      "memory usage: 38.6+ MB\n"
     ]
    }
   ],
   "source": [
    "### merge in order ozone, co, no2, so2, pb, pm10, pm25\n",
    "epa1=pd.merge(frepa_ozone,frepa_co,how='outer',on=['date','site_id','site_name','site_lat','site_lon'])\n",
    "epa2=pd.merge(epa1,frepa_no2,how='outer',on=['date','site_id','site_name','site_lat','site_lon'])\n",
    "epa3=pd.merge(epa2,frepa_so2,how='outer',on=['date','site_id','site_name','site_lat','site_lon'])\n",
    "epa4=pd.merge(epa3,frepa_pb,how='outer',on=['date','site_id','site_name','site_lat','site_lon'])\n",
    "epa5=pd.merge(epa4,frepa_pm10,how='outer',on=['date','site_id','site_name','site_lat','site_lon'])\n",
    "epa=pd.merge(epa5,frepa_pm25,how='outer',on=['date','site_id','site_name','site_lat','site_lon'])\n",
    "\n",
    "### Saving epa to in '00_SavedDataframes'\n",
    "epa.to_excel(path_header+path_df+'epa.xlsx',sheet_name='d_wrangling')\n",
    "epa.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second chunk of air quality data comes from TECQ and is composed of txt files which all have a header summary. To speed up loading of multiples files, another glob function is being used, called 'glob*_*load*_*txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3331: DtypeWarning: Columns (22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\Anne\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3254: DtypeWarning: Columns (107) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "### Function glob_load_txt which use glob to gather all the filenames\n",
    "### in a folder and return the data into one dataframe\n",
    "def glob_load_txt(path='C:\\\\Users\\\\',file_header='EPA',file_pollutant='CO'):\n",
    "    ### list of filenames\n",
    "    allfiles = glob.glob(path + file_header+'_'+file_pollutant+'_*.txt')\n",
    "    ### mydata list collect the data read by pd.read_table\n",
    "    mydata = []\n",
    "    for filename in allfiles:\n",
    "        ### read_table must skip the header summary, hence the skiprows=9\n",
    "        df = pd.read_table(filename,header=0,skiprows=9,sep='\\t')\n",
    "        mydata.append(df)\n",
    "    ### convert mydata list into a dataframe\n",
    "    mydataframe = pd.concat(mydata, axis=0, ignore_index=True)\n",
    "    ### return the dataframe\n",
    "    return mydataframe\n",
    "\n",
    "### Using glob_load_txt to load the data\n",
    "tamis_co=glob_load_txt(path=path_header+path_AQ+'\\\\',file_header='TAMIS',file_pollutant='CO')\n",
    "tamis_no2=glob_load_txt(path=path_header+path_AQ+'\\\\',file_header='TAMIS',file_pollutant='NO2')\n",
    "tamis_so2=glob_load_txt(path=path_header+path_AQ+'\\\\',file_header='TAMIS',file_pollutant='SO2')\n",
    "tamis_ozone=glob_load_txt(path=path_header+path_AQ+'\\\\',file_header='TAMIS',file_pollutant='Ozone')\n",
    "\n",
    "### Using just read_table for single files\n",
    "tamis_pm10_24hr=pd.read_table(path_header+path_AQ+'\\\\TAMIS_PM10_24HR.txt',header=0,skiprows=9,sep='\\t')\n",
    "tamis_pm10_detail=pd.read_table(path_header+path_AQ+'\\\\TAMIS_PM10_2008_2020.txt',header=0,skiprows=9,sep='\\t')\n",
    "tamis_pm25=pd.read_table(path_header+path_AQ+'\\\\TAMIS_PM2_5_24HR_2008_2020.txt',header=0,skiprows=9,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1302264 entries, 0 to 1302263\n",
      "Data columns (total 19 columns):\n",
      " #   Column                Non-Null Count    Dtype  \n",
      "---  ------                --------------    -----  \n",
      " 0   State                 1302264 non-null  object \n",
      " 1   Region                1302264 non-null  object \n",
      " 2   County                1302264 non-null  object \n",
      " 3   City                  1302264 non-null  object \n",
      " 4   AQS Code              1302264 non-null  int64  \n",
      " 5   Site Name             1302264 non-null  object \n",
      " 6   Latitude              1302264 non-null  float64\n",
      " 7   Longitude             1302264 non-null  float64\n",
      " 8   Year                  1302264 non-null  int64  \n",
      " 9   Month                 1302264 non-null  int64  \n",
      " 10  Day                   1302264 non-null  int64  \n",
      " 11  Date                  1302264 non-null  int64  \n",
      " 12  Start Hour            1302264 non-null  int64  \n",
      " 13  Start Minute          1302264 non-null  int64  \n",
      " 14  Start Time            1302264 non-null  object \n",
      " 15  Duration              1302264 non-null  object \n",
      " 16  Sampler Type          1302264 non-null  object \n",
      " 17  SOC                   1302264 non-null  int64  \n",
      " 18  Ozone (ppbv) <44201>  1302264 non-null  float64\n",
      "dtypes: float64(3), int64(8), object(8)\n",
      "memory usage: 188.8+ MB\n"
     ]
    }
   ],
   "source": [
    "### An example of output\n",
    "tamis_ozone.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TECQ*/*Tamis dataframes require some adjustement before merging. \n",
    "- The 'Date' column has to be converted into datetime. \n",
    "- Lead (Pb) concentrations are included in tamis_pm10_detail and therefore has to be retrieved.\n",
    "- TECQ provides hourly measurements of ozone and carbon monoxide concentrations. The maximum values for each 8 hour period has to be extracted. This task will be done later on because there some interest at keeping the hourly data for modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting date format from yyyymmdd to datetime yyyy-mm-dd\n",
    "\n",
    "tamis_ozone['date'] = pd.to_datetime(tamis_ozone['Date'], format='%Y%m%d').dt.strftime(\"%Y-%m-%d\")\n",
    "tamis_ozone['date']=pd.to_datetime(tamis_ozone['date'])\n",
    "tamis_co['date'] = pd.to_datetime(tamis_co['Date'], format='%Y%m%d').dt.strftime(\"%Y-%m-%d\")\n",
    "tamis_co['date']=pd.to_datetime(tamis_co['date'])\n",
    "tamis_no2['date'] = pd.to_datetime(tamis_no2['Date'], format='%Y%m%d').dt.strftime(\"%Y-%m-%d\")\n",
    "tamis_no2['date']=pd.to_datetime(tamis_no2['date'])\n",
    "tamis_so2['date'] = pd.to_datetime(tamis_so2['Date'], format='%Y%m%d').dt.strftime(\"%Y-%m-%d\")\n",
    "tamis_so2['date']=pd.to_datetime(tamis_so2['date'])\n",
    "tamis_pm10_24hr['date'] = pd.to_datetime(tamis_pm10_24hr['Date'], format='%Y%m%d').dt.strftime(\"%Y-%m-%d\")\n",
    "tamis_pm10_24hr['date']=pd.to_datetime(tamis_pm10_24hr['date'])\n",
    "tamis_pm25['date'] = pd.to_datetime(tamis_pm25['Date'], format='%Y%m%d').dt.strftime(\"%Y-%m-%d\")\n",
    "tamis_pm25['date']=pd.to_datetime(tamis_pm25['date'])\n",
    "tamis_pm10_detail['date'] = pd.to_datetime(tamis_pm10_detail['Date'], format='%Y%m%d').dt.strftime(\"%Y-%m-%d\")\n",
    "tamis_pm10_detail['date']=pd.to_datetime(tamis_pm10_detail['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  renaming columns\n",
    "\n",
    "col_tamis_ozone={'City':'city', 'AQS Code':'site_code', 'Site Name':'site_name',\n",
    "                 'Latitude':'site_lat', 'Longitude':'site_lon','Start Hour':'start_hour',\n",
    "                 'Start Time':'start_time','Duration':'duration','SOC':'ozone_soc',\n",
    "                 'Ozone (ppbv) <44201>':'ozone_1hr', 'date':'date'}\n",
    "\n",
    "col_tamis_co={'City':'city', 'AQS Code':'site_code', 'Site Name':'site_name',\n",
    "                 'Latitude':'site_lat', 'Longitude':'site_lon','Start Hour':'start_hour',\n",
    "                 'Start Time':'start_time','Duration':'duration','SOC':'co_soc',\n",
    "                 'Carbon Monoxide (ppmv) <42101>':'co_1hr', 'date':'date'}\n",
    "\n",
    "col_tamis_no2={'City':'city', 'AQS Code':'site_code', 'Site Name':'site_name',\n",
    "                 'Latitude':'site_lat', 'Longitude':'site_lon','Start Hour':'start_hour',\n",
    "                 'Start Time':'start_time','Duration':'duration','SOC':'no2_soc',\n",
    "                 'Nitrogen Dioxide (NO2) (ppbv) <42602>':'no2_1hr', 'date':'date'}\n",
    "\n",
    "col_tamis_so2={'City':'city', 'AQS Code':'site_code', 'Site Name':'site_name',\n",
    "                 'Latitude':'site_lat', 'Longitude':'site_lon','Start Hour':'start_hour',\n",
    "                 'Start Time':'start_time','Duration':'duration','SOC':'so2_soc',\n",
    "                 'Sulfur Dioxide (ppbv) <42401>':'so2_1hr', 'date':'date'}\n",
    "\n",
    "col_tamis_pm10_24hr={'City':'city', 'AQS Code':'site_code', 'Site Name':'site_name',\n",
    "                 'Latitude':'site_lat', 'Longitude':'site_lon','Start Hour':'start_hour',\n",
    "                 'Start Time':'start_time','Duration':'duration','SOC':'pm10_24hr_soc',\n",
    "                     'Pm10 - Lc (ug/m3 (LC)) <85101>':'pm10_24hr','Pm10 Total 0-10um Stp (ug/m3 (25 C)) <81102>':'pm10_total_24hr',\n",
    "                     'Pm10-2.5 - Local Conditions (ug/m3 (LC)) <86101>':'pm10_minus_pm2_5_24hr','date':'date'}\n",
    "\n",
    "col_tamis_pm25={'City':'city', 'AQS Code':'site_code', 'Site Name':'site_name',\n",
    "                 'Latitude':'site_lat', 'Longitude':'site_lon','Start Hour':'start_hour',\n",
    "                 'Start Time':'start_time','Duration':'duration','SOC':'pm25_soc',\n",
    "                 'Pm2.5 - Local Conditions (ug/m3 (LC)) <88101>':'pm25_24hr', 'date':'date'}\n",
    "     \n",
    "col_tamis_pm10_detail={'City':'city', 'AQS Code':'site_code', 'Site Name':'site_name',\n",
    "                 'Latitude':'site_lat', 'Longitude':'site_lon','Start Hour':'start_hour',\n",
    "                 'Start Time':'start_time','Duration':'duration',\n",
    "                 'Lead Pm10 Stp (ug/m3 (25 C)) <82128>':'pb_24hr', 'date':'date'}\n",
    "\n",
    "rtamis_ozone=tamis_ozone.rename(columns=col_tamis_ozone)\n",
    "rtamis_co=tamis_co.rename(columns=col_tamis_co)\n",
    "rtamis_no2=tamis_no2.rename(columns=col_tamis_no2)\n",
    "rtamis_so2=tamis_so2.rename(columns=col_tamis_so2)\n",
    "rtamis_pm10_24hr=tamis_pm10_24hr.rename(columns=col_tamis_pm10_24hr)\n",
    "rtamis_pm25=tamis_pm25.rename(columns=col_tamis_pm25)\n",
    "rtamis_pm10_detail=tamis_pm10_detail.rename(columns=col_tamis_pm10_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### subsetting\n",
    "newcol_tamis_ozone=col_tamis_ozone.values()\n",
    "sub_ozone=[name for name in newcol_tamis_ozone]\n",
    "frtamis_ozone=rtamis_ozone[sub_ozone]\n",
    "\n",
    "newcol_tamis_co=col_tamis_co.values()\n",
    "sub_co=[name for name in newcol_tamis_co]\n",
    "frtamis_co=rtamis_co[sub_co]\n",
    "\n",
    "newcol_tamis_no2=col_tamis_no2.values()\n",
    "sub_no2=[name for name in newcol_tamis_no2]\n",
    "frtamis_no2=rtamis_no2[sub_no2]\n",
    "\n",
    "newcol_tamis_so2=col_tamis_so2.values()\n",
    "sub_so2=[name for name in newcol_tamis_so2]\n",
    "frtamis_so2=rtamis_so2[sub_so2]\n",
    "\n",
    "newcol_tamis_pm10_24hr=col_tamis_pm10_24hr.values()\n",
    "sub_pm10_24hr=[name for name in newcol_tamis_pm10_24hr]\n",
    "frtamis_pm10_24hr=rtamis_pm10_24hr[sub_pm10_24hr]\n",
    "\n",
    "newcol_tamis_pm25=col_tamis_pm25.values()\n",
    "sub_pm25=[name for name in newcol_tamis_pm25]\n",
    "frtamis_pm25=rtamis_pm25[sub_pm25]\n",
    "\n",
    "### subsetting Lead data\n",
    "newcol_tamis_pm10_detail=col_tamis_pm10_detail.values()\n",
    "sub_pm10_detail=[name for name in newcol_tamis_pm10_detail]\n",
    "frtamis_pb=rtamis_pm10_detail[sub_pm10_detail]\n",
    "frtamis_ozone.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Monitoring Stations Information:###\n",
    "Information about each weather station are extracted to match their location to interesting attributes such as landuse, source emission...etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Monitoring stations from EPA\n",
    "epa_stations_ozone=frepa_ozone[['site_id','site_name','site_lat','site_lon']]\n",
    "epa_stations_co=frepa_co[['site_id','site_name','site_lat','site_lon']]\n",
    "epa_stations_no2=frepa_no2[['site_id','site_name','site_lat','site_lon']]\n",
    "epa_stations_so2=frepa_so2[['site_id','site_name','site_lat','site_lon']]\n",
    "epa_stations_pm10=frepa_pm10[['site_id','site_name','site_lat','site_lon']]\n",
    "epa_stations_pm25=frepa_pm25[['site_id','site_name','site_lat','site_lon']]\n",
    "epa_stations_pb=frepa_pb[['site_id','site_name','site_lat','site_lon']]\n",
    "\n",
    "epa_stations_ozone['poll']='ozone'\n",
    "epa_stations_co['poll']='co'\n",
    "epa_stations_no2['poll']='no2'\n",
    "epa_stations_so2['poll']='so2'\n",
    "epa_stations_pm10['poll']='pm10'\n",
    "epa_stations_pm25['poll']='pm25'\n",
    "epa_stations_pb['poll']='pb'\n",
    "\n",
    "epa_stations_ozone.drop_duplicates(inplace=True)\n",
    "epa_stations_co.drop_duplicates(inplace=True)\n",
    "epa_stations_no2.drop_duplicates(inplace=True)\n",
    "epa_stations_so2.drop_duplicates(inplace=True)\n",
    "epa_stations_pm10.drop_duplicates(inplace=True)\n",
    "epa_stations_pm25.drop_duplicates(inplace=True)\n",
    "epa_stations_pb.drop_duplicates(inplace=True)\n",
    "\n",
    "epa_stations=pd.concat([epa_stations_ozone,epa_stations_co,epa_stations_no2,epa_stations_so2,epa_stations_pm10,epa_stations_pm25,epa_stations_pb], axis=0)\n",
    "epa_stations.drop_duplicates(inplace=True)\n",
    "\n",
    "### Monitoring Stations from TECQ/Tamis\n",
    "tamis_station_ozone=frtamis_ozone[['city','site_code','site_lat','site_lon']]\n",
    "tamis_station_co=frtamis_co[['city','site_code','site_lat','site_lon']]\n",
    "tamis_station_no2=frtamis_no2[['city','site_code','site_lat','site_lon']]\n",
    "tamis_station_so2=frtamis_so2[['city','site_code','site_lat','site_lon']]\n",
    "tamis_station_pm10_24hr=frtamis_pm10_24hr[['city','site_code','site_lat','site_lon']]\n",
    "tamis_station_pm25=frtamis_pm25[['city','site_code','site_lat','site_lon']]\n",
    "tamis_station_pb=frtamis_pb[['city','site_code','site_lat','site_lon']]\n",
    "\n",
    "tamis_station_ozone.drop_duplicates(inplace=True)\n",
    "tamis_station_co.drop_duplicates(inplace=True)\n",
    "tamis_station_no2.drop_duplicates(inplace=True)\n",
    "tamis_station_so2.drop_duplicates(inplace=True)\n",
    "tamis_station_pm10_24hr.drop_duplicates(inplace=True)\n",
    "tamis_station_pm25.drop_duplicates(inplace=True)\n",
    "tamis_station_pb.drop_duplicates(inplace=True)\n",
    "\n",
    "tamis_station_ozone['poll']='ozone'\n",
    "tamis_station_co['poll']='co'\n",
    "tamis_station_no2['poll']='no2'\n",
    "tamis_station_so2['poll']='so2'\n",
    "tamis_station_pm10_24hr['poll']='pm10'\n",
    "tamis_station_pm25['poll']='pm25'\n",
    "tamis_station_pb['poll']='pb'\n",
    "\n",
    "tamis_stations=pd.concat([tamis_station_ozone,tamis_station_co,tamis_station_no2,tamis_station_so2,tamis_station_pm10_24hr,tamis_station_pm25,tamis_station_pb],axis=0)\n",
    "tamis_stations.drop_duplicates(inplace=True)\n",
    "\n",
    "### Saving the information in '00_SavedDataframes'\n",
    "epa_stations.to_excel(path_header+path_df+'epa_stations.xlsx',sheet_name='epa_stations')\n",
    "tamis_stations.to_excel(path_header+path_df+'tamis_stations.xlsx',sheet_name='tamis_stations')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exported xlsx files containing stations information that is plotted with all the shape files provided by the HGA tool to extract population data, landuse, roads...etc... The completed tables will be reimported in the EDA jupyter notebook. The mapping work will be*/*is stored in the folder '00*_*StuffAndThings' when polished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3. Data Definition and Cleaning:## \n",
    "###3.1. Location of Air Quality Stations:\n",
    "Logically the cleaning has to start by looking at duplicate stations between EPA and TECQ before looking at duplicate within the air quality data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epa_solo=epa_stations.drop('poll',axis=1)\n",
    "tamis_solo=tamis_stations.drop('poll',axis=1)\n",
    "print(epa_solo.shape,tamis_solo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 79 EPA stations and 58 TECQ stations. As TECQ reports data annually to EPA, there might be an overlap. Let's use the option 'indicator' of pd.merge to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### comparing epa_stations to tamis_stations\n",
    "allstations= epa_stations.merge(tamis_stations, indicator=True,how='outer')\n",
    "allstations._merge[allstations._merge=='both'].count()                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated by the presence of 'both', there are 22 redundent stations. Let's find out which ones and store this information in the dataframe 'redund_stations'. This dataframe will be used later during the cleaning of air quality data. Here is a preview of redund_stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redund_stations=allstations.loc[allstations['_merge']=='both']\n",
    "redund_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another fact to keep in mind, as shown below, is that some stations measure multiple types of pollutant, and some measure only one pollutant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#byloc=pd.melt(allstations,id_vars=['poll'],value_vars=['site_lat','site_lon'])\n",
    "#byloc.head()\n",
    "byloc=allstations.groupby(['site_lat','site_lon']).agg({'poll':'count'})\n",
    "byloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize the available information regarding air quality stations:\n",
    "- the dataframes epa_stations and tamis_stations contain the information about the stations.\n",
    "- the dataframe 'allstations' contain both datasets and a column '_merge' which indicates duplicate stations between both sets.\n",
    "- 22 stations are common to both EPA and TECQ datasets. There are saved under 'redund_stations'\n",
    "- the column 'poll' provides the type of measurement done at each station.\n",
    "- some stations measure multiple pollutants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.2. Meteorology Data:\n",
    "Houston is a very large city. The weather tends to vary from one side to the other. With this in mind, the weather data was collected from three far apart stations: Hobby airport to the North, IAH to the South, and the Galveston station to the East.This hypothesis will have to be chekced after cleaning the data.\n",
    "The method 'describe' shows that some data is numerical (i.e. temp, wind speed), some data is boolean (format 1 or Nan) and some data is informational (i.e. direction of wind). The dataset needs to be simplified into fewer columns showing the occurrence or not of an average meteorological event (i.e. rain, snow, wind, smoke, fog), some important numerical values (i.e. temperature, wind speed, rain amount) and of course date and the location of the weather stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cleaning meteo\n",
    "dicofog={'fog':'new','fog_ground':'new','fog_heavy':'new'}\n",
    "meteo['fog_all']=meteo.groupby(dicofog,axis=1).sum()\n",
    "meteo['fog_all'].replace([1,2,3],1,inplace=True)\n",
    "\n",
    "dicosnow={'snow_sleet':'new','snow_grains':'new','drizzle_freezing':'new','snow':'new',\n",
    "        'snow_drifting':'new','snow_glaze_rime':'new','rain_freezing':'new'}\n",
    "meteo['snow_all']=meteo.groupby(dicofog,axis=1).sum()\n",
    "meteo['snow_all'].replace([1,2,3,4,5,6,7,8],1,inplace=True)\n",
    "\n",
    "dicorain={'rain':'new','drizzle':'new','rain_mist':'new','drizzle':'new',\n",
    "        'rain_hail':'new'}\n",
    "meteo['rain_all']=meteo.groupby(dicofog,axis=1).sum()\n",
    "meteo['rain_all'].replace([1,2,3,4,5,6,7,8],1,inplace=True)\n",
    "\n",
    "dicowind_dmg={'wind_tornado':'new','wind_high_dmg':'new'}\n",
    "meteo['wind_dmg']=meteo.groupby(dicofog,axis=1).sum()\n",
    "meteo['wind_dmg'].replace([1,2,3,4,5,6,7,8],1,inplace=True)\n",
    "\n",
    "meteo_simple=meteo[['date', 'dust_sand','rain_prcp', 'smoke_haze', 'station_code',\n",
    "       'station_lat', 'station_lon', 'station_name', 'temp_avg', 'temp_max',\n",
    "       'temp_min', 'thunder', 'wind_avgspeed', 'wind_fastest_2min',\n",
    "       'fog_all', 'snow_all', 'rain_all', 'wind_dmg']]\n",
    "\n",
    "### Replace Nan by 0 in no numerical columns\n",
    "meteo_simple['dust_sand'] = meteo_simple['dust_sand'].fillna(0)\n",
    "meteo_simple['smoke_haze'] = meteo_simple['smoke_haze'].fillna(0)\n",
    "meteo_simple['thunder'] = meteo_simple['thunder'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temperature columns have some Nan values. It looks like it would be better to drop the rows without temp_min and temp_max, and to replace the NaN values in the temp*_*avg column by calculating an average from temp*_*min and temp*_*max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_simple['temp_min'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_simple['temp_max'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_simple['temp_avg'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop NaN in temperature columns\n",
    "meteo_simple = meteo_simple.dropna(axis=0, subset=['temp_max'])\n",
    "meteo_simple = meteo_simple.dropna(axis=0, subset=['temp_min'])\n",
    "\n",
    "### Fill up\n",
    "meteo_simple['temp_avg_where'] = np.where(meteo_simple.temp_avg.isnull(),(meteo_simple.temp_max+meteo_simple.temp_min)/2,meteo_simple.temp_avg)\n",
    "\n",
    "### Create a boolean wind column\n",
    "meteo_simple['wind_all'] = np.where(meteo_simple.wind_avgspeed.isnull(), False, True)\n",
    "\n",
    "### Remove remaining NaN\n",
    "meteo_simple['wind_avgspeed'] = meteo_simple['wind_avgspeed'].fillna(0)\n",
    "meteo_simple['wind_fastest_2min'] = meteo_simple['wind_fastest_2min'].fillna(0)\n",
    "\n",
    "### Drop 'temp_avg' and replace it by 'temp_avg_where'\n",
    "\n",
    "meteo_simple.drop(['temp_avg'], axis=1,inplace=True)\n",
    "meteo_simple.rename(columns={'temp_avg_where': 'temp_avg'},inplace=True)\n",
    "meteo_simple.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.3. Riopa Data Cleaning: ###\n",
    "There are some NaN and NaT values in the riopa dataset. NaT are simply due to the fact that no measurement of the particular pollutant was made in the respective row. That is fine because the main 'date' column has no missing values. Missing rain, fog, groundfog, and smoke values can be replaced by 0. The 9 missing temp*_*mean can be recalculated using temp*_*min and temp*_*max. Rows with missing pm25 will have to be dropped.\n",
    "As part of the sampling is done on people, which is of no interest in this project, the riopa dataset has to be subset on airtype (= indoor and outdoor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riopa.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cleaning riopa\n",
    "\n",
    "### Drop rows with missing pm25\n",
    "riopa.dropna(subset=['pm25'],inplace=True)\n",
    "\n",
    "### Replacing nan by 0\n",
    "riopa['rain'] = riopa['rain'].fillna(0)\n",
    "riopa['fog'] = riopa['fog'].fillna(0)\n",
    "riopa['groundfog'] = riopa['groundfog'].fillna(0)\n",
    "riopa['smoke_haze'] = riopa['smoke_haze'].fillna(0)\n",
    "riopa['comment_aer'] = riopa['comment_aer'].fillna(0)\n",
    "riopa['airexrate'] = riopa['airexrate'].fillna(0)\n",
    "riopa['ambient_rh'] = riopa['ambient_rh'].fillna(0)\n",
    "riopa['ambient_temp_c'] = riopa['ambient_temp_c'].fillna(0)\n",
    "riopa['location'] = riopa['location'].fillna(0)\n",
    "riopa['visitnumber'] = riopa['visitnumber'].fillna(0)\n",
    "riopa['tempid'] = riopa['tempid'].fillna(0)\n",
    "riopa['validity'] = riopa['validity'].fillna(0)\n",
    "riopa['comments_pm25'] = riopa['comments_pm25'].fillna(0)\n",
    "riopa['landuse_class'] = riopa['landuse_class'].fillna('tbd')\n",
    "\n",
    "### Making one fog column out of two\n",
    "dicofog2={'fog':'new','groundfog':'new'}\n",
    "riopa['fog_all']=riopa.groupby(dicofog2,axis=1).sum()\n",
    "riopa['fog_all'].replace([1,2,3],1,inplace=True)\n",
    "\n",
    "### Dropping the two fog columns\n",
    "riopa.drop(['fog','groundfog'], axis=1,inplace=True)\n",
    "\n",
    "### Filling the missing mean temperatures\n",
    "riopa['temp_avg'] = np.where(riopa.temp_mean.isnull(),(riopa.temp_max+riopa.temp_min)/2,riopa.temp_mean)\n",
    "riopa.drop(['temp_mean'], axis=1,inplace=True)\n",
    "\n",
    "### Looking at it\n",
    "\n",
    "riopa.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### subset to keep inly interesting columns (i.e. removing multiple dates)\n",
    "riopa_slim=riopa.drop(['date_start_aer','date_end_aer','date_temp','tempid','date_temp','date_start_meteo','date_end_meteo','date_noaa'], axis=1,inplace=False)\n",
    "\n",
    "### subset outdoor, indoor and leaving 'person' behind\n",
    "riopa_outdoor=riopa_slim.loc[riopa.airtype=='OUTDOOR']\n",
    "riopa_indoor=riopa_slim.loc[riopa.airtype=='INDOOR']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.4. Air Quality Data Cleaning: ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values in the 'frepa*_*pollutant' individual dataframes. There are null values in the merged epa dataframe because each air quality station may not record all the pollutants. That is expected and fine.\n",
    "It looks like there are no null values in the 'frtamis*_*pollutant' individual frames but this is because all the null values are question marks. The function 'theanswer' takes a dataframe and a column name to drop the rows for which the column contains a question mark and convert the column into a numerical column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### frtamis_pb displays no null values\n",
    "frtamis_pb.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### although frtamis_pb contains ? in the pb_24hr column\n",
    "frtamis_pb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove all question marks with the function 'theanswer'\n",
    "\n",
    "def theanswer(df,col):\n",
    "    ''' function drops rows where col has a ?'''\n",
    "    dfcond=df[df[col]=='?'].index\n",
    "    df.drop(dfcond,inplace=True)\n",
    "    ''' function converts column to numeric'''    \n",
    "    df[col]=pd.to_numeric(df[col])\n",
    "    return df\n",
    "\n",
    "ctamis_pm10=theanswer(frtamis_pm10_24hr,'pm10_24hr')\n",
    "ctamis_pm10=theanswer(frtamis_pm10_24hr,'pm10_total_24hr')\n",
    "ctamis_pm10=theanswer(frtamis_pm10_24hr,'pm10_minus_pm2_5_24hr')\n",
    "ctamis_pm25=theanswer(frtamis_pm25,'pm25_24hr')\n",
    "ctamis_pb=theanswer(frtamis_pb,'pb_24hr')\n",
    "\n",
    "### naming dataframes without ? in the data as \"clean\" like the dataframes above\n",
    "ctamis_ozone=frtamis_ozone\n",
    "ctamis_no2=frtamis_no2\n",
    "ctamis_so2=frtamis_so2\n",
    "ctamis_co=frtamis_co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4. Wrapping up:##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are ready for EDA.The method .describe() provides basics statistics for each dataset or individual columns. Additional work is required to get an understanding of the spatial distribution of the data. The geospatial work is done in another notebook and will be introduced in the EDA notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctamis_ozone.ozone_1hr.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
